{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The user interface has evolved dramatically over the years, bringing us incredible benefits in terms of time efficiency. For instance, in the past, we had to manually dial a number to call someone with an old phone. But now, with buttons, touch screens and even voice commands, we can make calls in a matter of seconds.</p> <p>In many machine learning projects, we face the challenge of comparing different models, datasets and frameworks. This can be very tedious\ud83d\ude12 and time-consuming\u23f3. Because each dataset has its own format, each framework has its own configuration and each model has its own input and output even they are dealing with the exact same task \ud83d\ude20.</p> <p>When it comes to MLOps (machine learning operations), you need to be able to keep up with all the new ideas and SOTA models in deep learning as quickly as possible\ud83d\ude80.</p> <p>Here comes Waffle\ud83e\uddc7. Waffle is a framework that lets you use lots of different deep learning tools through just one interface. It's user-friendly and easy\ud83d\ude0a. We believe it's going to make a big revolution in the AI industry.</p> <p></p> <p>Experience the power\ud83d\udcaa of revolution that Waffle\ud83e\uddc7 brings to you, unlocking limitless possibilities for your machine learning projects.</p>"},{"location":"#discussion","title":"Discussion","text":"<p>If you want to discuss about waffle or request new features, please use our discussion page</p> <p></p>"},{"location":"tutorials/","title":"Welcome to Waffle Tutorial \ud83e\uddc7\ud83e\uddc7\ud83e\uddc7","text":"<p>In this tutorial, we will learn how to use <code>Waffle</code>: </p> <ul> <li>Waffle Dataset</li> <li>Waffle Hub</li> </ul>"},{"location":"tutorials/active_filter/","title":"Active filter","text":"In\u00a0[1]: Copied! <pre>from waffle_hub.hub.adapter.ultralytics import UltralyticsHub\n\nhub = UltralyticsHub.load(\"ultralytics_mnist_detection\")\n</pre> from waffle_hub.hub.adapter.ultralytics import UltralyticsHub  hub = UltralyticsHub.load(\"ultralytics_mnist_detection\") <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/waffle_hub/__init__.py:16: UserWarning: \n            torch 1.13.1+cu117 has not been tested.\n            We recommend you to use one of ['1.13.1']\n            \n  warnings.warn(\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/waffle_hub/__init__.py:56: UserWarning: \n                ultralytics 8.0.91 has not been tested.\n                We recommend you to use one of ['8.0.87']\n                \n  warnings.warn(\n</pre> In\u00a0[1]: Copied! <pre>from waffle_menu.active_learning import RandomSampling, PL2NSampling\n</pre> from waffle_menu.active_learning import RandomSampling, PL2NSampling <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/waffle_hub/__init__.py:16: UserWarning: \n            torch 1.13.1+cu117 has not been tested.\n            We recommend you to use one of ['1.13.1']\n            \n  warnings.warn(\n</pre> In\u00a0[3]: Copied! <pre>RandomSampling(\n    seed=1234\n).sample(\n    image_dir=\"mnist/images\",\n    num_images=5,\n    result_dir=\"random_sampled\",\n    save_images=True\n)\n</pre> RandomSampling(     seed=1234 ).sample(     image_dir=\"mnist/images\",     num_images=5,     result_dir=\"random_sampled\",     save_images=True ) <pre>4it [00:00, 91.17it/s]\n</pre> Out[3]: <pre>&lt;waffle_menu.active_learning.active_learning.ActiveLearningCallback at 0x7fc993964430&gt;</pre> In\u00a0[4]: Copied! <pre>PL2NSampling(\n    hub=hub\n).sample(\n    image_dir=\"mnist/images\",\n    num_images=5,\n    result_dir=\"PL2N_sampled\",\n    save_images=True\n)\n</pre> PL2NSampling(     hub=hub ).sample(     image_dir=\"mnist/images\",     num_images=5,     result_dir=\"PL2N_sampled\",     save_images=True ) <pre>4it [00:00,  7.69it/s]\n</pre> Out[4]: <pre>&lt;waffle_menu.active_learning.active_learning.ActiveLearningCallback at 0x7fc99392e880&gt;</pre> In\u00a0[5]: Copied! <pre>import PIL.Image\nfrom pathlib import Path\n</pre> import PIL.Image from pathlib import Path In\u00a0[6]: Copied! <pre># display n images in a row with PIL\ndef display_images(images):\n    images = [PIL.Image.open(image) for image in images]\n    widths, heights = zip(*(i.size for i in images))\n    total_width = sum(widths)\n    max_height = max(heights)\n    new_im = PIL.Image.new(\"RGB\", (total_width, max_height))\n    x_offset = 0\n    for im in images:\n        new_im.paste(im, (x_offset, 0))\n        x_offset += im.size[0]\n    return new_im\n</pre> # display n images in a row with PIL def display_images(images):     images = [PIL.Image.open(image) for image in images]     widths, heights = zip(*(i.size for i in images))     total_width = sum(widths)     max_height = max(heights)     new_im = PIL.Image.new(\"RGB\", (total_width, max_height))     x_offset = 0     for im in images:         new_im.paste(im, (x_offset, 0))         x_offset += im.size[0]     return new_im In\u00a0[7]: Copied! <pre>display_images(list(Path(\"random_sampled/images\").glob(\"*.png\")))\n</pre> display_images(list(Path(\"random_sampled/images\").glob(\"*.png\"))) Out[7]: In\u00a0[8]: Copied! <pre>display_images(list(Path(\"PL2N_sampled/images\").glob(\"*.png\")))\n</pre> display_images(list(Path(\"PL2N_sampled/images\").glob(\"*.png\"))) Out[8]: In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/active_filter/#active-sampling-with-hub","title":"Active Sampling with hub\u00b6","text":"<p>Active Learning is a data sampling technique that allows you to select the most informative samples from a large unlabeled dataset. This is done by training a model on a small subset of labeled data and then using the model to predict the labels of the unlabeled data. The samples with the highest prediction uncertainty are then selected for labeling. This process is repeated until the model is trained on all the data. The model is then retrained on the entire dataset. This process is called active learning. </p> <p>You can get more information about active learning from our Blog Post. </p> <p>This is our private feature. If you want to use it, please contact us. </p>"},{"location":"tutorials/active_filter/#load-trained-hub","title":"Load trained Hub\u00b6","text":"<p>To use active learning, you need to load the trained hub.</p>"},{"location":"tutorials/active_filter/#import-sampling-methods","title":"Import Sampling Methods\u00b6","text":"<p>This tutorial uses the following sampling methods: </p> <ul> <li>Random Sampling</li> <li>PL2N Sampling</li> </ul>"},{"location":"tutorials/prepare_dataset/","title":"Prepare dataset","text":"In\u00a0[1]: Copied! <pre>! wget https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist.zip\n</pre> ! wget https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist.zip <pre>--2023-05-03 11:30:31--  https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist.zip\nResolving github.com (github.com)... 20.200.245.247\nConnecting to github.com (github.com)|20.200.245.247|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/snuailab/assets/main/waffle/sample_dataset/mnist.zip [following]\n--2023-05-03 11:30:31--  https://raw.githubusercontent.com/snuailab/assets/main/waffle/sample_dataset/mnist.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 157823 (154K) [application/zip]\nSaving to: \u2018mnist.zip\u2019\n\nmnist.zip           100%[===================&gt;] 154.12K  --.-KB/s    in 0.02s   \n\n2023-05-03 11:30:32 (9.02 MB/s) - \u2018mnist.zip\u2019 saved [157823/157823]\n\n</pre> In\u00a0[2]: Copied! <pre>! unzip mnist.zip -d mnist\n</pre> ! unzip mnist.zip -d mnist <pre>Archive:  mnist.zip\n  inflating: mnist/coco.json         \n   creating: mnist/images/\n  inflating: mnist/images/1.png      \n  inflating: mnist/images/10.png     \n  inflating: mnist/images/100.png    \n  inflating: mnist/images/11.png     \n  inflating: mnist/images/12.png     \n  inflating: mnist/images/13.png     \n  inflating: mnist/images/14.png     \n  inflating: mnist/images/15.png     \n  inflating: mnist/images/16.png     \n  inflating: mnist/images/17.png     \n  inflating: mnist/images/18.png     \n  inflating: mnist/images/19.png     \n  inflating: mnist/images/2.png      \n  inflating: mnist/images/20.png     \n  inflating: mnist/images/21.png     \n  inflating: mnist/images/22.png     \n  inflating: mnist/images/23.png     \n  inflating: mnist/images/24.png     \n  inflating: mnist/images/25.png     \n  inflating: mnist/images/26.png     \n  inflating: mnist/images/27.png     \n  inflating: mnist/images/28.png     \n  inflating: mnist/images/29.png     \n  inflating: mnist/images/3.png      \n  inflating: mnist/images/30.png     \n  inflating: mnist/images/31.png     \n  inflating: mnist/images/32.png     \n  inflating: mnist/images/33.png     \n  inflating: mnist/images/34.png     \n  inflating: mnist/images/35.png     \n  inflating: mnist/images/36.png     \n  inflating: mnist/images/37.png     \n  inflating: mnist/images/38.png     \n  inflating: mnist/images/39.png     \n  inflating: mnist/images/4.png      \n  inflating: mnist/images/40.png     \n  inflating: mnist/images/41.png     \n  inflating: mnist/images/42.png     \n  inflating: mnist/images/43.png     \n  inflating: mnist/images/44.png     \n  inflating: mnist/images/45.png     \n  inflating: mnist/images/46.png     \n  inflating: mnist/images/47.png     \n  inflating: mnist/images/48.png     \n  inflating: mnist/images/49.png     \n  inflating: mnist/images/5.png      \n  inflating: mnist/images/50.png     \n  inflating: mnist/images/51.png     \n  inflating: mnist/images/52.png     \n  inflating: mnist/images/53.png     \n  inflating: mnist/images/54.png     \n  inflating: mnist/images/55.png     \n  inflating: mnist/images/56.png     \n  inflating: mnist/images/57.png     \n  inflating: mnist/images/58.png     \n  inflating: mnist/images/59.png     \n  inflating: mnist/images/6.png      \n  inflating: mnist/images/60.png     \n  inflating: mnist/images/61.png     \n  inflating: mnist/images/62.png     \n  inflating: mnist/images/63.png     \n  inflating: mnist/images/64.png     \n  inflating: mnist/images/65.png     \n  inflating: mnist/images/66.png     \n  inflating: mnist/images/67.png     \n  inflating: mnist/images/68.png     \n  inflating: mnist/images/69.png     \n  inflating: mnist/images/7.png      \n  inflating: mnist/images/70.png     \n  inflating: mnist/images/71.png     \n  inflating: mnist/images/72.png     \n  inflating: mnist/images/73.png     \n  inflating: mnist/images/74.png     \n  inflating: mnist/images/75.png     \n  inflating: mnist/images/76.png     \n  inflating: mnist/images/77.png     \n  inflating: mnist/images/78.png     \n  inflating: mnist/images/79.png     \n  inflating: mnist/images/8.png      \n  inflating: mnist/images/80.png     \n  inflating: mnist/images/81.png     \n  inflating: mnist/images/82.png     \n  inflating: mnist/images/83.png     \n  inflating: mnist/images/84.png     \n  inflating: mnist/images/85.png     \n  inflating: mnist/images/86.png     \n  inflating: mnist/images/87.png     \n  inflating: mnist/images/88.png     \n  inflating: mnist/images/89.png     \n  inflating: mnist/images/9.png      \n  inflating: mnist/images/90.png     \n  inflating: mnist/images/91.png     \n  inflating: mnist/images/92.png     \n  inflating: mnist/images/93.png     \n  inflating: mnist/images/94.png     \n  inflating: mnist/images/95.png     \n  inflating: mnist/images/96.png     \n  inflating: mnist/images/97.png     \n  inflating: mnist/images/98.png     \n  inflating: mnist/images/99.png     \n  inflating: mnist/test.json         \n  inflating: mnist/train.json        \n  inflating: mnist/val.json          \n</pre> In\u00a0[4]: Copied! <pre>from waffle_hub.dataset import Dataset\nfrom waffle_hub import TaskType\n\ndataset = Dataset.from_coco(\n    name='mnist_det',\n    task=TaskType.OBJECT_DETECTION,\n    coco_file=\"./mnist/coco.json\",\n    coco_root_dir=\"./mnist/images\",\n)\n</pre> from waffle_hub.dataset import Dataset from waffle_hub import TaskType  dataset = Dataset.from_coco(     name='mnist_det',     task=TaskType.OBJECT_DETECTION,     coco_file=\"./mnist/coco.json\",     coco_root_dir=\"./mnist/images\", ) <pre>loading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n</pre> <pre>1it [00:00, 17.83it/s]:   0%|          | 0/100 [00:00&lt;?, ?it/s]\nImporting coco dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 1643.59it/s]\n</pre> <p><code>Dataset</code> instance provides several useful properties and methods. See Dataset Documentation for more details.</p> In\u00a0[\u00a0]: Copied! <pre>dataset.category_names\n</pre> dataset.category_names Out[\u00a0]: <pre>['1', '2']</pre> In\u00a0[6]: Copied! <pre>dataset.split(\n    train_ratio=0.8,\n    val_ratio=0.1,\n    test_ratio=0.1,\n)\nlist(dataset.set_dir.iterdir())\n</pre> dataset.split(     train_ratio=0.8,     val_ratio=0.1,     test_ratio=0.1, ) list(dataset.set_dir.iterdir()) Out[6]: <pre>[PosixPath('datasets/mnist_det/sets/test.json'),\n PosixPath('datasets/mnist_det/sets/val.json'),\n PosixPath('datasets/mnist_det/sets/train.json'),\n PosixPath('datasets/mnist_det/sets/unlabeled.json')]</pre> In\u00a0[7]: Copied! <pre>from waffle_hub import DataType\n\nexport_dir = dataset.export(DataType.YOLO)\nexport_dir\n</pre> from waffle_hub import DataType  export_dir = dataset.export(DataType.YOLO) export_dir Out[7]: <pre>'datasets/mnist_det/exports/YOLO'</pre> In\u00a0[8]: Copied! <pre>export_dir = dataset.export(DataType.HUGGINGFACE)\nexport_dir\n</pre> export_dir = dataset.export(DataType.HUGGINGFACE) export_dir <pre>Downloading and preparing dataset generator/default to /home/lhj/.cache/huggingface/datasets/generator/default-f529974887995952/0.0.0...\n</pre> <pre>                                                              \r</pre> <pre>Dataset generator downloaded and prepared to /home/lhj/.cache/huggingface/datasets/generator/default-f529974887995952/0.0.0. Subsequent calls will reuse this data.\nDownloading and preparing dataset generator/default to /home/lhj/.cache/huggingface/datasets/generator/default-0e082ffb6d530bbe/0.0.0...\n</pre> <pre>                                                        \r</pre> <pre>Dataset generator downloaded and prepared to /home/lhj/.cache/huggingface/datasets/generator/default-0e082ffb6d530bbe/0.0.0. Subsequent calls will reuse this data.\nDownloading and preparing dataset generator/default to /home/lhj/.cache/huggingface/datasets/generator/default-f67a2a27d69a518b/0.0.0...\n</pre> <pre>                                                        \r</pre> <pre>Dataset generator downloaded and prepared to /home/lhj/.cache/huggingface/datasets/generator/default-f67a2a27d69a518b/0.0.0. Subsequent calls will reuse this data.\n</pre> <pre>                                                                                          \r</pre> Out[8]: <pre>'datasets/mnist_det/exports/HUGGINGFACE'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/prepare_dataset/#download-sample-dataset","title":"Download Sample dataset\u00b6","text":"<p>We provide sample dataset for this tutorial. You can download it from asset.  This dataset can be used for <code>Classification</code>, <code>Object Detection</code>, <code>Semantic Segmentation</code>, <code>Instance Segmentation</code> tasks.</p>"},{"location":"tutorials/prepare_dataset/#convert-to-waffle-dataset","title":"Convert to Waffle Dataset\u00b6","text":"<p>You can create <code>Waffle Dataset</code> from <code>COCO</code>, <code>YOLO</code>, <code>HuggingFace</code> format.</p>"},{"location":"tutorials/prepare_dataset/#split-dataset","title":"Split Dataset\u00b6","text":""},{"location":"tutorials/prepare_dataset/#export-dataset-to-other-formats","title":"Export Dataset to other formats\u00b6","text":"<p>After splitting dataset, you can export dataset to other formats.  The returned value of <code>Dataset.export()</code> can be used in training <code>Hub</code> directly.</p>"},{"location":"tutorials/using_hub/","title":"Using hub","text":"In\u00a0[\u00a0]: Copied! <pre>from waffle_hub.hub.adapter.ultralytics import UltralyticsHub\nfrom waffle_hub import TaskType\n</pre> from waffle_hub.hub.adapter.ultralytics import UltralyticsHub from waffle_hub import TaskType <p>See what tasks, models are available in UltralyticsHub.</p> In\u00a0[2]: Copied! <pre>UltralyticsHub.MODEL_TYPES\n</pre> UltralyticsHub.MODEL_TYPES Out[2]: <pre>{'object_detection': {'yolov8': ['n', 's', 'm', 'l', 'x']},\n 'classification': {'yolov8': ['n', 's', 'm', 'l', 'x']},\n 'instance_segmentation': {'yolov8': ['n', 's', 'm', 'l', 'x']}}</pre> <p>By calling <code>UltralyticsHub.new()</code> method you can simply create an Ultralytics instance.</p> In\u00a0[3]: Copied! <pre>ultralytics_hub = UltralyticsHub.new(\n    name=\"ultralytics_mnist_detection\",\n    task=TaskType.OBJECT_DETECTION,\n    model_type=\"yolov8\",\n    model_size=\"n\",\n    categories=[\"1\", \"2\"]\n)\n</pre> ultralytics_hub = UltralyticsHub.new(     name=\"ultralytics_mnist_detection\",     task=TaskType.OBJECT_DETECTION,     model_type=\"yolov8\",     model_size=\"n\",     categories=[\"1\", \"2\"] ) <p><code>Hub</code> instance provides several useful properties and methods. See Hub Documentation for more details.</p> In\u00a0[4]: Copied! <pre>ultralytics_hub.categories\n</pre> ultralytics_hub.categories Out[4]: <pre>[{'supercategory': 'object', 'name': '1'},\n {'supercategory': 'object', 'name': '2'}]</pre> In\u00a0[5]: Copied! <pre>from waffle_hub.hub.adapter.hugging_face import HuggingFaceHub\nfrom waffle_hub import TaskType\n</pre> from waffle_hub.hub.adapter.hugging_face import HuggingFaceHub from waffle_hub import TaskType <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/waffle_hub/__init__.py:56: UserWarning: \n                transformers 4.28.1 has not been tested.\n                We recommend you to use one of ['4.27.4']\n                \n  warnings.warn(\n</pre> In\u00a0[6]: Copied! <pre>HuggingFaceHub.MODEL_TYPES\n</pre> HuggingFaceHub.MODEL_TYPES Out[6]: <pre>{'object_detection': {'DETA': {'base': 'jozhang97/deta-resnet-50'},\n  'DETR': {'base': 'facebook/detr-resnet-50',\n   'large': 'facebook/detr-resnet-101'},\n  'YOLOS': {'tiny': 'hustvl/yolos-tiny'}},\n 'classification': {'ViT': {'tiny': 'WinKawaks/vit-tiny-patch16-224',\n   'base': 'google/vit-base-patch16-224'}}}</pre> In\u00a0[7]: Copied! <pre>huggingface_hub = HuggingFaceHub.new(\n    name=\"huggingface_mnist_detection\",\n    task=TaskType.OBJECT_DETECTION,\n    model_type=\"DETR\",\n    model_size=\"base\",\n    categories=[\"1\", \"2\"]\n)\n</pre> huggingface_hub = HuggingFaceHub.new(     name=\"huggingface_mnist_detection\",     task=TaskType.OBJECT_DETECTION,     model_type=\"DETR\",     model_size=\"base\",     categories=[\"1\", \"2\"] ) In\u00a0[8]: Copied! <pre>from waffle_hub.hub.adapter.tx_model import TxModelHub\nfrom waffle_hub import TaskType\n</pre> from waffle_hub.hub.adapter.tx_model import TxModelHub from waffle_hub import TaskType In\u00a0[9]: Copied! <pre>TxModelHub.MODEL_TYPES\n</pre> TxModelHub.MODEL_TYPES Out[9]: <pre>{'object_detection': {'YOLOv5': ['s', 'm', 'l']},\n 'classification': {'Classifier': ['s', 'm', 'l']}}</pre> In\u00a0[10]: Copied! <pre>tx_model_hub = TxModelHub.new(\n    name=\"tx_model_mnist_detection\",\n    task=TaskType.OBJECT_DETECTION,\n    model_type=\"YOLOv5\",\n    model_size=\"s\",\n    categories=[\"1\", \"2\"]\n)\n</pre> tx_model_hub = TxModelHub.new(     name=\"tx_model_mnist_detection\",     task=TaskType.OBJECT_DETECTION,     model_type=\"YOLOv5\",     model_size=\"s\",     categories=[\"1\", \"2\"] ) In\u00a0[11]: Copied! <pre>from waffle_hub.dataset import Dataset\nfrom waffle_hub import DataType\n\ndataset = Dataset.load(\"mnist_det\")\n</pre> from waffle_hub.dataset import Dataset from waffle_hub import DataType  dataset = Dataset.load(\"mnist_det\") In\u00a0[\u00a0]: Copied! <pre>dataset_dir = dataset.export(DataType.YOLO)\ntrain_result = ultralytics_hub.train(\n    dataset_dir,\n    image_size=320,\n    epochs=50,\n    batch_size=16\n)\ntrain_result\n</pre> dataset_dir = dataset.export(DataType.YOLO) train_result = ultralytics_hub.train(     dataset_dir,     image_size=320,     epochs=50,     batch_size=16 ) train_result In\u00a0[13]: Copied! <pre>import torch\ntorch.use_deterministic_algorithms(False)\n</pre> import torch torch.use_deterministic_algorithms(False) In\u00a0[\u00a0]: Copied! <pre>dataset_dir = dataset.export(DataType.HUGGINGFACE)\ntrain_result = huggingface_hub.train(\n    dataset_dir,\n    image_size=320,\n    epochs=50,\n    batch_size=16\n)\ntrain_result\n</pre> dataset_dir = dataset.export(DataType.HUGGINGFACE) train_result = huggingface_hub.train(     dataset_dir,     image_size=320,     epochs=50,     batch_size=16 ) train_result In\u00a0[\u00a0]: Copied! <pre>dataset_dir = dataset.export(DataType.TX_MODEL)\ntrain_result = tx_model_hub.train(\n    dataset_dir,\n    image_size=320,\n    epochs=50,\n    batch_size=16,\n    pretrained_model=\"base_models/detectors/small/model.pth\"\n)\ntrain_result\n</pre> dataset_dir = dataset.export(DataType.TX_MODEL) train_result = tx_model_hub.train(     dataset_dir,     image_size=320,     epochs=50,     batch_size=16,     pretrained_model=\"base_models/detectors/small/model.pth\" ) train_result In\u00a0[16]: Copied! <pre>ultralytics_hub = UltralyticsHub.load(\"ultralytics_mnist_detection\")\nultralytics_hub.evaluate(\"mnist_det\", set_name=\"test\")\n</pre> ultralytics_hub = UltralyticsHub.load(\"ultralytics_mnist_detection\") ultralytics_hub.evaluate(\"mnist_det\", set_name=\"test\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 25.25it/s]\n</pre> Out[16]: <pre>EvaluateResult(metrics=[{'tag': 'mAP', 'value': 0.8066571950912476}])</pre> In\u00a0[17]: Copied! <pre>huggingface_hub = HuggingFaceHub.load(\"huggingface_mnist_detection\")\nhuggingface_hub.evaluate(\"mnist_det\", set_name=\"test\")\n</pre> huggingface_hub = HuggingFaceHub.load(\"huggingface_mnist_detection\") huggingface_hub.evaluate(\"mnist_det\", set_name=\"test\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 21.73it/s]\n</pre> Out[17]: <pre>EvaluateResult(metrics=[{'tag': 'mAP', 'value': 0.18526874482631683}])</pre> In\u00a0[25]: Copied! <pre>tx_model_hub = TxModelHub.load(\"tx_model_mnist_detection\")\ntx_model_hub.evaluate(\"mnist_det\", set_name=\"test\")\n</pre> tx_model_hub = TxModelHub.load(\"tx_model_mnist_detection\") tx_model_hub.evaluate(\"mnist_det\", set_name=\"test\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 25.43it/s]\n</pre> Out[25]: <pre>EvaluateResult(metrics=[{'tag': 'mAP', 'value': 0.1792079210281372}])</pre> In\u00a0[18]: Copied! <pre>import PIL.Image\n</pre> import PIL.Image In\u00a0[19]: Copied! <pre>ultralytics_hub = UltralyticsHub.load(\"ultralytics_mnist_detection\")\nultralytics_hub.inference(\"mnist/images\", draw=True)\nPIL.Image.open(ultralytics_hub.draw_dir / \"1.png\")\n</pre> ultralytics_hub = UltralyticsHub.load(\"ultralytics_mnist_detection\") ultralytics_hub.inference(\"mnist/images\", draw=True) PIL.Image.open(ultralytics_hub.draw_dir / \"1.png\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;00:00, 54.73it/s]\n</pre> Out[19]: In\u00a0[20]: Copied! <pre>huggingface_hub = HuggingFaceHub.load(\"huggingface_mnist_detection\")\nhuggingface_hub.inference(\"mnist/images\", draw=True)\nPIL.Image.open(huggingface_hub.draw_dir / \"1.png\")\n</pre> huggingface_hub = HuggingFaceHub.load(\"huggingface_mnist_detection\") huggingface_hub.inference(\"mnist/images\", draw=True) PIL.Image.open(huggingface_hub.draw_dir / \"1.png\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;00:00, 42.81it/s]\n</pre> Out[20]: In\u00a0[21]: Copied! <pre>tx_model_hub = TxModelHub.load(\"tx_model_mnist_detection\")\ntx_model_hub.inference(\"mnist/images\", draw=True)\nPIL.Image.open(tx_model_hub.draw_dir / \"1.png\")\n</pre> tx_model_hub = TxModelHub.load(\"tx_model_mnist_detection\") tx_model_hub.inference(\"mnist/images\", draw=True) PIL.Image.open(tx_model_hub.draw_dir / \"1.png\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;00:00, 62.38it/s]\n</pre> Out[21]: In\u00a0[22]: Copied! <pre>ultralytics_hub.export()\n</pre> ultralytics_hub.export() <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/ultralytics/nn/modules.py:474: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  elif self.dynamic or self.shape != shape:\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/ultralytics/yolo/utils/tal.py:241: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n  for i, stride in enumerate(strides):\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/torch/onnx/utils.py:617: UserWarning: ONNX Preprocess - Removing mutation from node aten::sub_ on block input: 'tensor.1'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:335.)\n  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n</pre> Out[22]: <pre>ExportResult(export_file=PosixPath('hubs/ultralytics_mnist_detection/weights/model.onnx'))</pre> In\u00a0[23]: Copied! <pre>huggingface_hub.export()\n</pre> huggingface_hub.export() <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py:575: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  if attn_weights.size() != (batch_size * self.num_heads, target_len, source_len):\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py:582: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  if attention_mask.size() != (batch_size, 1, target_len, source_len):\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/transformers/models/detr/modeling_detr.py:606: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  if attn_output.size() != (batch_size * self.num_heads, target_len, self.head_dim):\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/transformers/models/detr/image_processing_detr.py:1567: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n  for s, l, b in zip(scores, labels, boxes):\n</pre> Out[23]: <pre>ExportResult(export_file=PosixPath('hubs/huggingface_mnist_detection/weights/model.onnx'))</pre> In\u00a0[26]: Copied! <pre>tx_model_hub.export(device=\"cpu\")\n</pre> tx_model_hub.export(device=\"cpu\") <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/torch/onnx/utils.py:617: UserWarning: ONNX Preprocess - Removing mutation from node aten::sub_ on block input: 'tensor.1'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:335.)\n  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n</pre> Out[26]: <pre>ExportResult(export_file=PosixPath('hubs/tx_model_mnist_detection/weights/model.onnx'))</pre> In\u00a0[27]: Copied! <pre>ultralytics_hub.benchmark(image_size=320, batch_size=16)\n</pre> ultralytics_hub.benchmark(image_size=320, batch_size=16) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01&lt;00:00, 97.22it/s]\n</pre> Out[27]: <pre>{'inference_time': 1.0294418334960938,\n 'fps': 1554.2403154204742,\n 'image_size': [320, 320],\n 'batch_size': 16,\n 'precision': 'fp32',\n 'device': 'cuda:0',\n 'cpu_name': '13th Gen Intel(R) Core(TM) i9-13900KF',\n 'gpu_name': 'NVIDIA GeForce RTX 4090'}</pre> In\u00a0[28]: Copied! <pre>huggingface_hub.benchmark(image_size=320, batch_size=16)\n</pre> huggingface_hub.benchmark(image_size=320, batch_size=16) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:02&lt;00:00, 42.96it/s]\n</pre> Out[28]: <pre>{'inference_time': 2.328606128692627,\n 'fps': 687.1063252325563,\n 'image_size': [320, 320],\n 'batch_size': 16,\n 'precision': 'fp32',\n 'device': 'cuda:0',\n 'cpu_name': '13th Gen Intel(R) Core(TM) i9-13900KF',\n 'gpu_name': 'NVIDIA GeForce RTX 4090'}</pre> In\u00a0[29]: Copied! <pre>tx_model_hub.benchmark(image_size=320, batch_size=16)\n</pre> tx_model_hub.benchmark(image_size=320, batch_size=16) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 113.23it/s]\n</pre> Out[29]: <pre>{'inference_time': 0.8841898441314697,\n 'fps': 1809.5661362992278,\n 'image_size': [320, 320],\n 'batch_size': 16,\n 'precision': 'fp32',\n 'device': 'cuda:0',\n 'cpu_name': '13th Gen Intel(R) Core(TM) i9-13900KF',\n 'gpu_name': 'NVIDIA GeForce RTX 4090'}</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/using_hub/#hub","title":"Hub\u00b6","text":"<p>Hub provides same interface for several backends. Let see how it works.</p>"},{"location":"tutorials/using_hub/#create-ultralytics-hub","title":"Create Ultralytics Hub\u00b6","text":""},{"location":"tutorials/using_hub/#create-huggingface-hub","title":"Create Huggingface Hub\u00b6","text":"<p>As you did in Ultralytics Hub, you can create Huggingface Hub instance with exactly same way.</p>"},{"location":"tutorials/using_hub/#create-tx-model-hub","title":"Create Tx Model Hub\u00b6","text":"<p>Tx Model is a private deep learning framework of SNUAILAB. You can use it by asking our team!  You can also create Tx Model Hub instance with exactly same way.</p>"},{"location":"tutorials/using_hub/#train","title":"Train\u00b6","text":""},{"location":"tutorials/using_hub/#load-dataset","title":"Load Dataset\u00b6","text":""},{"location":"tutorials/using_hub/#ultralytics","title":"Ultralytics\u00b6","text":""},{"location":"tutorials/using_hub/#huggingface","title":"Huggingface\u00b6","text":""},{"location":"tutorials/using_hub/#tx-model","title":"Tx Model\u00b6","text":""},{"location":"tutorials/using_hub/#evaluate","title":"Evaluate\u00b6","text":""},{"location":"tutorials/using_hub/#inference","title":"Inference\u00b6","text":""},{"location":"tutorials/using_hub/#export-to-onnx","title":"Export to onnx\u00b6","text":""},{"location":"tutorials/using_hub/#benchmark","title":"Benchmark\u00b6","text":""},{"location":"tutorials/visualization/","title":"Visualization","text":"In\u00a0[1]: Copied! <pre>import cv2\nfrom matplotlib import pyplot as plt\n\ndef imshow(image_path):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)\n    plt.axis('off')\n</pre> import cv2 from matplotlib import pyplot as plt  def imshow(image_path):     image = cv2.imread(image_path)     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)     plt.imshow(image)     plt.axis('off')  In\u00a0[2]: Copied! <pre>imshow(\"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\")\n</pre> imshow(\"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\") In\u00a0[3]: Copied! <pre>imshow(\"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\")\n</pre> imshow(\"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\") <p>Load trained model</p> In\u00a0[4]: Copied! <pre>from waffle_hub.hub.adapter.ultralytics import UltralyticsHub\n\nultralytics_hub = UltralyticsHub.load(\"falldown_yolo\")\nultralytics_model = ultralytics_hub.get_model()\n</pre> from waffle_hub.hub.adapter.ultralytics import UltralyticsHub  ultralytics_hub = UltralyticsHub.load(\"falldown_yolo\") ultralytics_model = ultralytics_hub.get_model() <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/waffle_hub/__init__.py:56: UserWarning: \n                ultralytics 8.0.91 has not been tested.\n                We recommend you to use one of ['8.0.87']\n                \n  warnings.warn(\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/waffle_hub/__init__.py:56: UserWarning: \n                transformers 4.28.1 has not been tested.\n                We recommend you to use one of ['4.27.4']\n                \n  warnings.warn(\n</pre> <p>Hub provides several convinient functions to use trained models.</p> In\u00a0[21]: Copied! <pre>import torch\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef visualize_feature_maps(hub, image_path, target_layer):\n    model = hub.get_model()\n    load_image = hub.get_image_loader()\n    \n    image, image_info = load_image(image_path)\n    _, feature_maps = model.get_feature_maps(image.clone().unsqueeze(0), target_layer)\n\n    feature_map = list(feature_maps.values())[0]\n    feature_map = feature_map.squeeze().mean(dim=0).detach().numpy()\n\n    h, w = image.shape[1:]\n    feature_map = cv2.resize(feature_map, (w, h))\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    \n    axes[0].title.set_text(\"Input\")\n    axes[0].imshow(image.permute(1, 2, 0).detach().numpy())\n    axes[0].axis('off')\n    axes[1].title.set_text(f\"{hub.backend}: {target_layer}\")\n    axes[1].imshow(feature_map)\n    axes[1].axis('off')\n    plt.show()\n</pre> import torch import cv2 import numpy as np from matplotlib import pyplot as plt  def visualize_feature_maps(hub, image_path, target_layer):     model = hub.get_model()     load_image = hub.get_image_loader()          image, image_info = load_image(image_path)     _, feature_maps = model.get_feature_maps(image.clone().unsqueeze(0), target_layer)      feature_map = list(feature_maps.values())[0]     feature_map = feature_map.squeeze().mean(dim=0).detach().numpy()      h, w = image.shape[1:]     feature_map = cv2.resize(feature_map, (w, h))      fig, axes = plt.subplots(1, 2, figsize=(10, 5))          axes[0].title.set_text(\"Input\")     axes[0].imshow(image.permute(1, 2, 0).detach().numpy())     axes[0].axis('off')     axes[1].title.set_text(f\"{hub.backend}: {target_layer}\")     axes[1].imshow(feature_map)     axes[1].axis('off')     plt.show() In\u00a0[37]: Copied! <pre>ultralytics_layers = list(filter(lambda x: \".bn\" in x, ultralytics_model.get_layer_names()))\nultralytics_layers\n</pre> ultralytics_layers = list(filter(lambda x: \".bn\" in x, ultralytics_model.get_layer_names())) ultralytics_layers  Out[37]: <pre>['model.0.bn',\n 'model.1.bn',\n 'model.2.cv1.bn',\n 'model.2.cv2.bn',\n 'model.2.m.0.cv1.bn',\n 'model.2.m.0.cv2.bn',\n 'model.3.bn',\n 'model.4.cv1.bn',\n 'model.4.cv2.bn',\n 'model.4.m.0.cv1.bn',\n 'model.4.m.0.cv2.bn',\n 'model.4.m.1.cv1.bn',\n 'model.4.m.1.cv2.bn',\n 'model.5.bn',\n 'model.6.cv1.bn',\n 'model.6.cv2.bn',\n 'model.6.m.0.cv1.bn',\n 'model.6.m.0.cv2.bn',\n 'model.6.m.1.cv1.bn',\n 'model.6.m.1.cv2.bn',\n 'model.7.bn',\n 'model.8.cv1.bn',\n 'model.8.cv2.bn',\n 'model.8.m.0.cv1.bn',\n 'model.8.m.0.cv2.bn',\n 'model.9.conv.bn']</pre> In\u00a0[97]: Copied! <pre>visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.3.bn\")\n</pre> visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.3.bn\") In\u00a0[68]: Copied! <pre>visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.7.bn\")\n</pre> visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.7.bn\") In\u00a0[98]: Copied! <pre>visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.3.bn\")\n</pre> visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.3.bn\") In\u00a0[69]: Copied! <pre>visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.7.bn\")\n</pre> visualize_feature_maps(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.7.bn\") <p>Blend function for visualize GradCAM.</p> In\u00a0[61]: Copied! <pre>def blend(image, cam):\n\"\"\"blend image and cam (same size and normalized)\"\"\"\n    image = np.uint8(image.permute(1, 2, 0).detach().numpy() * 255)\n    cam = np.uint8(cam.detach().numpy().squeeze() * 255)\n\n    print(cam.shape)\n    cam = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n    cam = cv2.cvtColor(cam, cv2.COLOR_BGR2RGB)\n    return cv2.addWeighted(cam, 0.7, image, 0.3, 0)\n\ndef visualize_cam(hub, image_path, target_layer):\n    model = hub.get_model()\n    load_image = hub.get_image_loader()\n    \n    image, image_info = load_image(image_path)\n    cam = model.get_cam(image.clone().unsqueeze(0), target_layer)\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    \n    axes[0].title.set_text(\"Input\")\n    axes[0].imshow(image.permute(1, 2, 0).detach().numpy())\n    axes[0].axis('off')\n    axes[1].title.set_text(f\"CAM {hub.backend}: {target_layer}\")\n    axes[1].imshow(blend(image, cam))\n    axes[1].axis('off')\n    plt.show()\n</pre> def blend(image, cam):     \"\"\"blend image and cam (same size and normalized)\"\"\"     image = np.uint8(image.permute(1, 2, 0).detach().numpy() * 255)     cam = np.uint8(cam.detach().numpy().squeeze() * 255)      print(cam.shape)     cam = cv2.applyColorMap(cam, cv2.COLORMAP_JET)     cam = cv2.cvtColor(cam, cv2.COLOR_BGR2RGB)     return cv2.addWeighted(cam, 0.7, image, 0.3, 0)  def visualize_cam(hub, image_path, target_layer):     model = hub.get_model()     load_image = hub.get_image_loader()          image, image_info = load_image(image_path)     cam = model.get_cam(image.clone().unsqueeze(0), target_layer)      fig, axes = plt.subplots(1, 2, figsize=(10, 5))          axes[0].title.set_text(\"Input\")     axes[0].imshow(image.permute(1, 2, 0).detach().numpy())     axes[0].axis('off')     axes[1].title.set_text(f\"CAM {hub.backend}: {target_layer}\")     axes[1].imshow(blend(image, cam))     axes[1].axis('off')     plt.show() In\u00a0[95]: Copied! <pre>visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.3.bn\")\n</pre> visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.3.bn\") <pre>(224, 224)\n</pre> In\u00a0[\u00a0]: Copied! <pre>visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.7.bn\")\n</pre> visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"model.7.bn\") <pre>(224, 224)\n</pre> In\u00a0[96]: Copied! <pre>visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.3.bn\")\n</pre> visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.3.bn\") <pre>(224, 224)\n</pre> In\u00a0[\u00a0]: Copied! <pre>visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.7.bn\")\n</pre> visualize_cam(ultralytics_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"model.7.bn\") <pre>(224, 224)\n</pre> In\u00a0[73]: Copied! <pre>from waffle_hub.hub.adapter.hugging_face import HuggingFaceHub\n\nhuggingface_hub = HuggingFaceHub.load(\"falldown_hf_ori\")\nhuggingface_model = huggingface_hub.get_model()\nhuggingface_model.get_layer_names()\n</pre> from waffle_hub.hub.adapter.hugging_face import HuggingFaceHub  huggingface_hub = HuggingFaceHub.load(\"falldown_hf_ori\") huggingface_model = huggingface_hub.get_model() huggingface_model.get_layer_names() Out[73]: <pre>['',\n 'vit',\n 'vit.embeddings',\n 'vit.embeddings.patch_embeddings',\n 'vit.embeddings.patch_embeddings.projection',\n 'vit.embeddings.dropout',\n 'vit.encoder',\n 'vit.encoder.layer',\n 'vit.encoder.layer.0',\n 'vit.encoder.layer.0.attention',\n 'vit.encoder.layer.0.attention.attention',\n 'vit.encoder.layer.0.attention.attention.query',\n 'vit.encoder.layer.0.attention.attention.key',\n 'vit.encoder.layer.0.attention.attention.value',\n 'vit.encoder.layer.0.attention.attention.dropout',\n 'vit.encoder.layer.0.attention.output',\n 'vit.encoder.layer.0.attention.output.dense',\n 'vit.encoder.layer.0.attention.output.dropout',\n 'vit.encoder.layer.0.intermediate',\n 'vit.encoder.layer.0.intermediate.dense',\n 'vit.encoder.layer.0.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.0.output',\n 'vit.encoder.layer.0.output.dense',\n 'vit.encoder.layer.0.output.dropout',\n 'vit.encoder.layer.0.layernorm_before',\n 'vit.encoder.layer.0.layernorm_after',\n 'vit.encoder.layer.1',\n 'vit.encoder.layer.1.attention',\n 'vit.encoder.layer.1.attention.attention',\n 'vit.encoder.layer.1.attention.attention.query',\n 'vit.encoder.layer.1.attention.attention.key',\n 'vit.encoder.layer.1.attention.attention.value',\n 'vit.encoder.layer.1.attention.attention.dropout',\n 'vit.encoder.layer.1.attention.output',\n 'vit.encoder.layer.1.attention.output.dense',\n 'vit.encoder.layer.1.attention.output.dropout',\n 'vit.encoder.layer.1.intermediate',\n 'vit.encoder.layer.1.intermediate.dense',\n 'vit.encoder.layer.1.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.1.output',\n 'vit.encoder.layer.1.output.dense',\n 'vit.encoder.layer.1.output.dropout',\n 'vit.encoder.layer.1.layernorm_before',\n 'vit.encoder.layer.1.layernorm_after',\n 'vit.encoder.layer.2',\n 'vit.encoder.layer.2.attention',\n 'vit.encoder.layer.2.attention.attention',\n 'vit.encoder.layer.2.attention.attention.query',\n 'vit.encoder.layer.2.attention.attention.key',\n 'vit.encoder.layer.2.attention.attention.value',\n 'vit.encoder.layer.2.attention.attention.dropout',\n 'vit.encoder.layer.2.attention.output',\n 'vit.encoder.layer.2.attention.output.dense',\n 'vit.encoder.layer.2.attention.output.dropout',\n 'vit.encoder.layer.2.intermediate',\n 'vit.encoder.layer.2.intermediate.dense',\n 'vit.encoder.layer.2.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.2.output',\n 'vit.encoder.layer.2.output.dense',\n 'vit.encoder.layer.2.output.dropout',\n 'vit.encoder.layer.2.layernorm_before',\n 'vit.encoder.layer.2.layernorm_after',\n 'vit.encoder.layer.3',\n 'vit.encoder.layer.3.attention',\n 'vit.encoder.layer.3.attention.attention',\n 'vit.encoder.layer.3.attention.attention.query',\n 'vit.encoder.layer.3.attention.attention.key',\n 'vit.encoder.layer.3.attention.attention.value',\n 'vit.encoder.layer.3.attention.attention.dropout',\n 'vit.encoder.layer.3.attention.output',\n 'vit.encoder.layer.3.attention.output.dense',\n 'vit.encoder.layer.3.attention.output.dropout',\n 'vit.encoder.layer.3.intermediate',\n 'vit.encoder.layer.3.intermediate.dense',\n 'vit.encoder.layer.3.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.3.output',\n 'vit.encoder.layer.3.output.dense',\n 'vit.encoder.layer.3.output.dropout',\n 'vit.encoder.layer.3.layernorm_before',\n 'vit.encoder.layer.3.layernorm_after',\n 'vit.encoder.layer.4',\n 'vit.encoder.layer.4.attention',\n 'vit.encoder.layer.4.attention.attention',\n 'vit.encoder.layer.4.attention.attention.query',\n 'vit.encoder.layer.4.attention.attention.key',\n 'vit.encoder.layer.4.attention.attention.value',\n 'vit.encoder.layer.4.attention.attention.dropout',\n 'vit.encoder.layer.4.attention.output',\n 'vit.encoder.layer.4.attention.output.dense',\n 'vit.encoder.layer.4.attention.output.dropout',\n 'vit.encoder.layer.4.intermediate',\n 'vit.encoder.layer.4.intermediate.dense',\n 'vit.encoder.layer.4.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.4.output',\n 'vit.encoder.layer.4.output.dense',\n 'vit.encoder.layer.4.output.dropout',\n 'vit.encoder.layer.4.layernorm_before',\n 'vit.encoder.layer.4.layernorm_after',\n 'vit.encoder.layer.5',\n 'vit.encoder.layer.5.attention',\n 'vit.encoder.layer.5.attention.attention',\n 'vit.encoder.layer.5.attention.attention.query',\n 'vit.encoder.layer.5.attention.attention.key',\n 'vit.encoder.layer.5.attention.attention.value',\n 'vit.encoder.layer.5.attention.attention.dropout',\n 'vit.encoder.layer.5.attention.output',\n 'vit.encoder.layer.5.attention.output.dense',\n 'vit.encoder.layer.5.attention.output.dropout',\n 'vit.encoder.layer.5.intermediate',\n 'vit.encoder.layer.5.intermediate.dense',\n 'vit.encoder.layer.5.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.5.output',\n 'vit.encoder.layer.5.output.dense',\n 'vit.encoder.layer.5.output.dropout',\n 'vit.encoder.layer.5.layernorm_before',\n 'vit.encoder.layer.5.layernorm_after',\n 'vit.encoder.layer.6',\n 'vit.encoder.layer.6.attention',\n 'vit.encoder.layer.6.attention.attention',\n 'vit.encoder.layer.6.attention.attention.query',\n 'vit.encoder.layer.6.attention.attention.key',\n 'vit.encoder.layer.6.attention.attention.value',\n 'vit.encoder.layer.6.attention.attention.dropout',\n 'vit.encoder.layer.6.attention.output',\n 'vit.encoder.layer.6.attention.output.dense',\n 'vit.encoder.layer.6.attention.output.dropout',\n 'vit.encoder.layer.6.intermediate',\n 'vit.encoder.layer.6.intermediate.dense',\n 'vit.encoder.layer.6.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.6.output',\n 'vit.encoder.layer.6.output.dense',\n 'vit.encoder.layer.6.output.dropout',\n 'vit.encoder.layer.6.layernorm_before',\n 'vit.encoder.layer.6.layernorm_after',\n 'vit.encoder.layer.7',\n 'vit.encoder.layer.7.attention',\n 'vit.encoder.layer.7.attention.attention',\n 'vit.encoder.layer.7.attention.attention.query',\n 'vit.encoder.layer.7.attention.attention.key',\n 'vit.encoder.layer.7.attention.attention.value',\n 'vit.encoder.layer.7.attention.attention.dropout',\n 'vit.encoder.layer.7.attention.output',\n 'vit.encoder.layer.7.attention.output.dense',\n 'vit.encoder.layer.7.attention.output.dropout',\n 'vit.encoder.layer.7.intermediate',\n 'vit.encoder.layer.7.intermediate.dense',\n 'vit.encoder.layer.7.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.7.output',\n 'vit.encoder.layer.7.output.dense',\n 'vit.encoder.layer.7.output.dropout',\n 'vit.encoder.layer.7.layernorm_before',\n 'vit.encoder.layer.7.layernorm_after',\n 'vit.encoder.layer.8',\n 'vit.encoder.layer.8.attention',\n 'vit.encoder.layer.8.attention.attention',\n 'vit.encoder.layer.8.attention.attention.query',\n 'vit.encoder.layer.8.attention.attention.key',\n 'vit.encoder.layer.8.attention.attention.value',\n 'vit.encoder.layer.8.attention.attention.dropout',\n 'vit.encoder.layer.8.attention.output',\n 'vit.encoder.layer.8.attention.output.dense',\n 'vit.encoder.layer.8.attention.output.dropout',\n 'vit.encoder.layer.8.intermediate',\n 'vit.encoder.layer.8.intermediate.dense',\n 'vit.encoder.layer.8.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.8.output',\n 'vit.encoder.layer.8.output.dense',\n 'vit.encoder.layer.8.output.dropout',\n 'vit.encoder.layer.8.layernorm_before',\n 'vit.encoder.layer.8.layernorm_after',\n 'vit.encoder.layer.9',\n 'vit.encoder.layer.9.attention',\n 'vit.encoder.layer.9.attention.attention',\n 'vit.encoder.layer.9.attention.attention.query',\n 'vit.encoder.layer.9.attention.attention.key',\n 'vit.encoder.layer.9.attention.attention.value',\n 'vit.encoder.layer.9.attention.attention.dropout',\n 'vit.encoder.layer.9.attention.output',\n 'vit.encoder.layer.9.attention.output.dense',\n 'vit.encoder.layer.9.attention.output.dropout',\n 'vit.encoder.layer.9.intermediate',\n 'vit.encoder.layer.9.intermediate.dense',\n 'vit.encoder.layer.9.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.9.output',\n 'vit.encoder.layer.9.output.dense',\n 'vit.encoder.layer.9.output.dropout',\n 'vit.encoder.layer.9.layernorm_before',\n 'vit.encoder.layer.9.layernorm_after',\n 'vit.encoder.layer.10',\n 'vit.encoder.layer.10.attention',\n 'vit.encoder.layer.10.attention.attention',\n 'vit.encoder.layer.10.attention.attention.query',\n 'vit.encoder.layer.10.attention.attention.key',\n 'vit.encoder.layer.10.attention.attention.value',\n 'vit.encoder.layer.10.attention.attention.dropout',\n 'vit.encoder.layer.10.attention.output',\n 'vit.encoder.layer.10.attention.output.dense',\n 'vit.encoder.layer.10.attention.output.dropout',\n 'vit.encoder.layer.10.intermediate',\n 'vit.encoder.layer.10.intermediate.dense',\n 'vit.encoder.layer.10.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.10.output',\n 'vit.encoder.layer.10.output.dense',\n 'vit.encoder.layer.10.output.dropout',\n 'vit.encoder.layer.10.layernorm_before',\n 'vit.encoder.layer.10.layernorm_after',\n 'vit.encoder.layer.11',\n 'vit.encoder.layer.11.attention',\n 'vit.encoder.layer.11.attention.attention',\n 'vit.encoder.layer.11.attention.attention.query',\n 'vit.encoder.layer.11.attention.attention.key',\n 'vit.encoder.layer.11.attention.attention.value',\n 'vit.encoder.layer.11.attention.attention.dropout',\n 'vit.encoder.layer.11.attention.output',\n 'vit.encoder.layer.11.attention.output.dense',\n 'vit.encoder.layer.11.attention.output.dropout',\n 'vit.encoder.layer.11.intermediate',\n 'vit.encoder.layer.11.intermediate.dense',\n 'vit.encoder.layer.11.intermediate.intermediate_act_fn',\n 'vit.encoder.layer.11.output',\n 'vit.encoder.layer.11.output.dense',\n 'vit.encoder.layer.11.output.dropout',\n 'vit.encoder.layer.11.layernorm_before',\n 'vit.encoder.layer.11.layernorm_after',\n 'vit.layernorm',\n 'classifier']</pre> In\u00a0[89]: Copied! <pre>visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"vit.encoder.layer.2.layernorm_after\")\n</pre> visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"vit.encoder.layer.2.layernorm_after\") <pre>(224, 224)\n</pre> In\u00a0[91]: Copied! <pre>visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"vit.encoder.layer.10.layernorm_after\")\n</pre> visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/falldown/23_1080_4000kbps/622_frame_4080.jpg\", \"vit.encoder.layer.10.layernorm_after\") <pre>(224, 224)\n</pre> In\u00a0[90]: Copied! <pre>visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"vit.encoder.layer.2.layernorm_after\")\n</pre> visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"vit.encoder.layer.2.layernorm_after\") <pre>(224, 224)\n</pre> In\u00a0[92]: Copied! <pre>visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"vit.encoder.layer.10.layernorm_after\")\n</pre> visualize_cam(huggingface_hub, \"datasets/falldown/exports/YOLO/test/none/22_1080_4000kbps/256_frame_5250.jpg\", \"vit.encoder.layer.10.layernorm_after\") <pre>(224, 224)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/visualization/#visualization-in-deep-learning","title":"Visualization in Deep Learning\u00b6","text":"<p>Visualization is a powerful tool for understanding and interpreting machine learning models. There are many different ways to visualize models, including visualizing the learned features, visualizing the activations, and visualizing the output of intermediate layers. </p> <p>In this tutorial, we will cover the following topics: </p> <ol> <li>Load trained model from Hub</li> <li>Visualizing the learned features with Hub</li> <li>Visualizing the class activation with Hub</li> </ol>"},{"location":"tutorials/visualization/#load-trained-model-from-hub","title":"Load trained model from Hub\u00b6","text":"<p>In this tutorial, we'll use our custom <code>Safety Dataset</code> that contains two classes: <code>Falldown</code> and <code>NotFalldown</code>. </p>"},{"location":"tutorials/visualization/#visualizing-the-learned-features-with-hub","title":"Visualizing the learned features with Hub\u00b6","text":""},{"location":"tutorials/visualization/#visualizing-the-class-activation-with-hub","title":"Visualizing the class activation with Hub\u00b6","text":""},{"location":"tutorials/visualization/#gradcam-with-ultralytics-hub","title":"GradCAM with Ultralytics Hub\u00b6","text":""},{"location":"tutorials/visualization/#gradcam-with-huggingface-hub","title":"GradCAM with Huggingface Hub\u00b6","text":"<p>We also trained ViT-base model via Huggingface Hub.</p>"},{"location":"tutorials/dataset/","title":"Index","text":"<p>You can do following things with this Waffle Dataset:</p> <ul> <li>Import</li> <li>Split</li> <li>Export</li> </ul> <p>See Waffle Dataset for more details.</p>"},{"location":"tutorials/dataset/export/","title":"Export","text":"In\u00a0[2]: Copied! <pre>from waffle_hub.dataset import Dataset\n\ndataset = Dataset.load(name=\"mnist_coco\")\n</pre> from waffle_hub.dataset import Dataset  dataset = Dataset.load(name=\"mnist_coco\") <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[4]: Copied! <pre>dataset.export(\n    \"coco\"\n)\n</pre> dataset.export(     \"coco\" ) Out[4]: <pre>'datasets/mnist_coco/exports/COCO'</pre> In\u00a0[5]: Copied! <pre>dataset.export(\n    \"yolo\"\n)\n</pre> dataset.export(     \"yolo\" ) Out[5]: <pre>'datasets/mnist_coco/exports/YOLO'</pre> In\u00a0[7]: Copied! <pre>dataset.export(\n    \"transformers\"\n)\n</pre> dataset.export(     \"transformers\" ) <pre>Downloading and preparing dataset generator/default to /home/lhj/.cache/huggingface/datasets/generator/default-601358b2253d9047/0.0.0...\n</pre> <pre>                                                              \r</pre> <pre>Dataset generator downloaded and prepared to /home/lhj/.cache/huggingface/datasets/generator/default-601358b2253d9047/0.0.0. Subsequent calls will reuse this data.\nDownloading and preparing dataset generator/default to /home/lhj/.cache/huggingface/datasets/generator/default-048941841741712d/0.0.0...\n</pre> <pre>Found cached dataset generator (/home/lhj/.cache/huggingface/datasets/generator/default-048941841741712d/0.0.0)\n</pre> <pre>Dataset generator downloaded and prepared to /home/lhj/.cache/huggingface/datasets/generator/default-048941841741712d/0.0.0. Subsequent calls will reuse this data.\n</pre> <pre>                                                                                          \r</pre> Out[7]: <pre>'datasets/mnist_coco/exports/TRANSFORMERS'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/dataset/export/#export-dataset","title":"Export Dataset\u00b6","text":"<p>After you split your dataset, you can export it to specific format.</p>"},{"location":"tutorials/dataset/export/#load-dataset","title":"Load Dataset\u00b6","text":""},{"location":"tutorials/dataset/export/#export-dataset","title":"Export Dataset\u00b6","text":""},{"location":"tutorials/dataset/import/","title":"Import","text":"In\u00a0[1]: Copied! <pre>!wget https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist.zip\n!unzip mnist.zip -d coco\n</pre> !wget https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist.zip !unzip mnist.zip -d coco <pre>--2023-06-26 13:23:48--  https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist.zip\nResolving github.com (github.com)... 20.200.245.247\nConnecting to github.com (github.com)|20.200.245.247|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/snuailab/assets/main/waffle/sample_dataset/mnist.zip [following]\n--2023-06-26 13:23:48--  https://raw.githubusercontent.com/snuailab/assets/main/waffle/sample_dataset/mnist.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 157823 (154K) [application/zip]\nSaving to: \u2018mnist.zip\u2019\n\nmnist.zip           100%[===================&gt;] 154.12K  --.-KB/s    in 0.02s   \n\n2023-06-26 13:23:49 (8.20 MB/s) - \u2018mnist.zip\u2019 saved [157823/157823]\n\nArchive:  mnist.zip\n  inflating: coco/coco.json          \n   creating: coco/images/\n  inflating: coco/images/1.png       \n  inflating: coco/images/10.png      \n  inflating: coco/images/100.png     \n  inflating: coco/images/11.png      \n  inflating: coco/images/12.png      \n  inflating: coco/images/13.png      \n  inflating: coco/images/14.png      \n  inflating: coco/images/15.png      \n  inflating: coco/images/16.png      \n  inflating: coco/images/17.png      \n  inflating: coco/images/18.png      \n  inflating: coco/images/19.png      \n  inflating: coco/images/2.png       \n  inflating: coco/images/20.png      \n  inflating: coco/images/21.png      \n  inflating: coco/images/22.png      \n  inflating: coco/images/23.png      \n  inflating: coco/images/24.png      \n  inflating: coco/images/25.png      \n  inflating: coco/images/26.png      \n  inflating: coco/images/27.png      \n  inflating: coco/images/28.png      \n  inflating: coco/images/29.png      \n  inflating: coco/images/3.png       \n  inflating: coco/images/30.png      \n  inflating: coco/images/31.png      \n  inflating: coco/images/32.png      \n  inflating: coco/images/33.png      \n  inflating: coco/images/34.png      \n  inflating: coco/images/35.png      \n  inflating: coco/images/36.png      \n  inflating: coco/images/37.png      \n  inflating: coco/images/38.png      \n  inflating: coco/images/39.png      \n  inflating: coco/images/4.png       \n  inflating: coco/images/40.png      \n  inflating: coco/images/41.png      \n  inflating: coco/images/42.png      \n  inflating: coco/images/43.png      \n  inflating: coco/images/44.png      \n  inflating: coco/images/45.png      \n  inflating: coco/images/46.png      \n  inflating: coco/images/47.png      \n  inflating: coco/images/48.png      \n  inflating: coco/images/49.png      \n  inflating: coco/images/5.png       \n  inflating: coco/images/50.png      \n  inflating: coco/images/51.png      \n  inflating: coco/images/52.png      \n  inflating: coco/images/53.png      \n  inflating: coco/images/54.png      \n  inflating: coco/images/55.png      \n  inflating: coco/images/56.png      \n  inflating: coco/images/57.png      \n  inflating: coco/images/58.png      \n  inflating: coco/images/59.png      \n  inflating: coco/images/6.png       \n  inflating: coco/images/60.png      \n  inflating: coco/images/61.png      \n  inflating: coco/images/62.png      \n  inflating: coco/images/63.png      \n  inflating: coco/images/64.png      \n  inflating: coco/images/65.png      \n  inflating: coco/images/66.png      \n  inflating: coco/images/67.png      \n  inflating: coco/images/68.png      \n  inflating: coco/images/69.png      \n  inflating: coco/images/7.png       \n  inflating: coco/images/70.png      \n  inflating: coco/images/71.png      \n  inflating: coco/images/72.png      \n  inflating: coco/images/73.png      \n  inflating: coco/images/74.png      \n  inflating: coco/images/75.png      \n  inflating: coco/images/76.png      \n  inflating: coco/images/77.png      \n  inflating: coco/images/78.png      \n  inflating: coco/images/79.png      \n  inflating: coco/images/8.png       \n  inflating: coco/images/80.png      \n  inflating: coco/images/81.png      \n  inflating: coco/images/82.png      \n  inflating: coco/images/83.png      \n  inflating: coco/images/84.png      \n  inflating: coco/images/85.png      \n  inflating: coco/images/86.png      \n  inflating: coco/images/87.png      \n  inflating: coco/images/88.png      \n  inflating: coco/images/89.png      \n  inflating: coco/images/9.png       \n  inflating: coco/images/90.png      \n  inflating: coco/images/91.png      \n  inflating: coco/images/92.png      \n  inflating: coco/images/93.png      \n  inflating: coco/images/94.png      \n  inflating: coco/images/95.png      \n  inflating: coco/images/96.png      \n  inflating: coco/images/97.png      \n  inflating: coco/images/98.png      \n  inflating: coco/images/99.png      \n  inflating: coco/test.json          \n  inflating: coco/train.json         \n  inflating: coco/val.json           \n</pre> In\u00a0[\u00a0]: Copied! <pre>from waffle_hub.dataset import Dataset\n\nDataset.from_coco(\n    name=\"mnist_coco\",\n    task=\"object_detection\",\n    coco_file=\"coco/coco.json\",\n    coco_root_dir=\"coco/images\",\n)\n</pre> from waffle_hub.dataset import Dataset  Dataset.from_coco(     name=\"mnist_coco\",     task=\"object_detection\",     coco_file=\"coco/coco.json\",     coco_root_dir=\"coco/images\", ) <pre>loading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n</pre> <pre>1it [00:00, 52.54it/s]:   0%|          | 0/100 [00:00&lt;?, ?it/s]\nImporting coco dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 5002.57it/s]\n</pre> <pre>DatasetInfo(name='mnist_coco', task='OBJECT_DETECTION', created='2023-06-26 13:25:26')</pre> In\u00a0[5]: Copied! <pre>!wget https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist_huggingface_detection.zip\n!unzip mnist_huggingface_detection.zip -d huggingface\n</pre> !wget https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist_huggingface_detection.zip !unzip mnist_huggingface_detection.zip -d huggingface <pre>--2023-06-26 13:29:44--  https://github.com/snuailab/assets/raw/main/waffle/sample_dataset/mnist_huggingface_detection.zip\nResolving github.com (github.com)... 20.200.245.247\nConnecting to github.com (github.com)|20.200.245.247|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/snuailab/assets/main/waffle/sample_dataset/mnist_huggingface_detection.zip [following]\n--2023-06-26 13:29:44--  https://raw.githubusercontent.com/snuailab/assets/main/waffle/sample_dataset/mnist_huggingface_detection.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 226268 (221K) [application/zip]\nSaving to: \u2018mnist_huggingface_detection.zip\u2019\n\nmnist_huggingface_d 100%[===================&gt;] 220.96K  --.-KB/s    in 0.03s   \n\n2023-06-26 13:29:45 (7.64 MB/s) - \u2018mnist_huggingface_detection.zip\u2019 saved [226268/226268]\n\nArchive:  mnist_huggingface_detection.zip\n  inflating: huggingface/dataset_dict.json  \n   creating: huggingface/test/\n  inflating: huggingface/test/data-00000-of-00001.arrow  \n  inflating: huggingface/test/dataset_info.json  \n  inflating: huggingface/test/state.json  \n   creating: huggingface/train/\n  inflating: huggingface/train/data-00000-of-00001.arrow  \n  inflating: huggingface/train/dataset_info.json  \n  inflating: huggingface/train/state.json  \n   creating: huggingface/val/\n  inflating: huggingface/val/data-00000-of-00001.arrow  \n  inflating: huggingface/val/dataset_info.json  \n  inflating: huggingface/val/state.json  \n</pre> In\u00a0[8]: Copied! <pre>from waffle_hub.dataset import Dataset\n\nDataset.from_transformers(\n    name=\"mnist_transformers\",\n    task=\"object_detection\",\n    dataset_dir=\"huggingface\"\n)\n</pre> from waffle_hub.dataset import Dataset  Dataset.from_transformers(     name=\"mnist_transformers\",     task=\"object_detection\",     dataset_dir=\"huggingface\" ) Out[8]: <pre>DatasetInfo(name='mnist_transformers', task='OBJECT_DETECTION', created='2023-06-26 13:30:30')</pre>"},{"location":"tutorials/dataset/import/#import-your-dataset-to-waffle-dataset","title":"Import Your Dataset to Waffle Dataset\u00b6","text":"<p>You may have your own dataset such as coco, yolo, huggingface(transformers). You can simply import your dataset to Waffle Dataset using <code>from_{format}</code> function.</p> <p>We provide the sample dataset for this tutorial. Follow the below steps!</p>"},{"location":"tutorials/dataset/import/#coco-format","title":"COCO Format\u00b6","text":""},{"location":"tutorials/dataset/import/#transformers-huggingface-format","title":"Transformers (Huggingface) Format\u00b6","text":""},{"location":"tutorials/dataset/import/#yolo-format-comming-soon","title":"YOLO Format (Comming Soon)\u00b6","text":""},{"location":"tutorials/dataset/split/","title":"Split","text":"In\u00a0[1]: Copied! <pre>from waffle_hub.dataset import Dataset\n\ndataset = Dataset.load(name=\"mnist_coco\")\n</pre> from waffle_hub.dataset import Dataset  dataset = Dataset.load(name=\"mnist_coco\") <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre>dataset.split(train_ratio=0.8, val_ratio=0.2)\ntrain_ids, val_ids, test_ids, unlabeled_ids = dataset.get_split_ids()\n</pre> dataset.split(train_ratio=0.8, val_ratio=0.2) train_ids, val_ids, test_ids, unlabeled_ids = dataset.get_split_ids() In\u00a0[4]: Copied! <pre>len(train_ids), len(val_ids), len(test_ids), len(unlabeled_ids)\n</pre> len(train_ids), len(val_ids), len(test_ids), len(unlabeled_ids) Out[4]: <pre>(79, 21, 21, 0)</pre>"},{"location":"tutorials/dataset/split/#split-dataset","title":"Split Dataset\u00b6","text":"<p>After you import your dataset, you can split it into train, validation and test sets.</p>"},{"location":"tutorials/dataset/split/#load-dataset","title":"Load Dataset\u00b6","text":""},{"location":"tutorials/dataset/split/#split-dataset","title":"Split Dataset\u00b6","text":""},{"location":"tutorials/hub/","title":"Index","text":"<p>You can do following things with this Waffle Hub:</p> <ul> <li>Create</li> <li>Train</li> <li>Inference</li> <li>Export</li> </ul> <p>See Waffle Hub for more details.</p>"},{"location":"tutorials/hub/create/","title":"Create","text":"In\u00a0[1]: Copied! <pre>from waffle_hub.hub import Hub\n\nHub.get_available_backends()\n</pre> from waffle_hub.hub import Hub  Hub.get_available_backends() <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> Out[1]: <pre>['ultralytics', 'autocare_dlt', 'transformers']</pre> In\u00a0[2]: Copied! <pre>Hub.get_available_tasks(\"ultralytics\")\n</pre> Hub.get_available_tasks(\"ultralytics\") Out[2]: <pre>[OBJECT_DETECTION, CLASSIFICATION, INSTANCE_SEGMENTATION]</pre> In\u00a0[4]: Copied! <pre>Hub.get_available_model_types(\"ultralytics\", \"OBJECT_DETECTION\")\n</pre> Hub.get_available_model_types(\"ultralytics\", \"OBJECT_DETECTION\") Out[4]: <pre>['yolov8']</pre> In\u00a0[5]: Copied! <pre>Hub.get_available_model_sizes(\"ultralytics\", \"OBJECT_DETECTION\", \"yolov8\")\n</pre> Hub.get_available_model_sizes(\"ultralytics\", \"OBJECT_DETECTION\", \"yolov8\") Out[5]: <pre>['n', 's', 'm', 'l', 'x']</pre> In\u00a0[6]: Copied! <pre>Hub.new(\n    name=\"detector\",\n    backend=\"ultralytics\",\n    task=\"OBJECT_DETECTION\",\n    model_type=\"yolov8\",\n    model_size=\"n\",\n    categories=[\"1\", \"2\"]\n)\n</pre> Hub.new(     name=\"detector\",     backend=\"ultralytics\",     task=\"OBJECT_DETECTION\",     model_type=\"yolov8\",     model_size=\"n\",     categories=[\"1\", \"2\"] ) Out[6]: <pre>ModelConfig(name='detector', backend='ultralytics', version='8.0.112', task='OBJECT_DETECTION', model_type='yolov8', model_size='n', categories=[{'supercategory': 'object', 'name': '1'}, {'supercategory': 'object', 'name': '2'}])</pre>"},{"location":"tutorials/hub/create/#hub","title":"Hub\u00b6","text":"<p>Hub provide a unified interface for training, evaluating, exporting and benchmarking. You can create a hub by <code>Hub.new</code> function.</p>"},{"location":"tutorials/hub/create/#select-backend","title":"Select Backend\u00b6","text":""},{"location":"tutorials/hub/create/#create-hub","title":"Create Hub\u00b6","text":""},{"location":"tutorials/hub/export/","title":"Export","text":"In\u00a0[1]: Copied! <pre>from waffle_hub.hub import Hub\n\nhub = Hub.load(\"detector\")\n</pre> from waffle_hub.hub import Hub  hub = Hub.load(\"detector\") <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>hub.export()\n</pre> hub.export() <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/ultralytics/nn/modules/head.py:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  elif self.dynamic or self.shape != shape:\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/ultralytics/yolo/utils/tal.py:251: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n  for i, stride in enumerate(strides):\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/torch/onnx/utils.py:617: UserWarning: ONNX Preprocess - Removing mutation from node aten::sub_ on block input: 'tensor.1'. This changes graph semantics. (Triggered internally at ../torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp:335.)\n  _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)\n</pre> Out[2]: <pre>ExportResult(export_file=PosixPath('hubs/detector/weights/model.onnx'))</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/hub/export/#export","title":"Export\u00b6","text":"<p>You can simply export model to ONNX format by calling <code>export</code> method.</p>"},{"location":"tutorials/hub/export/#load-trained-hub","title":"Load trained hub\u00b6","text":""},{"location":"tutorials/hub/inference/","title":"Inference","text":"In\u00a0[1]: Copied! <pre>from waffle_hub.hub import Hub\n\nhub = Hub.load(\"detector\")\n</pre> from waffle_hub.hub import Hub  hub = Hub.load(\"detector\") <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>hub.inference(\"datasets/sample_dataset/raw\", draw=True)\n</pre> hub.inference(\"datasets/sample_dataset/raw\", draw=True) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:01&lt;00:00, 21.83it/s]\n</pre> Out[2]: <pre>InferenceResult(predictions=[{'64.png': [{'category_id': 1, 'bbox': [15.699748992919922, 221.29031372070312, 30.880027770996094, 41.581939697265625], 'area': 1284.0514526234474, 'iscrowd': 0, 'score': 0.28791025280952454}]}, {'16.png': [{'category_id': 2, 'bbox': [217.4972381591797, 117.44782257080078, 37.26664733886719, 38.72087860107422], 'area': 1442.997327477322, 'iscrowd': 0, 'score': 0.26117798686027527}]}, {'57.png': []}, {'49.png': [{'category_id': 1, 'bbox': [204.57733154296875, 40.964134216308594, 25.1239013671875, 45.170310974121094], 'area': 1134.8544376390055, 'iscrowd': 0, 'score': 0.3347718417644501}]}, {'12.png': [{'category_id': 2, 'bbox': [115.34486389160156, 154.91452026367188, 46.30793762207031, 47.6512451171875], 'area': 2206.6308865007013, 'iscrowd': 0, 'score': 0.3162873685359955}]}, {'1.png': []}, {'62.png': []}, {'25.png': []}, {'31.png': [{'category_id': 2, 'bbox': [239.40914916992188, 101.19197082519531, 38.38958740234375, 36.43768310546875], 'area': 1398.8276203162968, 'iscrowd': 0, 'score': 0.4278999865055084}]}, {'75.png': []}, {'28.png': []}, {'68.png': [{'category_id': 2, 'bbox': [55.332733154296875, 210.92510986328125, 30.071205139160156, 43.10931396484375], 'area': 1296.349023645278, 'iscrowd': 0, 'score': 0.2601146996021271}]}, {'78.png': []}, {'21.png': []}, {'24.png': []}, {'48.png': []}, {'66.png': []}, {'29.png': []}, {'17.png': []}, {'72.png': []}, {'26.png': []}, {'83.png': [{'category_id': 2, 'bbox': [167.5634765625, 163.506591796875, 50.10844421386719, 39.64286804199219], 'area': 1986.4424417598639, 'iscrowd': 0, 'score': 0.5714513659477234}]}, {'34.png': []}, {'74.png': [{'category_id': 1, 'bbox': [37.420570373535156, 89.76423645019531, 27.22539520263672, 48.70039367675781], 'area': 1325.8874643737217, 'iscrowd': 0, 'score': 0.2839807868003845}]}, {'81.png': []}, {'53.png': []}, {'86.png': [{'category_id': 2, 'bbox': [195.80435180664062, 130.57034301757812, 43.819427490234375, 43.37945556640625], 'area': 1900.8629077579826, 'iscrowd': 0, 'score': 0.4105566143989563}]}, {'100.png': []}, {'13.png': []}, {'93.png': []}, {'50.png': []}, {'46.png': [{'category_id': 2, 'bbox': [123.13134765625, 160.3269805908203, 37.01921081542969, 36.95628356933594], 'area': 1368.0924524080474, 'iscrowd': 0, 'score': 0.254962682723999}]}, {'87.png': []}, {'99.png': []}, {'70.png': []}, {'23.png': []}, {'39.png': []}, {'82.png': [{'category_id': 2, 'bbox': [211.895263671875, 78.58123779296875, 50.189361572265625, 49.73396301269531], 'area': 2496.11585206585, 'iscrowd': 0, 'score': 0.27707433700561523}]}, {'76.png': []}, {'89.png': [{'category_id': 2, 'bbox': [178.29513549804688, 89.27041625976562, 51.0772705078125, 51.00230407714844], 'area': 2605.058481870219, 'iscrowd': 0, 'score': 0.30976563692092896}]}, {'43.png': []}, {'90.png': []}, {'3.png': []}, {'73.png': []}, {'79.png': [{'category_id': 2, 'bbox': [18.207271575927734, 148.6448516845703, 40.750587463378906, 31.612564086914062], 'area': 1288.2305577654624, 'iscrowd': 0, 'score': 0.2868784964084625}]}, {'19.png': [{'category_id': 1, 'bbox': [60.22168731689453, 132.01463317871094, 30.809982299804688, 42.731201171875], 'area': 1316.547551754862, 'iscrowd': 0, 'score': 0.2698998749256134}]}, {'37.png': []}, {'59.png': []}, {'55.png': []}, {'51.png': [{'category_id': 2, 'bbox': [189.66282653808594, 138.12823486328125, 40.666748046875, 37.56011962890625], 'area': 1527.4479215592146, 'iscrowd': 0, 'score': 0.27740681171417236}]}, {'84.png': []}, {'35.png': []}, {'22.png': []}, {'27.png': []}, {'32.png': [{'category_id': 2, 'bbox': [13.526335716247559, 170.05580139160156, 41.24719715118408, 50.27195739746094], 'area': 2073.5773379489983, 'iscrowd': 0, 'score': 0.2735309898853302}]}, {'56.png': []}, {'10.png': [{'category_id': 2, 'bbox': [73.55338287353516, 149.84918212890625, 40.37998962402344, 40.407684326171875], 'area': 1631.6618738216348, 'iscrowd': 0, 'score': 0.2940123975276947}]}, {'33.png': [{'category_id': 2, 'bbox': [146.58322143554688, 50.57087707519531, 51.0399169921875, 50.615455627441406], 'area': 2583.408653746359, 'iscrowd': 0, 'score': 0.3443734347820282}]}, {'97.png': []}, {'9.png': []}, {'36.png': []}, {'52.png': [{'category_id': 1, 'bbox': [56.94493865966797, 126.25347900390625, 30.93561553955078, 46.59068298339844], 'area': 1441.3114564995049, 'iscrowd': 0, 'score': 0.2802749574184418}]}, {'77.png': []}, {'42.png': [{'category_id': 2, 'bbox': [22.972209930419922, 153.0837860107422, 51.191097259521484, 49.96095275878906], 'area': 2557.555991853529, 'iscrowd': 0, 'score': 0.4557249844074249}]}, {'58.png': []}, {'20.png': []}, {'98.png': []}, {'45.png': []}, {'95.png': [{'category_id': 2, 'bbox': [135.7186279296875, 185.03683471679688, 48.518463134765625, 49.50962829589844], 'area': 2402.1310752904974, 'iscrowd': 0, 'score': 0.4621478319168091}]}, {'63.png': []}, {'60.png': []}, {'7.png': []}, {'91.png': []}, {'71.png': [{'category_id': 2, 'bbox': [116.2231674194336, 156.55174255371094, 39.835914611816406, 34.575836181640625], 'area': 1377.3600577639882, 'iscrowd': 0, 'score': 0.28209322690963745}]}, {'54.png': []}, {'61.png': [{'category_id': 1, 'bbox': [162.65049743652344, 32.24998092651367, 28.676223754882812, 46.46525955200195], 'area': 1332.448179741914, 'iscrowd': 0, 'score': 0.3702806830406189}]}, {'67.png': []}, {'47.png': []}, {'15.png': [{'category_id': 2, 'bbox': [239.67098999023438, 52.437110900878906, 41.602020263671875, 36.334739685058594], 'area': 1511.5985766530503, 'iscrowd': 0, 'score': 0.4141892194747925}]}, {'41.png': [{'category_id': 2, 'bbox': [176.78184509277344, 182.23760986328125, 44.89263916015625, 39.39543151855469], 'area': 1768.564891721122, 'iscrowd': 0, 'score': 0.5122777223587036}]}, {'92.png': []}, {'44.png': [{'category_id': 2, 'bbox': [72.23949432373047, 244.91317749023438, 42.879432678222656, 43.69970703125], 'area': 1873.8186457045376, 'iscrowd': 0, 'score': 0.49187156558036804}]}, {'94.png': []}, {'5.png': []}, {'80.png': []}, {'85.png': []}, {'4.png': [{'category_id': 2, 'bbox': [120.52671813964844, 159.70941162109375, 41.58671569824219, 46.9268798828125], 'area': 1951.534812292084, 'iscrowd': 0, 'score': 0.3688421845436096}]}, {'96.png': []}, {'88.png': []}, {'11.png': []}, {'40.png': []}, {'69.png': [{'category_id': 2, 'bbox': [94.79696655273438, 170.20692443847656, 50.76220703125, 51.12968444824219], 'area': 2595.4556274041533, 'iscrowd': 0, 'score': 0.31696584820747375}]}, {'6.png': [{'category_id': 2, 'bbox': [29.867534637451172, 140.296875, 49.5589714050293, 47.301971435546875], 'area': 2344.23704977578, 'iscrowd': 0, 'score': 0.4115096628665924}]}, {'65.png': [{'category_id': 2, 'bbox': [201.6232452392578, 200.9598388671875, 44.87451171875, 30.996322631835938], 'area': 1390.9448431804776, 'iscrowd': 0, 'score': 0.4535251557826996}]}, {'30.png': []}, {'18.png': [{'category_id': 2, 'bbox': [163.74658203125, 88.21577453613281, 45.591949462890625, 46.23143005371094], 'area': 2107.781022605952, 'iscrowd': 0, 'score': 0.3151126205921173}]}, {'2.png': []}, {'14.png': []}, {'8.png': []}, {'38.png': [{'category_id': 1, 'bbox': [88.44695281982422, 245.02532958984375, 35.15911102294922, 43.525665283203125], 'area': 1530.3236980398651, 'iscrowd': 0, 'score': 0.30850347876548767}]}], draw_dir=PosixPath('hubs/detector/inferences/draws'))</pre> In\u00a0[4]: Copied! <pre>import PIL.Image\n\nresults = list(hub.draw_dir.iterdir())\n</pre> import PIL.Image  results = list(hub.draw_dir.iterdir())  In\u00a0[5]: Copied! <pre>PIL.Image.open(results[0])\n</pre> PIL.Image.open(results[0]) Out[5]: In\u00a0[6]: Copied! <pre>PIL.Image.open(results[1])\n</pre> PIL.Image.open(results[1]) Out[6]: In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/hub/inference/#inference","title":"Inference\u00b6","text":"<p>You can simply inference a hub by calling <code>inference</code> method of <code>Hub</code> class.</p>"},{"location":"tutorials/hub/inference/#load-trained-hub","title":"Load trained hub\u00b6","text":""},{"location":"tutorials/hub/train/","title":"Train","text":"In\u00a0[1]: Copied! <pre>from waffle_hub.hub import Hub\n\nhub = Hub.new(\n    name=\"detector\",\n    backend=\"ultralytics\",\n    task=\"OBJECT_DETECTION\",\n    model_type=\"yolov8\",\n    model_size=\"n\",\n    categories=[\"1\", \"2\"]\n)\n</pre> from waffle_hub.hub import Hub  hub = Hub.new(     name=\"detector\",     backend=\"ultralytics\",     task=\"OBJECT_DETECTION\",     model_type=\"yolov8\",     model_size=\"n\",     categories=[\"1\", \"2\"] ) <pre>/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>hub = Hub.load(\"detector\")\n</pre> hub = Hub.load(\"detector\") In\u00a0[3]: Copied! <pre>hub\n</pre> hub Out[3]: <pre>ModelConfig(name='detector', backend='ultralytics', version='8.0.112', task='OBJECT_DETECTION', model_type='yolov8', model_size='n', categories=[{'supercategory': 'object', 'name': '1'}, {'supercategory': 'object', 'name': '2'}])</pre> In\u00a0[5]: Copied! <pre># Use sample dataset for this tutorial\nfrom waffle_hub.dataset import Dataset\n\ndataset = Dataset.sample(\"sample_dataset\", task=\"object_detection\")\n</pre> # Use sample dataset for this tutorial from waffle_hub.dataset import Dataset  dataset = Dataset.sample(\"sample_dataset\", task=\"object_detection\") <pre>loading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n</pre> <pre>1it [00:00, 59.89it/s]:   0%|          | 0/100 [00:00&lt;?, ?it/s]\nImporting coco dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 5698.47it/s]\n</pre> In\u00a0[6]: Copied! <pre>dataset.split(0.8, 0.2)\n</pre> dataset.split(0.8, 0.2) In\u00a0[7]: Copied! <pre>hub.train(dataset, epochs=50)\n</pre> hub.train(dataset, epochs=50) <pre>New https://pypi.org/project/ultralytics/8.0.123 available \ud83d\ude03 Update with 'pip install -U ultralytics'\nUltralytics YOLOv8.0.112 \ud83d\ude80 Python-3.9.16 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24215MiB)\nWARNING \u26a0\ufe0f Upgrade to torch&gt;=2.0.0 for deterministic training.\nyolo/engine/trainer: task=detect, mode=train, model=yolov8n.pt, data=/home/lhj/ws/release/waffle/docs/tutorials/hub/datasets/sample_dataset/exports/YOLO/data.yaml, epochs=50, patience=50, batch=64, imgsz=[640, 640], save=True, save_period=-1, cache=False, device=0, workers=2, project=hubs/detector, name=artifacts, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=True, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=hubs/detector/artifacts\nOverriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \nModel summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nTensorBoard: Start with 'tensorboard --logdir hubs/detector/artifacts', view at http://localhost:6006/\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed \u2705\nWARNING \u26a0\ufe0f updating to 'imgsz=640'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\noptimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\ntrain: Scanning /home/lhj/ws/release/waffle/docs/tutorials/hub/datasets/sample_dataset/exports/YOLO/train/labels... 79 images, 0 backgrounds, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [00:00&lt;00:00, 6366.12it/s]\ntrain: New cache created: /home/lhj/ws/release/waffle/docs/tutorials/hub/datasets/sample_dataset/exports/YOLO/train/labels.cache\nalbumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\nWARNING \u26a0\ufe0f 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\nval: Scanning /home/lhj/ws/release/waffle/docs/tutorials/hub/datasets/sample_dataset/exports/YOLO/val/labels... 21 images, 0 backgrounds, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 21/21 [00:00&lt;00:00, 4837.72it/s]\nval: New cache created: /home/lhj/ws/release/waffle/docs/tutorials/hub/datasets/sample_dataset/exports/YOLO/val/labels.cache\nPlotting labels to hubs/detector/artifacts/labels.jpg... \nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to hubs/detector/artifacts\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/50      8.05G       1.35      4.646      1.035         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  3.37it/s]\n/home/lhj/anaconda3/envs/waffle/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  7.96it/s]\n                   all         21         21    0.00268        0.8      0.279      0.221\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/50      8.05G      1.575      5.092      1.179         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  4.89it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.11it/s]\n                   all         21         21    0.00301        0.9      0.287      0.235\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/50      8.05G      1.409      4.601      1.086         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.85it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 18.04it/s]\n                   all         21         21    0.00318       0.95        0.3      0.243\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/50      8.05G      1.395      4.785      1.085         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.71it/s]\n                   all         21         21    0.00317       0.95        0.3      0.256\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/50      8.05G      1.462      4.589      1.095         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.82it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.09it/s]\n                   all         21         21    0.00317       0.95      0.293       0.27\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/50      8.05G      1.396      4.978       1.08         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 18.11it/s]\n                   all         21         21    0.00333          1      0.297      0.241\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/50      8.07G      1.209      4.545     0.9995         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.50it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 16.24it/s]\n                   all         21         21    0.00337          1      0.262      0.219\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/50      8.07G     0.9826      4.332      0.945         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.77it/s]\n                   all         21         21    0.00337          1      0.234      0.214\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/50      8.07G     0.6574      4.055     0.8496         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.63it/s]\n                   all         21         21    0.00333          1      0.385      0.331\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/50      8.07G     0.6343      3.874      0.833         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.79it/s]\n                   all         21         21    0.00337          1      0.515      0.452\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/50      8.07G     0.6128      3.512     0.7921         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.66it/s]\n                   all         21         21    0.00339          1      0.602      0.494\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/50      8.07G     0.6772      2.915     0.8324         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.65it/s]\n                   all         21         21    0.00338          1      0.696      0.582\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/50      8.07G     0.6111      2.222     0.8684         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.61it/s]\n                   all         21         21     0.0034          1      0.716      0.605\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/50      8.07G     0.5946      1.971     0.8091         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.25it/s]\n                   all         21         21    0.00339          1      0.754      0.595\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/50      8.07G     0.7179      1.955     0.9884         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 15.20it/s]\n                   all         21         21     0.0034          1      0.774      0.653\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/50      8.07G     0.6793      1.752     0.8643         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  6.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 17.96it/s]\n                   all         21         21    0.00338          1      0.813      0.692\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/50      8.07G     0.6969      1.592     0.9367         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 20.51it/s]\n                   all         21         21    0.00336          1      0.857      0.687\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/50      8.07G     0.6333      1.443     0.8665         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.41it/s]\n                   all         21         21    0.00339          1      0.877      0.731\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/50      8.07G     0.5324      1.451     0.8002         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.42it/s]\n                   all         21         21     0.0034          1      0.867      0.712\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/50      8.07G     0.5891      1.397     0.8683         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.55it/s]\n                   all         21         21    0.00346          1      0.879      0.732\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/50      8.07G       0.53      1.273     0.8027         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.48it/s]\n                   all         21         21    0.00371          1      0.952      0.834\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/50      8.07G     0.6902      1.273      0.896         13        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.69it/s]\n                   all         21         21    0.00391          1      0.948      0.787\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/50      8.07G     0.6887      1.321     0.8982         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.86it/s]\n                   all         21         21    0.00383          1      0.972      0.838\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/50      8.07G     0.6426      1.132     0.9158         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.73it/s]\n                   all         21         21     0.0037          1      0.959      0.801\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      25/50      8.07G     0.6047        1.1     0.8697         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 21.90it/s]\n                   all         21         21    0.00355          1       0.99      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      26/50      8.07G     0.6092      1.041     0.8788         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.18it/s]\n                   all         21         21    0.00349          1      0.977      0.834\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      27/50      8.07G     0.5892      1.029     0.8259         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.47it/s]\n                   all         21         21     0.0034          1      0.981       0.82\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      28/50      8.07G     0.5722     0.9661      0.873         13        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.48it/s]\n                   all         21         21    0.00338          1      0.995      0.846\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      29/50      8.07G     0.5316     0.9626     0.8278         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 21.96it/s]\n                   all         21         21    0.00336          1       0.97      0.822\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      30/50      8.07G     0.5911     0.9537     0.8898         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.47it/s]\n                   all         21         21    0.00363          1      0.978      0.823\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      31/50      8.07G     0.5412     0.9979     0.8238         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.31it/s]\n                   all         21         21    0.00342          1      0.995      0.852\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      32/50      8.07G     0.5441      0.847     0.8341         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.03it/s]\n                   all         21         21    0.00339          1      0.995      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      33/50      8.07G     0.5497     0.8736     0.8077         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.46it/s]\n                   all         21         21    0.00334          1      0.995      0.859\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      34/50      8.07G      0.494     0.8033     0.8161         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 21.80it/s]\n                   all         21         21    0.00353          1      0.995      0.875\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      35/50      8.07G     0.5713     0.8376     0.8583         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.59it/s]\n                   all         21         21      0.975          1      0.995      0.811\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      36/50      8.07G     0.5074     0.7759     0.8489         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.54it/s]\n                   all         21         21          1      0.556      0.995      0.842\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      37/50      8.07G     0.5478     0.9739     0.8804         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 23.31it/s]\n                   all         21         21    0.00335          1      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      38/50      8.07G     0.5109      0.793     0.8102         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.82it/s]\n                   all         21         21      0.947          1      0.995      0.879\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      39/50      8.07G     0.5767     0.8693     0.8706         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.38it/s]\n                   all         21         21      0.986      0.837      0.995      0.874\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      40/50      8.07G     0.5938     0.8053     0.8787         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 23.14it/s]\n                   all         21         21          1      0.444       0.99      0.889\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      41/50      8.07G     0.4485     0.8481      0.813         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.44it/s]\n                   all         21         21          1       0.35      0.972       0.86\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      42/50      8.07G     0.5216     0.7309     0.8386         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.28it/s]\n                   all         21         21          1       0.57      0.981      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      43/50      8.07G     0.4611     0.7521     0.8164         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.30it/s]\n                   all         21         21      0.944      0.618      0.981       0.85\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      44/50      8.07G     0.4799     0.8456      0.816         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  6.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 20.08it/s]\n                   all         21         21      0.961      0.727      0.986      0.878\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      45/50      8.07G     0.4227     0.6774     0.8516         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.60it/s]\n                   all         21         21          1      0.681      0.986      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      46/50      8.07G     0.4401     0.6882     0.7938         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 20.89it/s]\n                   all         21         21          1       0.76      0.995      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      47/50      8.07G     0.4235     0.6814     0.8128         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.19it/s]\n                   all         21         21          1      0.841      0.995        0.9\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      48/50      8.07G     0.4322     0.6854     0.8143         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 21.71it/s]\n                   all         21         21      0.993       0.97      0.995      0.906\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      49/50      8.07G     0.3993     0.6641     0.8059         15        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 22.18it/s]\n                   all         21         21      0.995      0.982      0.995      0.892\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      50/50      8.07G     0.4161     0.6213      0.809         14        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 15.49it/s]\n                   all         21         21      0.991          1      0.995      0.907\n\n50 epochs completed in 0.009 hours.\nOptimizer stripped from hubs/detector/artifacts/weights/last.pt, 6.2MB\nOptimizer stripped from hubs/detector/artifacts/weights/best.pt, 6.2MB\n\nValidating hubs/detector/artifacts/weights/best.pt...\nUltralytics YOLOv8.0.112 \ud83d\ude80 Python-3.9.16 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24215MiB)\nModel summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 16.60it/s]\n                   all         21         21      0.992          1      0.995      0.907\n                     1         21         10      0.992          1      0.995      0.883\n                     2         21         11      0.992          1      0.995      0.932\nSpeed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.3ms postprocess per image\nResults saved to hubs/detector/artifacts\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.27s/it]\n</pre> Out[7]: <pre>TrainResult(best_ckpt_file=PosixPath('hubs/detector/weights/best_ckpt.pt'), last_ckpt_file=PosixPath('hubs/detector/weights/last_ckpt.pt'), metrics=[[{'tag': 'epoch', 'value': 0.0}, {'tag': 'train/box_loss', 'value': 1.3501}, {'tag': 'train/cls_loss', 'value': 4.6464}, {'tag': 'train/dfl_loss', 'value': 1.0351}, {'tag': 'metrics/precision(B)', 'value': 0.00268}, {'tag': 'metrics/recall(B)', 'value': 0.8}, {'tag': 'metrics/mAP50(B)', 'value': 0.27948}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.22095}, {'tag': 'val/box_loss', 'value': 0.43408}, {'tag': 'val/cls_loss', 'value': 3.8634}, {'tag': 'val/dfl_loss', 'value': 0.82598}, {'tag': 'lr/pg0', 'value': 0.0991}, {'tag': 'lr/pg1', 'value': 0.0001}, {'tag': 'lr/pg2', 'value': 0.0001}], [{'tag': 'epoch', 'value': 1.0}, {'tag': 'train/box_loss', 'value': 1.5745}, {'tag': 'train/cls_loss', 'value': 5.0921}, {'tag': 'train/dfl_loss', 'value': 1.1791}, {'tag': 'metrics/precision(B)', 'value': 0.00301}, {'tag': 'metrics/recall(B)', 'value': 0.9}, {'tag': 'metrics/mAP50(B)', 'value': 0.28713}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.23503}, {'tag': 'val/box_loss', 'value': 0.42816}, {'tag': 'val/cls_loss', 'value': 3.8297}, {'tag': 'val/dfl_loss', 'value': 0.81922}, {'tag': 'lr/pg0', 'value': 0.097294}, {'tag': 'lr/pg1', 'value': 0.00029406}, {'tag': 'lr/pg2', 'value': 0.00029406}], [{'tag': 'epoch', 'value': 2.0}, {'tag': 'train/box_loss', 'value': 1.4092}, {'tag': 'train/cls_loss', 'value': 4.601}, {'tag': 'train/dfl_loss', 'value': 1.0856}, {'tag': 'metrics/precision(B)', 'value': 0.00318}, {'tag': 'metrics/recall(B)', 'value': 0.95}, {'tag': 'metrics/mAP50(B)', 'value': 0.29955}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.2428}, {'tag': 'val/box_loss', 'value': 0.44174}, {'tag': 'val/cls_loss', 'value': 3.8018}, {'tag': 'val/dfl_loss', 'value': 0.80306}, {'tag': 'lr/pg0', 'value': 0.09548}, {'tag': 'lr/pg1', 'value': 0.0004802}, {'tag': 'lr/pg2', 'value': 0.0004802}], [{'tag': 'epoch', 'value': 3.0}, {'tag': 'train/box_loss', 'value': 1.3949}, {'tag': 'train/cls_loss', 'value': 4.7847}, {'tag': 'train/dfl_loss', 'value': 1.0853}, {'tag': 'metrics/precision(B)', 'value': 0.00317}, {'tag': 'metrics/recall(B)', 'value': 0.95}, {'tag': 'metrics/mAP50(B)', 'value': 0.30017}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.25635}, {'tag': 'val/box_loss', 'value': 0.4457}, {'tag': 'val/cls_loss', 'value': 3.7635}, {'tag': 'val/dfl_loss', 'value': 0.79997}, {'tag': 'lr/pg0', 'value': 0.093658}, {'tag': 'lr/pg1', 'value': 0.00065842}, {'tag': 'lr/pg2', 'value': 0.00065842}], [{'tag': 'epoch', 'value': 4.0}, {'tag': 'train/box_loss', 'value': 1.4621}, {'tag': 'train/cls_loss', 'value': 4.5888}, {'tag': 'train/dfl_loss', 'value': 1.0952}, {'tag': 'metrics/precision(B)', 'value': 0.00317}, {'tag': 'metrics/recall(B)', 'value': 0.95}, {'tag': 'metrics/mAP50(B)', 'value': 0.29295}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.2701}, {'tag': 'val/box_loss', 'value': 0.43005}, {'tag': 'val/cls_loss', 'value': 3.7548}, {'tag': 'val/dfl_loss', 'value': 0.80128}, {'tag': 'lr/pg0', 'value': 0.091829}, {'tag': 'lr/pg1', 'value': 0.00082872}, {'tag': 'lr/pg2', 'value': 0.00082872}], [{'tag': 'epoch', 'value': 5.0}, {'tag': 'train/box_loss', 'value': 1.3964}, {'tag': 'train/cls_loss', 'value': 4.9783}, {'tag': 'train/dfl_loss', 'value': 1.0798}, {'tag': 'metrics/precision(B)', 'value': 0.00333}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.29703}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.24147}, {'tag': 'val/box_loss', 'value': 0.41959}, {'tag': 'val/cls_loss', 'value': 3.7923}, {'tag': 'val/dfl_loss', 'value': 0.8005}, {'tag': 'lr/pg0', 'value': 0.089991}, {'tag': 'lr/pg1', 'value': 0.0009911}, {'tag': 'lr/pg2', 'value': 0.0009911}], [{'tag': 'epoch', 'value': 6.0}, {'tag': 'train/box_loss', 'value': 1.2091}, {'tag': 'train/cls_loss', 'value': 4.5448}, {'tag': 'train/dfl_loss', 'value': 0.99952}, {'tag': 'metrics/precision(B)', 'value': 0.00337}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.2619}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.21933}, {'tag': 'val/box_loss', 'value': 0.42009}, {'tag': 'val/cls_loss', 'value': 3.8168}, {'tag': 'val/dfl_loss', 'value': 0.81327}, {'tag': 'lr/pg0', 'value': 0.088146}, {'tag': 'lr/pg1', 'value': 0.0011456}, {'tag': 'lr/pg2', 'value': 0.0011456}], [{'tag': 'epoch', 'value': 7.0}, {'tag': 'train/box_loss', 'value': 0.98263}, {'tag': 'train/cls_loss', 'value': 4.3317}, {'tag': 'train/dfl_loss', 'value': 0.94503}, {'tag': 'metrics/precision(B)', 'value': 0.00337}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.23364}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.21407}, {'tag': 'val/box_loss', 'value': 0.49657}, {'tag': 'val/cls_loss', 'value': 3.792}, {'tag': 'val/dfl_loss', 'value': 0.82097}, {'tag': 'lr/pg0', 'value': 0.086292}, {'tag': 'lr/pg1', 'value': 0.0012921}, {'tag': 'lr/pg2', 'value': 0.0012921}], [{'tag': 'epoch', 'value': 8.0}, {'tag': 'train/box_loss', 'value': 0.65743}, {'tag': 'train/cls_loss', 'value': 4.0554}, {'tag': 'train/dfl_loss', 'value': 0.84959}, {'tag': 'metrics/precision(B)', 'value': 0.00333}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.38451}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.3307}, {'tag': 'val/box_loss', 'value': 0.58206}, {'tag': 'val/cls_loss', 'value': 3.7688}, {'tag': 'val/dfl_loss', 'value': 0.82093}, {'tag': 'lr/pg0', 'value': 0.084431}, {'tag': 'lr/pg1', 'value': 0.0014307}, {'tag': 'lr/pg2', 'value': 0.0014307}], [{'tag': 'epoch', 'value': 9.0}, {'tag': 'train/box_loss', 'value': 0.63434}, {'tag': 'train/cls_loss', 'value': 3.8742}, {'tag': 'train/dfl_loss', 'value': 0.83304}, {'tag': 'metrics/precision(B)', 'value': 0.00337}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.515}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.45197}, {'tag': 'val/box_loss', 'value': 0.60736}, {'tag': 'val/cls_loss', 'value': 3.6357}, {'tag': 'val/dfl_loss', 'value': 0.81601}, {'tag': 'lr/pg0', 'value': 0.082561}, {'tag': 'lr/pg1', 'value': 0.0015614}, {'tag': 'lr/pg2', 'value': 0.0015614}], [{'tag': 'epoch', 'value': 10.0}, {'tag': 'train/box_loss', 'value': 0.61282}, {'tag': 'train/cls_loss', 'value': 3.512}, {'tag': 'train/dfl_loss', 'value': 0.79209}, {'tag': 'metrics/precision(B)', 'value': 0.00339}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.60177}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.49371}, {'tag': 'val/box_loss', 'value': 0.64081}, {'tag': 'val/cls_loss', 'value': 3.4475}, {'tag': 'val/dfl_loss', 'value': 0.80918}, {'tag': 'lr/pg0', 'value': 0.080684}, {'tag': 'lr/pg1', 'value': 0.0016842}, {'tag': 'lr/pg2', 'value': 0.0016842}], [{'tag': 'epoch', 'value': 11.0}, {'tag': 'train/box_loss', 'value': 0.67718}, {'tag': 'train/cls_loss', 'value': 2.9147}, {'tag': 'train/dfl_loss', 'value': 0.83244}, {'tag': 'metrics/precision(B)', 'value': 0.00338}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.69629}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.582}, {'tag': 'val/box_loss', 'value': 0.59014}, {'tag': 'val/cls_loss', 'value': 3.2775}, {'tag': 'val/dfl_loss', 'value': 0.78841}, {'tag': 'lr/pg0', 'value': 0.078799}, {'tag': 'lr/pg1', 'value': 0.0017991}, {'tag': 'lr/pg2', 'value': 0.0017991}], [{'tag': 'epoch', 'value': 12.0}, {'tag': 'train/box_loss', 'value': 0.61112}, {'tag': 'train/cls_loss', 'value': 2.2224}, {'tag': 'train/dfl_loss', 'value': 0.86835}, {'tag': 'metrics/precision(B)', 'value': 0.0034}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.71624}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.60464}, {'tag': 'val/box_loss', 'value': 0.57163}, {'tag': 'val/cls_loss', 'value': 3.2206}, {'tag': 'val/dfl_loss', 'value': 0.79575}, {'tag': 'lr/pg0', 'value': 0.076906}, {'tag': 'lr/pg1', 'value': 0.001906}, {'tag': 'lr/pg2', 'value': 0.001906}], [{'tag': 'epoch', 'value': 13.0}, {'tag': 'train/box_loss', 'value': 0.59459}, {'tag': 'train/cls_loss', 'value': 1.9712}, {'tag': 'train/dfl_loss', 'value': 0.80912}, {'tag': 'metrics/precision(B)', 'value': 0.00339}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.75376}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.59454}, {'tag': 'val/box_loss', 'value': 0.60448}, {'tag': 'val/cls_loss', 'value': 3.2301}, {'tag': 'val/dfl_loss', 'value': 0.79842}, {'tag': 'lr/pg0', 'value': 0.075005}, {'tag': 'lr/pg1', 'value': 0.002005}, {'tag': 'lr/pg2', 'value': 0.002005}], [{'tag': 'epoch', 'value': 14.0}, {'tag': 'train/box_loss', 'value': 0.71785}, {'tag': 'train/cls_loss', 'value': 1.9547}, {'tag': 'train/dfl_loss', 'value': 0.98837}, {'tag': 'metrics/precision(B)', 'value': 0.0034}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.77386}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.65288}, {'tag': 'val/box_loss', 'value': 0.49612}, {'tag': 'val/cls_loss', 'value': 3.2141}, {'tag': 'val/dfl_loss', 'value': 0.79957}, {'tag': 'lr/pg0', 'value': 0.073096}, {'tag': 'lr/pg1', 'value': 0.0020961}, {'tag': 'lr/pg2', 'value': 0.0020961}], [{'tag': 'epoch', 'value': 15.0}, {'tag': 'train/box_loss', 'value': 0.6793}, {'tag': 'train/cls_loss', 'value': 1.7516}, {'tag': 'train/dfl_loss', 'value': 0.86429}, {'tag': 'metrics/precision(B)', 'value': 0.00338}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.81266}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.69205}, {'tag': 'val/box_loss', 'value': 0.57349}, {'tag': 'val/cls_loss', 'value': 3.2207}, {'tag': 'val/dfl_loss', 'value': 0.81375}, {'tag': 'lr/pg0', 'value': 0.071179}, {'tag': 'lr/pg1', 'value': 0.0021793}, {'tag': 'lr/pg2', 'value': 0.0021793}], [{'tag': 'epoch', 'value': 16.0}, {'tag': 'train/box_loss', 'value': 0.69685}, {'tag': 'train/cls_loss', 'value': 1.5918}, {'tag': 'train/dfl_loss', 'value': 0.93667}, {'tag': 'metrics/precision(B)', 'value': 0.00336}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.8569}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.68663}, {'tag': 'val/box_loss', 'value': 0.58732}, {'tag': 'val/cls_loss', 'value': 3.2442}, {'tag': 'val/dfl_loss', 'value': 0.8026}, {'tag': 'lr/pg0', 'value': 0.069255}, {'tag': 'lr/pg1', 'value': 0.0022546}, {'tag': 'lr/pg2', 'value': 0.0022546}], [{'tag': 'epoch', 'value': 17.0}, {'tag': 'train/box_loss', 'value': 0.63334}, {'tag': 'train/cls_loss', 'value': 1.4425}, {'tag': 'train/dfl_loss', 'value': 0.86646}, {'tag': 'metrics/precision(B)', 'value': 0.00339}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.87667}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.73053}, {'tag': 'val/box_loss', 'value': 0.50378}, {'tag': 'val/cls_loss', 'value': 3.2284}, {'tag': 'val/dfl_loss', 'value': 0.8031}, {'tag': 'lr/pg0', 'value': 0.067322}, {'tag': 'lr/pg1', 'value': 0.0023219}, {'tag': 'lr/pg2', 'value': 0.0023219}], [{'tag': 'epoch', 'value': 18.0}, {'tag': 'train/box_loss', 'value': 0.53245}, {'tag': 'train/cls_loss', 'value': 1.4508}, {'tag': 'train/dfl_loss', 'value': 0.80017}, {'tag': 'metrics/precision(B)', 'value': 0.0034}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.86682}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.71193}, {'tag': 'val/box_loss', 'value': 0.5868}, {'tag': 'val/cls_loss', 'value': 3.2135}, {'tag': 'val/dfl_loss', 'value': 0.81075}, {'tag': 'lr/pg0', 'value': 0.065381}, {'tag': 'lr/pg1', 'value': 0.0023813}, {'tag': 'lr/pg2', 'value': 0.0023813}], [{'tag': 'epoch', 'value': 19.0}, {'tag': 'train/box_loss', 'value': 0.58909}, {'tag': 'train/cls_loss', 'value': 1.3975}, {'tag': 'train/dfl_loss', 'value': 0.86834}, {'tag': 'metrics/precision(B)', 'value': 0.00346}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.87881}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.73247}, {'tag': 'val/box_loss', 'value': 0.60195}, {'tag': 'val/cls_loss', 'value': 3.237}, {'tag': 'val/dfl_loss', 'value': 0.81828}, {'tag': 'lr/pg0', 'value': 0.063433}, {'tag': 'lr/pg1', 'value': 0.0024328}, {'tag': 'lr/pg2', 'value': 0.0024328}], [{'tag': 'epoch', 'value': 20.0}, {'tag': 'train/box_loss', 'value': 0.53003}, {'tag': 'train/cls_loss', 'value': 1.2732}, {'tag': 'train/dfl_loss', 'value': 0.80269}, {'tag': 'metrics/precision(B)', 'value': 0.00371}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.95167}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.83357}, {'tag': 'val/box_loss', 'value': 0.50689}, {'tag': 'val/cls_loss', 'value': 3.202}, {'tag': 'val/dfl_loss', 'value': 0.8019}, {'tag': 'lr/pg0', 'value': 0.061476}, {'tag': 'lr/pg1', 'value': 0.0024764}, {'tag': 'lr/pg2', 'value': 0.0024764}], [{'tag': 'epoch', 'value': 21.0}, {'tag': 'train/box_loss', 'value': 0.69022}, {'tag': 'train/cls_loss', 'value': 1.2731}, {'tag': 'train/dfl_loss', 'value': 0.89605}, {'tag': 'metrics/precision(B)', 'value': 0.00391}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.94833}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.78659}, {'tag': 'val/box_loss', 'value': 0.60027}, {'tag': 'val/cls_loss', 'value': 3.2329}, {'tag': 'val/dfl_loss', 'value': 0.82252}, {'tag': 'lr/pg0', 'value': 0.059512}, {'tag': 'lr/pg1', 'value': 0.0025121}, {'tag': 'lr/pg2', 'value': 0.0025121}], [{'tag': 'epoch', 'value': 22.0}, {'tag': 'train/box_loss', 'value': 0.68866}, {'tag': 'train/cls_loss', 'value': 1.321}, {'tag': 'train/dfl_loss', 'value': 0.89818}, {'tag': 'metrics/precision(B)', 'value': 0.00383}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.97227}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.83799}, {'tag': 'val/box_loss', 'value': 0.56671}, {'tag': 'val/cls_loss', 'value': 3.2398}, {'tag': 'val/dfl_loss', 'value': 0.8096}, {'tag': 'lr/pg0', 'value': 0.05754}, {'tag': 'lr/pg1', 'value': 0.0025398}, {'tag': 'lr/pg2', 'value': 0.0025398}], [{'tag': 'epoch', 'value': 23.0}, {'tag': 'train/box_loss', 'value': 0.64262}, {'tag': 'train/cls_loss', 'value': 1.1323}, {'tag': 'train/dfl_loss', 'value': 0.91579}, {'tag': 'metrics/precision(B)', 'value': 0.0037}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.95864}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.80125}, {'tag': 'val/box_loss', 'value': 0.53914}, {'tag': 'val/cls_loss', 'value': 3.2466}, {'tag': 'val/dfl_loss', 'value': 0.80866}, {'tag': 'lr/pg0', 'value': 0.05556}, {'tag': 'lr/pg1', 'value': 0.0025596}, {'tag': 'lr/pg2', 'value': 0.0025596}], [{'tag': 'epoch', 'value': 24.0}, {'tag': 'train/box_loss', 'value': 0.60471}, {'tag': 'train/cls_loss', 'value': 1.1004}, {'tag': 'train/dfl_loss', 'value': 0.86974}, {'tag': 'metrics/precision(B)', 'value': 0.00355}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.99045}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.79496}, {'tag': 'val/box_loss', 'value': 0.6285}, {'tag': 'val/cls_loss', 'value': 3.173}, {'tag': 'val/dfl_loss', 'value': 0.81681}, {'tag': 'lr/pg0', 'value': 0.053572}, {'tag': 'lr/pg1', 'value': 0.0025715}, {'tag': 'lr/pg2', 'value': 0.0025715}], [{'tag': 'epoch', 'value': 25.0}, {'tag': 'train/box_loss', 'value': 0.60922}, {'tag': 'train/cls_loss', 'value': 1.0412}, {'tag': 'train/dfl_loss', 'value': 0.87879}, {'tag': 'metrics/precision(B)', 'value': 0.00349}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.97682}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.83378}, {'tag': 'val/box_loss', 'value': 0.58855}, {'tag': 'val/cls_loss', 'value': 3.1961}, {'tag': 'val/dfl_loss', 'value': 0.81326}, {'tag': 'lr/pg0', 'value': 0.051576}, {'tag': 'lr/pg1', 'value': 0.0025755}, {'tag': 'lr/pg2', 'value': 0.0025755}], [{'tag': 'epoch', 'value': 26.0}, {'tag': 'train/box_loss', 'value': 0.58924}, {'tag': 'train/cls_loss', 'value': 1.0289}, {'tag': 'train/dfl_loss', 'value': 0.82589}, {'tag': 'metrics/precision(B)', 'value': 0.0034}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.98136}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.82034}, {'tag': 'val/box_loss', 'value': 0.64944}, {'tag': 'val/cls_loss', 'value': 3.1635}, {'tag': 'val/dfl_loss', 'value': 0.81535}, {'tag': 'lr/pg0', 'value': 0.049572}, {'tag': 'lr/pg1', 'value': 0.0025716}, {'tag': 'lr/pg2', 'value': 0.0025716}], [{'tag': 'epoch', 'value': 27.0}, {'tag': 'train/box_loss', 'value': 0.57216}, {'tag': 'train/cls_loss', 'value': 0.96613}, {'tag': 'train/dfl_loss', 'value': 0.87305}, {'tag': 'metrics/precision(B)', 'value': 0.00338}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.84577}, {'tag': 'val/box_loss', 'value': 0.56407}, {'tag': 'val/cls_loss', 'value': 3.0513}, {'tag': 'val/dfl_loss', 'value': 0.8024}, {'tag': 'lr/pg0', 'value': 0.04756}, {'tag': 'lr/pg1', 'value': 0.0025597}, {'tag': 'lr/pg2', 'value': 0.0025597}], [{'tag': 'epoch', 'value': 28.0}, {'tag': 'train/box_loss', 'value': 0.53161}, {'tag': 'train/cls_loss', 'value': 0.96257}, {'tag': 'train/dfl_loss', 'value': 0.82785}, {'tag': 'metrics/precision(B)', 'value': 0.00336}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.9699}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.82215}, {'tag': 'val/box_loss', 'value': 0.58077}, {'tag': 'val/cls_loss', 'value': 3.0708}, {'tag': 'val/dfl_loss', 'value': 0.80563}, {'tag': 'lr/pg0', 'value': 0.04554}, {'tag': 'lr/pg1', 'value': 0.0025399}, {'tag': 'lr/pg2', 'value': 0.0025399}], [{'tag': 'epoch', 'value': 29.0}, {'tag': 'train/box_loss', 'value': 0.59106}, {'tag': 'train/cls_loss', 'value': 0.95372}, {'tag': 'train/dfl_loss', 'value': 0.8898}, {'tag': 'metrics/precision(B)', 'value': 0.00363}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.97761}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.82256}, {'tag': 'val/box_loss', 'value': 0.63684}, {'tag': 'val/cls_loss', 'value': 2.9952}, {'tag': 'val/dfl_loss', 'value': 0.81421}, {'tag': 'lr/pg0', 'value': 0.043512}, {'tag': 'lr/pg1', 'value': 0.0025122}, {'tag': 'lr/pg2', 'value': 0.0025122}], [{'tag': 'epoch', 'value': 30.0}, {'tag': 'train/box_loss', 'value': 0.54118}, {'tag': 'train/cls_loss', 'value': 0.99789}, {'tag': 'train/dfl_loss', 'value': 0.82375}, {'tag': 'metrics/precision(B)', 'value': 0.00342}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.85187}, {'tag': 'val/box_loss', 'value': 0.5704}, {'tag': 'val/cls_loss', 'value': 2.8481}, {'tag': 'val/dfl_loss', 'value': 0.80421}, {'tag': 'lr/pg0', 'value': 0.041477}, {'tag': 'lr/pg1', 'value': 0.0024766}, {'tag': 'lr/pg2', 'value': 0.0024766}], [{'tag': 'epoch', 'value': 31.0}, {'tag': 'train/box_loss', 'value': 0.5441}, {'tag': 'train/cls_loss', 'value': 0.84697}, {'tag': 'train/dfl_loss', 'value': 0.83413}, {'tag': 'metrics/precision(B)', 'value': 0.00339}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.80418}, {'tag': 'val/box_loss', 'value': 0.65833}, {'tag': 'val/cls_loss', 'value': 2.6579}, {'tag': 'val/dfl_loss', 'value': 0.81958}, {'tag': 'lr/pg0', 'value': 0.039433}, {'tag': 'lr/pg1', 'value': 0.0024331}, {'tag': 'lr/pg2', 'value': 0.0024331}], [{'tag': 'epoch', 'value': 32.0}, {'tag': 'train/box_loss', 'value': 0.54969}, {'tag': 'train/cls_loss', 'value': 0.87359}, {'tag': 'train/dfl_loss', 'value': 0.80767}, {'tag': 'metrics/precision(B)', 'value': 0.00334}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.85862}, {'tag': 'val/box_loss', 'value': 0.53264}, {'tag': 'val/cls_loss', 'value': 2.622}, {'tag': 'val/dfl_loss', 'value': 0.7936}, {'tag': 'lr/pg0', 'value': 0.037382}, {'tag': 'lr/pg1', 'value': 0.0023816}, {'tag': 'lr/pg2', 'value': 0.0023816}], [{'tag': 'epoch', 'value': 33.0}, {'tag': 'train/box_loss', 'value': 0.494}, {'tag': 'train/cls_loss', 'value': 0.80329}, {'tag': 'train/dfl_loss', 'value': 0.81615}, {'tag': 'metrics/precision(B)', 'value': 0.00353}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.87549}, {'tag': 'val/box_loss', 'value': 0.52511}, {'tag': 'val/cls_loss', 'value': 2.6191}, {'tag': 'val/dfl_loss', 'value': 0.78882}, {'tag': 'lr/pg0', 'value': 0.035322}, {'tag': 'lr/pg1', 'value': 0.0023222}, {'tag': 'lr/pg2', 'value': 0.0023222}], [{'tag': 'epoch', 'value': 34.0}, {'tag': 'train/box_loss', 'value': 0.57125}, {'tag': 'train/cls_loss', 'value': 0.83759}, {'tag': 'train/dfl_loss', 'value': 0.85834}, {'tag': 'metrics/precision(B)', 'value': 0.97521}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.81067}, {'tag': 'val/box_loss', 'value': 0.63479}, {'tag': 'val/cls_loss', 'value': 2.5756}, {'tag': 'val/dfl_loss', 'value': 0.80614}, {'tag': 'lr/pg0', 'value': 0.033255}, {'tag': 'lr/pg1', 'value': 0.0022549}, {'tag': 'lr/pg2', 'value': 0.0022549}], [{'tag': 'epoch', 'value': 35.0}, {'tag': 'train/box_loss', 'value': 0.5074}, {'tag': 'train/cls_loss', 'value': 0.77593}, {'tag': 'train/dfl_loss', 'value': 0.84894}, {'tag': 'metrics/precision(B)', 'value': 1.0}, {'tag': 'metrics/recall(B)', 'value': 0.55576}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.84226}, {'tag': 'val/box_loss', 'value': 0.59675}, {'tag': 'val/cls_loss', 'value': 2.3746}, {'tag': 'val/dfl_loss', 'value': 0.79787}, {'tag': 'lr/pg0', 'value': 0.03118}, {'tag': 'lr/pg1', 'value': 0.0021797}, {'tag': 'lr/pg2', 'value': 0.0021797}], [{'tag': 'epoch', 'value': 36.0}, {'tag': 'train/box_loss', 'value': 0.5478}, {'tag': 'train/cls_loss', 'value': 0.9739}, {'tag': 'train/dfl_loss', 'value': 0.8804}, {'tag': 'metrics/precision(B)', 'value': 0.00335}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.88408}, {'tag': 'val/box_loss', 'value': 0.49261}, {'tag': 'val/cls_loss', 'value': 2.5684}, {'tag': 'val/dfl_loss', 'value': 0.7895}, {'tag': 'lr/pg0', 'value': 0.029097}, {'tag': 'lr/pg1', 'value': 0.0020966}, {'tag': 'lr/pg2', 'value': 0.0020966}], [{'tag': 'epoch', 'value': 37.0}, {'tag': 'train/box_loss', 'value': 0.5109}, {'tag': 'train/cls_loss', 'value': 0.79298}, {'tag': 'train/dfl_loss', 'value': 0.81025}, {'tag': 'metrics/precision(B)', 'value': 0.94683}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.87914}, {'tag': 'val/box_loss', 'value': 0.53933}, {'tag': 'val/cls_loss', 'value': 2.4032}, {'tag': 'val/dfl_loss', 'value': 0.79472}, {'tag': 'lr/pg0', 'value': 0.027006}, {'tag': 'lr/pg1', 'value': 0.0020055}, {'tag': 'lr/pg2', 'value': 0.0020055}], [{'tag': 'epoch', 'value': 38.0}, {'tag': 'train/box_loss', 'value': 0.5767}, {'tag': 'train/cls_loss', 'value': 0.86935}, {'tag': 'train/dfl_loss', 'value': 0.87058}, {'tag': 'metrics/precision(B)', 'value': 0.98576}, {'tag': 'metrics/recall(B)', 'value': 0.83699}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.87418}, {'tag': 'val/box_loss', 'value': 0.59056}, {'tag': 'val/cls_loss', 'value': 2.3744}, {'tag': 'val/dfl_loss', 'value': 0.79638}, {'tag': 'lr/pg0', 'value': 0.024907}, {'tag': 'lr/pg1', 'value': 0.0019065}, {'tag': 'lr/pg2', 'value': 0.0019065}], [{'tag': 'epoch', 'value': 39.0}, {'tag': 'train/box_loss', 'value': 0.59383}, {'tag': 'train/cls_loss', 'value': 0.80533}, {'tag': 'train/dfl_loss', 'value': 0.87874}, {'tag': 'metrics/precision(B)', 'value': 1.0}, {'tag': 'metrics/recall(B)', 'value': 0.4442}, {'tag': 'metrics/mAP50(B)', 'value': 0.99045}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.88933}, {'tag': 'val/box_loss', 'value': 0.58402}, {'tag': 'val/cls_loss', 'value': 2.2775}, {'tag': 'val/dfl_loss', 'value': 0.81}, {'tag': 'lr/pg0', 'value': 0.0228}, {'tag': 'lr/pg1', 'value': 0.0017996}, {'tag': 'lr/pg2', 'value': 0.0017996}], [{'tag': 'epoch', 'value': 40.0}, {'tag': 'train/box_loss', 'value': 0.4485}, {'tag': 'train/cls_loss', 'value': 0.84811}, {'tag': 'train/dfl_loss', 'value': 0.81296}, {'tag': 'metrics/precision(B)', 'value': 1.0}, {'tag': 'metrics/recall(B)', 'value': 0.35018}, {'tag': 'metrics/mAP50(B)', 'value': 0.97167}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.8598}, {'tag': 'val/box_loss', 'value': 0.58368}, {'tag': 'val/cls_loss', 'value': 2.2709}, {'tag': 'val/dfl_loss', 'value': 0.81143}, {'tag': 'lr/pg0', 'value': 0.020685}, {'tag': 'lr/pg1', 'value': 0.0016848}, {'tag': 'lr/pg2', 'value': 0.0016848}], [{'tag': 'epoch', 'value': 41.0}, {'tag': 'train/box_loss', 'value': 0.5216}, {'tag': 'train/cls_loss', 'value': 0.7309}, {'tag': 'train/dfl_loss', 'value': 0.8386}, {'tag': 'metrics/precision(B)', 'value': 1.0}, {'tag': 'metrics/recall(B)', 'value': 0.56993}, {'tag': 'metrics/mAP50(B)', 'value': 0.98136}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.88798}, {'tag': 'val/box_loss', 'value': 0.56186}, {'tag': 'val/cls_loss', 'value': 2.1034}, {'tag': 'val/dfl_loss', 'value': 0.80235}, {'tag': 'lr/pg0', 'value': 0.018562}, {'tag': 'lr/pg1', 'value': 0.0015621}, {'tag': 'lr/pg2', 'value': 0.0015621}], [{'tag': 'epoch', 'value': 42.0}, {'tag': 'train/box_loss', 'value': 0.46109}, {'tag': 'train/cls_loss', 'value': 0.75215}, {'tag': 'train/dfl_loss', 'value': 0.81637}, {'tag': 'metrics/precision(B)', 'value': 0.94392}, {'tag': 'metrics/recall(B)', 'value': 0.61791}, {'tag': 'metrics/mAP50(B)', 'value': 0.98136}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.85015}, {'tag': 'val/box_loss', 'value': 0.64211}, {'tag': 'val/cls_loss', 'value': 1.9825}, {'tag': 'val/dfl_loss', 'value': 0.81823}, {'tag': 'lr/pg0', 'value': 0.016431}, {'tag': 'lr/pg1', 'value': 0.0014314}, {'tag': 'lr/pg2', 'value': 0.0014314}], [{'tag': 'epoch', 'value': 43.0}, {'tag': 'train/box_loss', 'value': 0.47991}, {'tag': 'train/cls_loss', 'value': 0.84562}, {'tag': 'train/dfl_loss', 'value': 0.816}, {'tag': 'metrics/precision(B)', 'value': 0.96134}, {'tag': 'metrics/recall(B)', 'value': 0.72676}, {'tag': 'metrics/mAP50(B)', 'value': 0.98591}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.8781}, {'tag': 'val/box_loss', 'value': 0.55408}, {'tag': 'val/cls_loss', 'value': 1.8485}, {'tag': 'val/dfl_loss', 'value': 0.796}, {'tag': 'lr/pg0', 'value': 0.014293}, {'tag': 'lr/pg1', 'value': 0.0012928}, {'tag': 'lr/pg2', 'value': 0.0012928}], [{'tag': 'epoch', 'value': 44.0}, {'tag': 'train/box_loss', 'value': 0.42274}, {'tag': 'train/cls_loss', 'value': 0.67743}, {'tag': 'train/dfl_loss', 'value': 0.85156}, {'tag': 'metrics/precision(B)', 'value': 1.0}, {'tag': 'metrics/recall(B)', 'value': 0.68122}, {'tag': 'metrics/mAP50(B)', 'value': 0.98591}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.88629}, {'tag': 'val/box_loss', 'value': 0.59008}, {'tag': 'val/cls_loss', 'value': 1.8945}, {'tag': 'val/dfl_loss', 'value': 0.80049}, {'tag': 'lr/pg0', 'value': 0.012146}, {'tag': 'lr/pg1', 'value': 0.0011463}, {'tag': 'lr/pg2', 'value': 0.0011463}], [{'tag': 'epoch', 'value': 45.0}, {'tag': 'train/box_loss', 'value': 0.44008}, {'tag': 'train/cls_loss', 'value': 0.68818}, {'tag': 'train/dfl_loss', 'value': 0.79383}, {'tag': 'metrics/precision(B)', 'value': 1.0}, {'tag': 'metrics/recall(B)', 'value': 0.75958}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.90281}, {'tag': 'val/box_loss', 'value': 0.48885}, {'tag': 'val/cls_loss', 'value': 1.8046}, {'tag': 'val/dfl_loss', 'value': 0.78786}, {'tag': 'lr/pg0', 'value': 0.0099919}, {'tag': 'lr/pg1', 'value': 0.0009919}, {'tag': 'lr/pg2', 'value': 0.0009919}], [{'tag': 'epoch', 'value': 46.0}, {'tag': 'train/box_loss', 'value': 0.42351}, {'tag': 'train/cls_loss', 'value': 0.68141}, {'tag': 'train/dfl_loss', 'value': 0.81276}, {'tag': 'metrics/precision(B)', 'value': 1.0}, {'tag': 'metrics/recall(B)', 'value': 0.84113}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.90002}, {'tag': 'val/box_loss', 'value': 0.52886}, {'tag': 'val/cls_loss', 'value': 1.6638}, {'tag': 'val/dfl_loss', 'value': 0.79098}, {'tag': 'lr/pg0', 'value': 0.0078296}, {'tag': 'lr/pg1', 'value': 0.00082956}, {'tag': 'lr/pg2', 'value': 0.00082956}], [{'tag': 'epoch', 'value': 47.0}, {'tag': 'train/box_loss', 'value': 0.43221}, {'tag': 'train/cls_loss', 'value': 0.68535}, {'tag': 'train/dfl_loss', 'value': 0.81431}, {'tag': 'metrics/precision(B)', 'value': 0.99289}, {'tag': 'metrics/recall(B)', 'value': 0.97043}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.90593}, {'tag': 'val/box_loss', 'value': 0.48647}, {'tag': 'val/cls_loss', 'value': 1.5789}, {'tag': 'val/dfl_loss', 'value': 0.78382}, {'tag': 'lr/pg0', 'value': 0.0056593}, {'tag': 'lr/pg1', 'value': 0.0006593}, {'tag': 'lr/pg2', 'value': 0.0006593}], [{'tag': 'epoch', 'value': 48.0}, {'tag': 'train/box_loss', 'value': 0.39935}, {'tag': 'train/cls_loss', 'value': 0.66413}, {'tag': 'train/dfl_loss', 'value': 0.80588}, {'tag': 'metrics/precision(B)', 'value': 0.99488}, {'tag': 'metrics/recall(B)', 'value': 0.98173}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.8921}, {'tag': 'val/box_loss', 'value': 0.50336}, {'tag': 'val/cls_loss', 'value': 1.557}, {'tag': 'val/dfl_loss', 'value': 0.78832}, {'tag': 'lr/pg0', 'value': 0.0034811}, {'tag': 'lr/pg1', 'value': 0.00048112}, {'tag': 'lr/pg2', 'value': 0.00048112}], [{'tag': 'epoch', 'value': 49.0}, {'tag': 'train/box_loss', 'value': 0.41614}, {'tag': 'train/cls_loss', 'value': 0.62133}, {'tag': 'train/dfl_loss', 'value': 0.809}, {'tag': 'metrics/precision(B)', 'value': 0.99142}, {'tag': 'metrics/recall(B)', 'value': 1.0}, {'tag': 'metrics/mAP50(B)', 'value': 0.995}, {'tag': 'metrics/mAP50-95(B)', 'value': 0.9074}, {'tag': 'val/box_loss', 'value': 0.49075}, {'tag': 'val/cls_loss', 'value': 1.4943}, {'tag': 'val/dfl_loss', 'value': 0.78682}, {'tag': 'lr/pg0', 'value': 0.001295}, {'tag': 'lr/pg1', 'value': 0.00029502}, {'tag': 'lr/pg2', 'value': 0.00029502}]], eval_metrics=[{'tag': 'mAP', 'value': 0.3143564462661743}])</pre> In\u00a0[8]: Copied! <pre>hub.get_metrics()\n</pre> hub.get_metrics() Out[8]: <pre>[[{'tag': 'epoch', 'value': 0.0},\n  {'tag': 'train/box_loss', 'value': 1.3501},\n  {'tag': 'train/cls_loss', 'value': 4.6464},\n  {'tag': 'train/dfl_loss', 'value': 1.0351},\n  {'tag': 'metrics/precision(B)', 'value': 0.00268},\n  {'tag': 'metrics/recall(B)', 'value': 0.8},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.27948},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.22095},\n  {'tag': 'val/box_loss', 'value': 0.43408},\n  {'tag': 'val/cls_loss', 'value': 3.8634},\n  {'tag': 'val/dfl_loss', 'value': 0.82598},\n  {'tag': 'lr/pg0', 'value': 0.0991},\n  {'tag': 'lr/pg1', 'value': 0.0001},\n  {'tag': 'lr/pg2', 'value': 0.0001}],\n [{'tag': 'epoch', 'value': 1.0},\n  {'tag': 'train/box_loss', 'value': 1.5745},\n  {'tag': 'train/cls_loss', 'value': 5.0921},\n  {'tag': 'train/dfl_loss', 'value': 1.1791},\n  {'tag': 'metrics/precision(B)', 'value': 0.00301},\n  {'tag': 'metrics/recall(B)', 'value': 0.9},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.28713},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.23503},\n  {'tag': 'val/box_loss', 'value': 0.42816},\n  {'tag': 'val/cls_loss', 'value': 3.8297},\n  {'tag': 'val/dfl_loss', 'value': 0.81922},\n  {'tag': 'lr/pg0', 'value': 0.097294},\n  {'tag': 'lr/pg1', 'value': 0.00029406},\n  {'tag': 'lr/pg2', 'value': 0.00029406}],\n [{'tag': 'epoch', 'value': 2.0},\n  {'tag': 'train/box_loss', 'value': 1.4092},\n  {'tag': 'train/cls_loss', 'value': 4.601},\n  {'tag': 'train/dfl_loss', 'value': 1.0856},\n  {'tag': 'metrics/precision(B)', 'value': 0.00318},\n  {'tag': 'metrics/recall(B)', 'value': 0.95},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.29955},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.2428},\n  {'tag': 'val/box_loss', 'value': 0.44174},\n  {'tag': 'val/cls_loss', 'value': 3.8018},\n  {'tag': 'val/dfl_loss', 'value': 0.80306},\n  {'tag': 'lr/pg0', 'value': 0.09548},\n  {'tag': 'lr/pg1', 'value': 0.0004802},\n  {'tag': 'lr/pg2', 'value': 0.0004802}],\n [{'tag': 'epoch', 'value': 3.0},\n  {'tag': 'train/box_loss', 'value': 1.3949},\n  {'tag': 'train/cls_loss', 'value': 4.7847},\n  {'tag': 'train/dfl_loss', 'value': 1.0853},\n  {'tag': 'metrics/precision(B)', 'value': 0.00317},\n  {'tag': 'metrics/recall(B)', 'value': 0.95},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.30017},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.25635},\n  {'tag': 'val/box_loss', 'value': 0.4457},\n  {'tag': 'val/cls_loss', 'value': 3.7635},\n  {'tag': 'val/dfl_loss', 'value': 0.79997},\n  {'tag': 'lr/pg0', 'value': 0.093658},\n  {'tag': 'lr/pg1', 'value': 0.00065842},\n  {'tag': 'lr/pg2', 'value': 0.00065842}],\n [{'tag': 'epoch', 'value': 4.0},\n  {'tag': 'train/box_loss', 'value': 1.4621},\n  {'tag': 'train/cls_loss', 'value': 4.5888},\n  {'tag': 'train/dfl_loss', 'value': 1.0952},\n  {'tag': 'metrics/precision(B)', 'value': 0.00317},\n  {'tag': 'metrics/recall(B)', 'value': 0.95},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.29295},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.2701},\n  {'tag': 'val/box_loss', 'value': 0.43005},\n  {'tag': 'val/cls_loss', 'value': 3.7548},\n  {'tag': 'val/dfl_loss', 'value': 0.80128},\n  {'tag': 'lr/pg0', 'value': 0.091829},\n  {'tag': 'lr/pg1', 'value': 0.00082872},\n  {'tag': 'lr/pg2', 'value': 0.00082872}],\n [{'tag': 'epoch', 'value': 5.0},\n  {'tag': 'train/box_loss', 'value': 1.3964},\n  {'tag': 'train/cls_loss', 'value': 4.9783},\n  {'tag': 'train/dfl_loss', 'value': 1.0798},\n  {'tag': 'metrics/precision(B)', 'value': 0.00333},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.29703},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.24147},\n  {'tag': 'val/box_loss', 'value': 0.41959},\n  {'tag': 'val/cls_loss', 'value': 3.7923},\n  {'tag': 'val/dfl_loss', 'value': 0.8005},\n  {'tag': 'lr/pg0', 'value': 0.089991},\n  {'tag': 'lr/pg1', 'value': 0.0009911},\n  {'tag': 'lr/pg2', 'value': 0.0009911}],\n [{'tag': 'epoch', 'value': 6.0},\n  {'tag': 'train/box_loss', 'value': 1.2091},\n  {'tag': 'train/cls_loss', 'value': 4.5448},\n  {'tag': 'train/dfl_loss', 'value': 0.99952},\n  {'tag': 'metrics/precision(B)', 'value': 0.00337},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.2619},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.21933},\n  {'tag': 'val/box_loss', 'value': 0.42009},\n  {'tag': 'val/cls_loss', 'value': 3.8168},\n  {'tag': 'val/dfl_loss', 'value': 0.81327},\n  {'tag': 'lr/pg0', 'value': 0.088146},\n  {'tag': 'lr/pg1', 'value': 0.0011456},\n  {'tag': 'lr/pg2', 'value': 0.0011456}],\n [{'tag': 'epoch', 'value': 7.0},\n  {'tag': 'train/box_loss', 'value': 0.98263},\n  {'tag': 'train/cls_loss', 'value': 4.3317},\n  {'tag': 'train/dfl_loss', 'value': 0.94503},\n  {'tag': 'metrics/precision(B)', 'value': 0.00337},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.23364},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.21407},\n  {'tag': 'val/box_loss', 'value': 0.49657},\n  {'tag': 'val/cls_loss', 'value': 3.792},\n  {'tag': 'val/dfl_loss', 'value': 0.82097},\n  {'tag': 'lr/pg0', 'value': 0.086292},\n  {'tag': 'lr/pg1', 'value': 0.0012921},\n  {'tag': 'lr/pg2', 'value': 0.0012921}],\n [{'tag': 'epoch', 'value': 8.0},\n  {'tag': 'train/box_loss', 'value': 0.65743},\n  {'tag': 'train/cls_loss', 'value': 4.0554},\n  {'tag': 'train/dfl_loss', 'value': 0.84959},\n  {'tag': 'metrics/precision(B)', 'value': 0.00333},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.38451},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.3307},\n  {'tag': 'val/box_loss', 'value': 0.58206},\n  {'tag': 'val/cls_loss', 'value': 3.7688},\n  {'tag': 'val/dfl_loss', 'value': 0.82093},\n  {'tag': 'lr/pg0', 'value': 0.084431},\n  {'tag': 'lr/pg1', 'value': 0.0014307},\n  {'tag': 'lr/pg2', 'value': 0.0014307}],\n [{'tag': 'epoch', 'value': 9.0},\n  {'tag': 'train/box_loss', 'value': 0.63434},\n  {'tag': 'train/cls_loss', 'value': 3.8742},\n  {'tag': 'train/dfl_loss', 'value': 0.83304},\n  {'tag': 'metrics/precision(B)', 'value': 0.00337},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.515},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.45197},\n  {'tag': 'val/box_loss', 'value': 0.60736},\n  {'tag': 'val/cls_loss', 'value': 3.6357},\n  {'tag': 'val/dfl_loss', 'value': 0.81601},\n  {'tag': 'lr/pg0', 'value': 0.082561},\n  {'tag': 'lr/pg1', 'value': 0.0015614},\n  {'tag': 'lr/pg2', 'value': 0.0015614}],\n [{'tag': 'epoch', 'value': 10.0},\n  {'tag': 'train/box_loss', 'value': 0.61282},\n  {'tag': 'train/cls_loss', 'value': 3.512},\n  {'tag': 'train/dfl_loss', 'value': 0.79209},\n  {'tag': 'metrics/precision(B)', 'value': 0.00339},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.60177},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.49371},\n  {'tag': 'val/box_loss', 'value': 0.64081},\n  {'tag': 'val/cls_loss', 'value': 3.4475},\n  {'tag': 'val/dfl_loss', 'value': 0.80918},\n  {'tag': 'lr/pg0', 'value': 0.080684},\n  {'tag': 'lr/pg1', 'value': 0.0016842},\n  {'tag': 'lr/pg2', 'value': 0.0016842}],\n [{'tag': 'epoch', 'value': 11.0},\n  {'tag': 'train/box_loss', 'value': 0.67718},\n  {'tag': 'train/cls_loss', 'value': 2.9147},\n  {'tag': 'train/dfl_loss', 'value': 0.83244},\n  {'tag': 'metrics/precision(B)', 'value': 0.00338},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.69629},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.582},\n  {'tag': 'val/box_loss', 'value': 0.59014},\n  {'tag': 'val/cls_loss', 'value': 3.2775},\n  {'tag': 'val/dfl_loss', 'value': 0.78841},\n  {'tag': 'lr/pg0', 'value': 0.078799},\n  {'tag': 'lr/pg1', 'value': 0.0017991},\n  {'tag': 'lr/pg2', 'value': 0.0017991}],\n [{'tag': 'epoch', 'value': 12.0},\n  {'tag': 'train/box_loss', 'value': 0.61112},\n  {'tag': 'train/cls_loss', 'value': 2.2224},\n  {'tag': 'train/dfl_loss', 'value': 0.86835},\n  {'tag': 'metrics/precision(B)', 'value': 0.0034},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.71624},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.60464},\n  {'tag': 'val/box_loss', 'value': 0.57163},\n  {'tag': 'val/cls_loss', 'value': 3.2206},\n  {'tag': 'val/dfl_loss', 'value': 0.79575},\n  {'tag': 'lr/pg0', 'value': 0.076906},\n  {'tag': 'lr/pg1', 'value': 0.001906},\n  {'tag': 'lr/pg2', 'value': 0.001906}],\n [{'tag': 'epoch', 'value': 13.0},\n  {'tag': 'train/box_loss', 'value': 0.59459},\n  {'tag': 'train/cls_loss', 'value': 1.9712},\n  {'tag': 'train/dfl_loss', 'value': 0.80912},\n  {'tag': 'metrics/precision(B)', 'value': 0.00339},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.75376},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.59454},\n  {'tag': 'val/box_loss', 'value': 0.60448},\n  {'tag': 'val/cls_loss', 'value': 3.2301},\n  {'tag': 'val/dfl_loss', 'value': 0.79842},\n  {'tag': 'lr/pg0', 'value': 0.075005},\n  {'tag': 'lr/pg1', 'value': 0.002005},\n  {'tag': 'lr/pg2', 'value': 0.002005}],\n [{'tag': 'epoch', 'value': 14.0},\n  {'tag': 'train/box_loss', 'value': 0.71785},\n  {'tag': 'train/cls_loss', 'value': 1.9547},\n  {'tag': 'train/dfl_loss', 'value': 0.98837},\n  {'tag': 'metrics/precision(B)', 'value': 0.0034},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.77386},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.65288},\n  {'tag': 'val/box_loss', 'value': 0.49612},\n  {'tag': 'val/cls_loss', 'value': 3.2141},\n  {'tag': 'val/dfl_loss', 'value': 0.79957},\n  {'tag': 'lr/pg0', 'value': 0.073096},\n  {'tag': 'lr/pg1', 'value': 0.0020961},\n  {'tag': 'lr/pg2', 'value': 0.0020961}],\n [{'tag': 'epoch', 'value': 15.0},\n  {'tag': 'train/box_loss', 'value': 0.6793},\n  {'tag': 'train/cls_loss', 'value': 1.7516},\n  {'tag': 'train/dfl_loss', 'value': 0.86429},\n  {'tag': 'metrics/precision(B)', 'value': 0.00338},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.81266},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.69205},\n  {'tag': 'val/box_loss', 'value': 0.57349},\n  {'tag': 'val/cls_loss', 'value': 3.2207},\n  {'tag': 'val/dfl_loss', 'value': 0.81375},\n  {'tag': 'lr/pg0', 'value': 0.071179},\n  {'tag': 'lr/pg1', 'value': 0.0021793},\n  {'tag': 'lr/pg2', 'value': 0.0021793}],\n [{'tag': 'epoch', 'value': 16.0},\n  {'tag': 'train/box_loss', 'value': 0.69685},\n  {'tag': 'train/cls_loss', 'value': 1.5918},\n  {'tag': 'train/dfl_loss', 'value': 0.93667},\n  {'tag': 'metrics/precision(B)', 'value': 0.00336},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.8569},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.68663},\n  {'tag': 'val/box_loss', 'value': 0.58732},\n  {'tag': 'val/cls_loss', 'value': 3.2442},\n  {'tag': 'val/dfl_loss', 'value': 0.8026},\n  {'tag': 'lr/pg0', 'value': 0.069255},\n  {'tag': 'lr/pg1', 'value': 0.0022546},\n  {'tag': 'lr/pg2', 'value': 0.0022546}],\n [{'tag': 'epoch', 'value': 17.0},\n  {'tag': 'train/box_loss', 'value': 0.63334},\n  {'tag': 'train/cls_loss', 'value': 1.4425},\n  {'tag': 'train/dfl_loss', 'value': 0.86646},\n  {'tag': 'metrics/precision(B)', 'value': 0.00339},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.87667},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.73053},\n  {'tag': 'val/box_loss', 'value': 0.50378},\n  {'tag': 'val/cls_loss', 'value': 3.2284},\n  {'tag': 'val/dfl_loss', 'value': 0.8031},\n  {'tag': 'lr/pg0', 'value': 0.067322},\n  {'tag': 'lr/pg1', 'value': 0.0023219},\n  {'tag': 'lr/pg2', 'value': 0.0023219}],\n [{'tag': 'epoch', 'value': 18.0},\n  {'tag': 'train/box_loss', 'value': 0.53245},\n  {'tag': 'train/cls_loss', 'value': 1.4508},\n  {'tag': 'train/dfl_loss', 'value': 0.80017},\n  {'tag': 'metrics/precision(B)', 'value': 0.0034},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.86682},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.71193},\n  {'tag': 'val/box_loss', 'value': 0.5868},\n  {'tag': 'val/cls_loss', 'value': 3.2135},\n  {'tag': 'val/dfl_loss', 'value': 0.81075},\n  {'tag': 'lr/pg0', 'value': 0.065381},\n  {'tag': 'lr/pg1', 'value': 0.0023813},\n  {'tag': 'lr/pg2', 'value': 0.0023813}],\n [{'tag': 'epoch', 'value': 19.0},\n  {'tag': 'train/box_loss', 'value': 0.58909},\n  {'tag': 'train/cls_loss', 'value': 1.3975},\n  {'tag': 'train/dfl_loss', 'value': 0.86834},\n  {'tag': 'metrics/precision(B)', 'value': 0.00346},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.87881},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.73247},\n  {'tag': 'val/box_loss', 'value': 0.60195},\n  {'tag': 'val/cls_loss', 'value': 3.237},\n  {'tag': 'val/dfl_loss', 'value': 0.81828},\n  {'tag': 'lr/pg0', 'value': 0.063433},\n  {'tag': 'lr/pg1', 'value': 0.0024328},\n  {'tag': 'lr/pg2', 'value': 0.0024328}],\n [{'tag': 'epoch', 'value': 20.0},\n  {'tag': 'train/box_loss', 'value': 0.53003},\n  {'tag': 'train/cls_loss', 'value': 1.2732},\n  {'tag': 'train/dfl_loss', 'value': 0.80269},\n  {'tag': 'metrics/precision(B)', 'value': 0.00371},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.95167},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.83357},\n  {'tag': 'val/box_loss', 'value': 0.50689},\n  {'tag': 'val/cls_loss', 'value': 3.202},\n  {'tag': 'val/dfl_loss', 'value': 0.8019},\n  {'tag': 'lr/pg0', 'value': 0.061476},\n  {'tag': 'lr/pg1', 'value': 0.0024764},\n  {'tag': 'lr/pg2', 'value': 0.0024764}],\n [{'tag': 'epoch', 'value': 21.0},\n  {'tag': 'train/box_loss', 'value': 0.69022},\n  {'tag': 'train/cls_loss', 'value': 1.2731},\n  {'tag': 'train/dfl_loss', 'value': 0.89605},\n  {'tag': 'metrics/precision(B)', 'value': 0.00391},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.94833},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.78659},\n  {'tag': 'val/box_loss', 'value': 0.60027},\n  {'tag': 'val/cls_loss', 'value': 3.2329},\n  {'tag': 'val/dfl_loss', 'value': 0.82252},\n  {'tag': 'lr/pg0', 'value': 0.059512},\n  {'tag': 'lr/pg1', 'value': 0.0025121},\n  {'tag': 'lr/pg2', 'value': 0.0025121}],\n [{'tag': 'epoch', 'value': 22.0},\n  {'tag': 'train/box_loss', 'value': 0.68866},\n  {'tag': 'train/cls_loss', 'value': 1.321},\n  {'tag': 'train/dfl_loss', 'value': 0.89818},\n  {'tag': 'metrics/precision(B)', 'value': 0.00383},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.97227},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.83799},\n  {'tag': 'val/box_loss', 'value': 0.56671},\n  {'tag': 'val/cls_loss', 'value': 3.2398},\n  {'tag': 'val/dfl_loss', 'value': 0.8096},\n  {'tag': 'lr/pg0', 'value': 0.05754},\n  {'tag': 'lr/pg1', 'value': 0.0025398},\n  {'tag': 'lr/pg2', 'value': 0.0025398}],\n [{'tag': 'epoch', 'value': 23.0},\n  {'tag': 'train/box_loss', 'value': 0.64262},\n  {'tag': 'train/cls_loss', 'value': 1.1323},\n  {'tag': 'train/dfl_loss', 'value': 0.91579},\n  {'tag': 'metrics/precision(B)', 'value': 0.0037},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.95864},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.80125},\n  {'tag': 'val/box_loss', 'value': 0.53914},\n  {'tag': 'val/cls_loss', 'value': 3.2466},\n  {'tag': 'val/dfl_loss', 'value': 0.80866},\n  {'tag': 'lr/pg0', 'value': 0.05556},\n  {'tag': 'lr/pg1', 'value': 0.0025596},\n  {'tag': 'lr/pg2', 'value': 0.0025596}],\n [{'tag': 'epoch', 'value': 24.0},\n  {'tag': 'train/box_loss', 'value': 0.60471},\n  {'tag': 'train/cls_loss', 'value': 1.1004},\n  {'tag': 'train/dfl_loss', 'value': 0.86974},\n  {'tag': 'metrics/precision(B)', 'value': 0.00355},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.99045},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.79496},\n  {'tag': 'val/box_loss', 'value': 0.6285},\n  {'tag': 'val/cls_loss', 'value': 3.173},\n  {'tag': 'val/dfl_loss', 'value': 0.81681},\n  {'tag': 'lr/pg0', 'value': 0.053572},\n  {'tag': 'lr/pg1', 'value': 0.0025715},\n  {'tag': 'lr/pg2', 'value': 0.0025715}],\n [{'tag': 'epoch', 'value': 25.0},\n  {'tag': 'train/box_loss', 'value': 0.60922},\n  {'tag': 'train/cls_loss', 'value': 1.0412},\n  {'tag': 'train/dfl_loss', 'value': 0.87879},\n  {'tag': 'metrics/precision(B)', 'value': 0.00349},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.97682},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.83378},\n  {'tag': 'val/box_loss', 'value': 0.58855},\n  {'tag': 'val/cls_loss', 'value': 3.1961},\n  {'tag': 'val/dfl_loss', 'value': 0.81326},\n  {'tag': 'lr/pg0', 'value': 0.051576},\n  {'tag': 'lr/pg1', 'value': 0.0025755},\n  {'tag': 'lr/pg2', 'value': 0.0025755}],\n [{'tag': 'epoch', 'value': 26.0},\n  {'tag': 'train/box_loss', 'value': 0.58924},\n  {'tag': 'train/cls_loss', 'value': 1.0289},\n  {'tag': 'train/dfl_loss', 'value': 0.82589},\n  {'tag': 'metrics/precision(B)', 'value': 0.0034},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.98136},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.82034},\n  {'tag': 'val/box_loss', 'value': 0.64944},\n  {'tag': 'val/cls_loss', 'value': 3.1635},\n  {'tag': 'val/dfl_loss', 'value': 0.81535},\n  {'tag': 'lr/pg0', 'value': 0.049572},\n  {'tag': 'lr/pg1', 'value': 0.0025716},\n  {'tag': 'lr/pg2', 'value': 0.0025716}],\n [{'tag': 'epoch', 'value': 27.0},\n  {'tag': 'train/box_loss', 'value': 0.57216},\n  {'tag': 'train/cls_loss', 'value': 0.96613},\n  {'tag': 'train/dfl_loss', 'value': 0.87305},\n  {'tag': 'metrics/precision(B)', 'value': 0.00338},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.84577},\n  {'tag': 'val/box_loss', 'value': 0.56407},\n  {'tag': 'val/cls_loss', 'value': 3.0513},\n  {'tag': 'val/dfl_loss', 'value': 0.8024},\n  {'tag': 'lr/pg0', 'value': 0.04756},\n  {'tag': 'lr/pg1', 'value': 0.0025597},\n  {'tag': 'lr/pg2', 'value': 0.0025597}],\n [{'tag': 'epoch', 'value': 28.0},\n  {'tag': 'train/box_loss', 'value': 0.53161},\n  {'tag': 'train/cls_loss', 'value': 0.96257},\n  {'tag': 'train/dfl_loss', 'value': 0.82785},\n  {'tag': 'metrics/precision(B)', 'value': 0.00336},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.9699},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.82215},\n  {'tag': 'val/box_loss', 'value': 0.58077},\n  {'tag': 'val/cls_loss', 'value': 3.0708},\n  {'tag': 'val/dfl_loss', 'value': 0.80563},\n  {'tag': 'lr/pg0', 'value': 0.04554},\n  {'tag': 'lr/pg1', 'value': 0.0025399},\n  {'tag': 'lr/pg2', 'value': 0.0025399}],\n [{'tag': 'epoch', 'value': 29.0},\n  {'tag': 'train/box_loss', 'value': 0.59106},\n  {'tag': 'train/cls_loss', 'value': 0.95372},\n  {'tag': 'train/dfl_loss', 'value': 0.8898},\n  {'tag': 'metrics/precision(B)', 'value': 0.00363},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.97761},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.82256},\n  {'tag': 'val/box_loss', 'value': 0.63684},\n  {'tag': 'val/cls_loss', 'value': 2.9952},\n  {'tag': 'val/dfl_loss', 'value': 0.81421},\n  {'tag': 'lr/pg0', 'value': 0.043512},\n  {'tag': 'lr/pg1', 'value': 0.0025122},\n  {'tag': 'lr/pg2', 'value': 0.0025122}],\n [{'tag': 'epoch', 'value': 30.0},\n  {'tag': 'train/box_loss', 'value': 0.54118},\n  {'tag': 'train/cls_loss', 'value': 0.99789},\n  {'tag': 'train/dfl_loss', 'value': 0.82375},\n  {'tag': 'metrics/precision(B)', 'value': 0.00342},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.85187},\n  {'tag': 'val/box_loss', 'value': 0.5704},\n  {'tag': 'val/cls_loss', 'value': 2.8481},\n  {'tag': 'val/dfl_loss', 'value': 0.80421},\n  {'tag': 'lr/pg0', 'value': 0.041477},\n  {'tag': 'lr/pg1', 'value': 0.0024766},\n  {'tag': 'lr/pg2', 'value': 0.0024766}],\n [{'tag': 'epoch', 'value': 31.0},\n  {'tag': 'train/box_loss', 'value': 0.5441},\n  {'tag': 'train/cls_loss', 'value': 0.84697},\n  {'tag': 'train/dfl_loss', 'value': 0.83413},\n  {'tag': 'metrics/precision(B)', 'value': 0.00339},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.80418},\n  {'tag': 'val/box_loss', 'value': 0.65833},\n  {'tag': 'val/cls_loss', 'value': 2.6579},\n  {'tag': 'val/dfl_loss', 'value': 0.81958},\n  {'tag': 'lr/pg0', 'value': 0.039433},\n  {'tag': 'lr/pg1', 'value': 0.0024331},\n  {'tag': 'lr/pg2', 'value': 0.0024331}],\n [{'tag': 'epoch', 'value': 32.0},\n  {'tag': 'train/box_loss', 'value': 0.54969},\n  {'tag': 'train/cls_loss', 'value': 0.87359},\n  {'tag': 'train/dfl_loss', 'value': 0.80767},\n  {'tag': 'metrics/precision(B)', 'value': 0.00334},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.85862},\n  {'tag': 'val/box_loss', 'value': 0.53264},\n  {'tag': 'val/cls_loss', 'value': 2.622},\n  {'tag': 'val/dfl_loss', 'value': 0.7936},\n  {'tag': 'lr/pg0', 'value': 0.037382},\n  {'tag': 'lr/pg1', 'value': 0.0023816},\n  {'tag': 'lr/pg2', 'value': 0.0023816}],\n [{'tag': 'epoch', 'value': 33.0},\n  {'tag': 'train/box_loss', 'value': 0.494},\n  {'tag': 'train/cls_loss', 'value': 0.80329},\n  {'tag': 'train/dfl_loss', 'value': 0.81615},\n  {'tag': 'metrics/precision(B)', 'value': 0.00353},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.87549},\n  {'tag': 'val/box_loss', 'value': 0.52511},\n  {'tag': 'val/cls_loss', 'value': 2.6191},\n  {'tag': 'val/dfl_loss', 'value': 0.78882},\n  {'tag': 'lr/pg0', 'value': 0.035322},\n  {'tag': 'lr/pg1', 'value': 0.0023222},\n  {'tag': 'lr/pg2', 'value': 0.0023222}],\n [{'tag': 'epoch', 'value': 34.0},\n  {'tag': 'train/box_loss', 'value': 0.57125},\n  {'tag': 'train/cls_loss', 'value': 0.83759},\n  {'tag': 'train/dfl_loss', 'value': 0.85834},\n  {'tag': 'metrics/precision(B)', 'value': 0.97521},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.81067},\n  {'tag': 'val/box_loss', 'value': 0.63479},\n  {'tag': 'val/cls_loss', 'value': 2.5756},\n  {'tag': 'val/dfl_loss', 'value': 0.80614},\n  {'tag': 'lr/pg0', 'value': 0.033255},\n  {'tag': 'lr/pg1', 'value': 0.0022549},\n  {'tag': 'lr/pg2', 'value': 0.0022549}],\n [{'tag': 'epoch', 'value': 35.0},\n  {'tag': 'train/box_loss', 'value': 0.5074},\n  {'tag': 'train/cls_loss', 'value': 0.77593},\n  {'tag': 'train/dfl_loss', 'value': 0.84894},\n  {'tag': 'metrics/precision(B)', 'value': 1.0},\n  {'tag': 'metrics/recall(B)', 'value': 0.55576},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.84226},\n  {'tag': 'val/box_loss', 'value': 0.59675},\n  {'tag': 'val/cls_loss', 'value': 2.3746},\n  {'tag': 'val/dfl_loss', 'value': 0.79787},\n  {'tag': 'lr/pg0', 'value': 0.03118},\n  {'tag': 'lr/pg1', 'value': 0.0021797},\n  {'tag': 'lr/pg2', 'value': 0.0021797}],\n [{'tag': 'epoch', 'value': 36.0},\n  {'tag': 'train/box_loss', 'value': 0.5478},\n  {'tag': 'train/cls_loss', 'value': 0.9739},\n  {'tag': 'train/dfl_loss', 'value': 0.8804},\n  {'tag': 'metrics/precision(B)', 'value': 0.00335},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.88408},\n  {'tag': 'val/box_loss', 'value': 0.49261},\n  {'tag': 'val/cls_loss', 'value': 2.5684},\n  {'tag': 'val/dfl_loss', 'value': 0.7895},\n  {'tag': 'lr/pg0', 'value': 0.029097},\n  {'tag': 'lr/pg1', 'value': 0.0020966},\n  {'tag': 'lr/pg2', 'value': 0.0020966}],\n [{'tag': 'epoch', 'value': 37.0},\n  {'tag': 'train/box_loss', 'value': 0.5109},\n  {'tag': 'train/cls_loss', 'value': 0.79298},\n  {'tag': 'train/dfl_loss', 'value': 0.81025},\n  {'tag': 'metrics/precision(B)', 'value': 0.94683},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.87914},\n  {'tag': 'val/box_loss', 'value': 0.53933},\n  {'tag': 'val/cls_loss', 'value': 2.4032},\n  {'tag': 'val/dfl_loss', 'value': 0.79472},\n  {'tag': 'lr/pg0', 'value': 0.027006},\n  {'tag': 'lr/pg1', 'value': 0.0020055},\n  {'tag': 'lr/pg2', 'value': 0.0020055}],\n [{'tag': 'epoch', 'value': 38.0},\n  {'tag': 'train/box_loss', 'value': 0.5767},\n  {'tag': 'train/cls_loss', 'value': 0.86935},\n  {'tag': 'train/dfl_loss', 'value': 0.87058},\n  {'tag': 'metrics/precision(B)', 'value': 0.98576},\n  {'tag': 'metrics/recall(B)', 'value': 0.83699},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.87418},\n  {'tag': 'val/box_loss', 'value': 0.59056},\n  {'tag': 'val/cls_loss', 'value': 2.3744},\n  {'tag': 'val/dfl_loss', 'value': 0.79638},\n  {'tag': 'lr/pg0', 'value': 0.024907},\n  {'tag': 'lr/pg1', 'value': 0.0019065},\n  {'tag': 'lr/pg2', 'value': 0.0019065}],\n [{'tag': 'epoch', 'value': 39.0},\n  {'tag': 'train/box_loss', 'value': 0.59383},\n  {'tag': 'train/cls_loss', 'value': 0.80533},\n  {'tag': 'train/dfl_loss', 'value': 0.87874},\n  {'tag': 'metrics/precision(B)', 'value': 1.0},\n  {'tag': 'metrics/recall(B)', 'value': 0.4442},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.99045},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.88933},\n  {'tag': 'val/box_loss', 'value': 0.58402},\n  {'tag': 'val/cls_loss', 'value': 2.2775},\n  {'tag': 'val/dfl_loss', 'value': 0.81},\n  {'tag': 'lr/pg0', 'value': 0.0228},\n  {'tag': 'lr/pg1', 'value': 0.0017996},\n  {'tag': 'lr/pg2', 'value': 0.0017996}],\n [{'tag': 'epoch', 'value': 40.0},\n  {'tag': 'train/box_loss', 'value': 0.4485},\n  {'tag': 'train/cls_loss', 'value': 0.84811},\n  {'tag': 'train/dfl_loss', 'value': 0.81296},\n  {'tag': 'metrics/precision(B)', 'value': 1.0},\n  {'tag': 'metrics/recall(B)', 'value': 0.35018},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.97167},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.8598},\n  {'tag': 'val/box_loss', 'value': 0.58368},\n  {'tag': 'val/cls_loss', 'value': 2.2709},\n  {'tag': 'val/dfl_loss', 'value': 0.81143},\n  {'tag': 'lr/pg0', 'value': 0.020685},\n  {'tag': 'lr/pg1', 'value': 0.0016848},\n  {'tag': 'lr/pg2', 'value': 0.0016848}],\n [{'tag': 'epoch', 'value': 41.0},\n  {'tag': 'train/box_loss', 'value': 0.5216},\n  {'tag': 'train/cls_loss', 'value': 0.7309},\n  {'tag': 'train/dfl_loss', 'value': 0.8386},\n  {'tag': 'metrics/precision(B)', 'value': 1.0},\n  {'tag': 'metrics/recall(B)', 'value': 0.56993},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.98136},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.88798},\n  {'tag': 'val/box_loss', 'value': 0.56186},\n  {'tag': 'val/cls_loss', 'value': 2.1034},\n  {'tag': 'val/dfl_loss', 'value': 0.80235},\n  {'tag': 'lr/pg0', 'value': 0.018562},\n  {'tag': 'lr/pg1', 'value': 0.0015621},\n  {'tag': 'lr/pg2', 'value': 0.0015621}],\n [{'tag': 'epoch', 'value': 42.0},\n  {'tag': 'train/box_loss', 'value': 0.46109},\n  {'tag': 'train/cls_loss', 'value': 0.75215},\n  {'tag': 'train/dfl_loss', 'value': 0.81637},\n  {'tag': 'metrics/precision(B)', 'value': 0.94392},\n  {'tag': 'metrics/recall(B)', 'value': 0.61791},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.98136},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.85015},\n  {'tag': 'val/box_loss', 'value': 0.64211},\n  {'tag': 'val/cls_loss', 'value': 1.9825},\n  {'tag': 'val/dfl_loss', 'value': 0.81823},\n  {'tag': 'lr/pg0', 'value': 0.016431},\n  {'tag': 'lr/pg1', 'value': 0.0014314},\n  {'tag': 'lr/pg2', 'value': 0.0014314}],\n [{'tag': 'epoch', 'value': 43.0},\n  {'tag': 'train/box_loss', 'value': 0.47991},\n  {'tag': 'train/cls_loss', 'value': 0.84562},\n  {'tag': 'train/dfl_loss', 'value': 0.816},\n  {'tag': 'metrics/precision(B)', 'value': 0.96134},\n  {'tag': 'metrics/recall(B)', 'value': 0.72676},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.98591},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.8781},\n  {'tag': 'val/box_loss', 'value': 0.55408},\n  {'tag': 'val/cls_loss', 'value': 1.8485},\n  {'tag': 'val/dfl_loss', 'value': 0.796},\n  {'tag': 'lr/pg0', 'value': 0.014293},\n  {'tag': 'lr/pg1', 'value': 0.0012928},\n  {'tag': 'lr/pg2', 'value': 0.0012928}],\n [{'tag': 'epoch', 'value': 44.0},\n  {'tag': 'train/box_loss', 'value': 0.42274},\n  {'tag': 'train/cls_loss', 'value': 0.67743},\n  {'tag': 'train/dfl_loss', 'value': 0.85156},\n  {'tag': 'metrics/precision(B)', 'value': 1.0},\n  {'tag': 'metrics/recall(B)', 'value': 0.68122},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.98591},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.88629},\n  {'tag': 'val/box_loss', 'value': 0.59008},\n  {'tag': 'val/cls_loss', 'value': 1.8945},\n  {'tag': 'val/dfl_loss', 'value': 0.80049},\n  {'tag': 'lr/pg0', 'value': 0.012146},\n  {'tag': 'lr/pg1', 'value': 0.0011463},\n  {'tag': 'lr/pg2', 'value': 0.0011463}],\n [{'tag': 'epoch', 'value': 45.0},\n  {'tag': 'train/box_loss', 'value': 0.44008},\n  {'tag': 'train/cls_loss', 'value': 0.68818},\n  {'tag': 'train/dfl_loss', 'value': 0.79383},\n  {'tag': 'metrics/precision(B)', 'value': 1.0},\n  {'tag': 'metrics/recall(B)', 'value': 0.75958},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.90281},\n  {'tag': 'val/box_loss', 'value': 0.48885},\n  {'tag': 'val/cls_loss', 'value': 1.8046},\n  {'tag': 'val/dfl_loss', 'value': 0.78786},\n  {'tag': 'lr/pg0', 'value': 0.0099919},\n  {'tag': 'lr/pg1', 'value': 0.0009919},\n  {'tag': 'lr/pg2', 'value': 0.0009919}],\n [{'tag': 'epoch', 'value': 46.0},\n  {'tag': 'train/box_loss', 'value': 0.42351},\n  {'tag': 'train/cls_loss', 'value': 0.68141},\n  {'tag': 'train/dfl_loss', 'value': 0.81276},\n  {'tag': 'metrics/precision(B)', 'value': 1.0},\n  {'tag': 'metrics/recall(B)', 'value': 0.84113},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.90002},\n  {'tag': 'val/box_loss', 'value': 0.52886},\n  {'tag': 'val/cls_loss', 'value': 1.6638},\n  {'tag': 'val/dfl_loss', 'value': 0.79098},\n  {'tag': 'lr/pg0', 'value': 0.0078296},\n  {'tag': 'lr/pg1', 'value': 0.00082956},\n  {'tag': 'lr/pg2', 'value': 0.00082956}],\n [{'tag': 'epoch', 'value': 47.0},\n  {'tag': 'train/box_loss', 'value': 0.43221},\n  {'tag': 'train/cls_loss', 'value': 0.68535},\n  {'tag': 'train/dfl_loss', 'value': 0.81431},\n  {'tag': 'metrics/precision(B)', 'value': 0.99289},\n  {'tag': 'metrics/recall(B)', 'value': 0.97043},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.90593},\n  {'tag': 'val/box_loss', 'value': 0.48647},\n  {'tag': 'val/cls_loss', 'value': 1.5789},\n  {'tag': 'val/dfl_loss', 'value': 0.78382},\n  {'tag': 'lr/pg0', 'value': 0.0056593},\n  {'tag': 'lr/pg1', 'value': 0.0006593},\n  {'tag': 'lr/pg2', 'value': 0.0006593}],\n [{'tag': 'epoch', 'value': 48.0},\n  {'tag': 'train/box_loss', 'value': 0.39935},\n  {'tag': 'train/cls_loss', 'value': 0.66413},\n  {'tag': 'train/dfl_loss', 'value': 0.80588},\n  {'tag': 'metrics/precision(B)', 'value': 0.99488},\n  {'tag': 'metrics/recall(B)', 'value': 0.98173},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.8921},\n  {'tag': 'val/box_loss', 'value': 0.50336},\n  {'tag': 'val/cls_loss', 'value': 1.557},\n  {'tag': 'val/dfl_loss', 'value': 0.78832},\n  {'tag': 'lr/pg0', 'value': 0.0034811},\n  {'tag': 'lr/pg1', 'value': 0.00048112},\n  {'tag': 'lr/pg2', 'value': 0.00048112}],\n [{'tag': 'epoch', 'value': 49.0},\n  {'tag': 'train/box_loss', 'value': 0.41614},\n  {'tag': 'train/cls_loss', 'value': 0.62133},\n  {'tag': 'train/dfl_loss', 'value': 0.809},\n  {'tag': 'metrics/precision(B)', 'value': 0.99142},\n  {'tag': 'metrics/recall(B)', 'value': 1.0},\n  {'tag': 'metrics/mAP50(B)', 'value': 0.995},\n  {'tag': 'metrics/mAP50-95(B)', 'value': 0.9074},\n  {'tag': 'val/box_loss', 'value': 0.49075},\n  {'tag': 'val/cls_loss', 'value': 1.4943},\n  {'tag': 'val/dfl_loss', 'value': 0.78682},\n  {'tag': 'lr/pg0', 'value': 0.001295},\n  {'tag': 'lr/pg1', 'value': 0.00029502},\n  {'tag': 'lr/pg2', 'value': 0.00029502}]]</pre> In\u00a0[9]: Copied! <pre>hub.get_evaluate_result()\n</pre> hub.get_evaluate_result() Out[9]: <pre>[{'tag': 'mAP', 'value': 0.3143564462661743}]</pre> In\u00a0[10]: Copied! <pre>hub.best_ckpt_file\n</pre> hub.best_ckpt_file Out[10]: <pre>PosixPath('hubs/detector/weights/best_ckpt.pt')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/hub/train/#train","title":"Train\u00b6","text":"<p>You can simply train a hub by calling <code>train</code> method of <code>Hub</code> class.</p>"},{"location":"tutorials/hub/train/#create-hub-or-load-hub","title":"Create hub or load hub\u00b6","text":""},{"location":"tutorials/hub/train/#load-dataset","title":"Load Dataset\u00b6","text":""},{"location":"tutorials/hub/train/#train","title":"train\u00b6","text":""},{"location":"tutorials/hub/hubs/detector/train/","title":"Train","text":"In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n        from ultralytics import YOLO\n        model = YOLO(\"yolov8n.pt\", task=\"detect\")\n        model.train(\n            data=\"/home/lhj/ws/release/waffle/docs/tutorials/hub/datasets/sample_dataset/exports/YOLO/data.yaml\",\n            epochs=50,\n            batch=64,\n            imgsz=[640, 640],\n            lr0=0.01,\n            lrf=0.01,\n            rect=True,\n            device=\"0\",\n            workers=2,\n            seed=0,\n            verbose=True,\n            project=\"hubs/detector\",\n            name=\"artifacts\",\n        )\n</pre> if __name__ == \"__main__\":         from ultralytics import YOLO         model = YOLO(\"yolov8n.pt\", task=\"detect\")         model.train(             data=\"/home/lhj/ws/release/waffle/docs/tutorials/hub/datasets/sample_dataset/exports/YOLO/data.yaml\",             epochs=50,             batch=64,             imgsz=[640, 640],             lr0=0.01,             lrf=0.01,             rect=True,             device=\"0\",             workers=2,             seed=0,             verbose=True,             project=\"hubs/detector\",             name=\"artifacts\",         )"},{"location":"tutorials/menu/","title":"Index","text":"<p>You can do following things with this Waffle Menu:</p>"},{"location":"waffle_hub/","title":"Waffle Hub","text":"<p><code>Waffle Hub</code> provide two key component classes: <code>Hub</code> and <code>Dataset</code>.</p>"},{"location":"waffle_hub/#hub","title":"Hub","text":"<p><code>Hub</code> provides same interface for various powerfull Deep Learning Frameworks. Here is our brief system architecture.</p> <p></p> <p>Each input and output adapter is responsible for converting our interface to the framework's interface. For example, <code>Ultralytics</code> uses <code>imgsz</code> for image size parameter, but <code>detectron2</code> uses <code>IMAGE_SIZE</code>. So, we need to convert our interface to the framework's interface. <code>waffle_hub</code> provides <code>InputAdapter</code> and <code>OutputAdapter</code> for this purpose.</p>"},{"location":"waffle_hub/#dataset","title":"Dataset","text":"<p><code>Dataset</code> class support many types of data format such as <code>coco</code>, <code>yolo</code>. You can use it to convert dataset or manage dataset.</p>"},{"location":"waffle_hub/dataset/dataset/","title":"Dataset","text":""},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.add_annotations","title":"<code>add_annotations(annotations)</code>","text":"<p>Add \"Annotation\"s to dataset.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>list[Annotation]</code> <p>list of \"Annotation\"s</p> required"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.add_categories","title":"<code>add_categories(categories)</code>","text":"<p>Add \"Category\"s to dataset.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>list[Category]</code> <p>list of \"Category\"s</p> required"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.add_images","title":"<code>add_images(images)</code>","text":"<p>Add \"Image\"s to dataset.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[Image]</code> <p>list of \"Image\"s</p> required"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.add_predictions","title":"<code>add_predictions(predictions)</code>","text":"<p>Add \"Annotation\"s to dataset.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>list[Annotation]</code> <p>list of \"Annotation\"s</p> required"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.check_trainable","title":"<code>check_trainable()</code>","text":"<p>Check if Dataset is trainable or not.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if dataset has not enough annotations.</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.clone","title":"<code>clone(src_name, name, src_root_dir=None, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Clone Existing Dataset. This method clones an existing dataset.</p> <p>Parameters:</p> Name Type Description Default <code>src_name</code> <code>str</code> <p>Dataset name to clone. It should be Waffle Created Dataset.</p> required <code>name</code> <code>str</code> <p>New Dataset name</p> required <code>src_root_dir</code> <code>str</code> <p>Source Dataset root directory. Defaults to None.</p> <code>None</code> <code>root_dir</code> <code>str</code> <p>New Dataset root directory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if source dataset does not exist.</p> <code>FileExistsError</code> <p>if new dataset name already exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = Dataset.clone(\"my_dataset\", \"my_dataset_clone\")\n&gt;&gt;&gt; ds.name\n'my_dataset_clone'  # cloned dataset name\n&gt;&gt;&gt; ds.task\n'CLASSIFICATION'   # original dataset task\n</code></pre> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.delete","title":"<code>delete()</code>","text":"<p>Delete Dataset</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.dummy","title":"<code>dummy(name, task, image_num=100, category_num=10, unlabeled_image_num=0, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Create Dummy Dataset (for debugging).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name</p> required <code>task</code> <code>str</code> <p>Dataset task</p> required <code>image_num</code> <code>int</code> <p>Number of images. Defaults to 100.</p> <code>100</code> <code>category_num</code> <code>int</code> <p>Number of categories. Defaults to 10.</p> <code>10</code> <code>unlabeld_image_num</code> <code>int</code> <p>Number of unlabeled images. Defaults to 0.</p> required <code>root_dir</code> <code>str</code> <p>Dataset root directory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>if dataset name already exists</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = Dataset.dummy(\"my_dataset\", \"CLASSIFICATION\", image_num=100, category_num=10)\n&gt;&gt;&gt; len(ds.get_images())\n100\n&gt;&gt;&gt; len(ds.get_categories())\n10\n</code></pre>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.export","title":"<code>export(data_type)</code>","text":"<p>Export Dataset to Specific data formats</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>Union[str, DataType]</code> <p>export data type. one of [\"YOLO\", \"COCO\"].</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if data_type is not one of DataType.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = Dataset.load(\"some_dataset\")\n&gt;&gt;&gt; dataset.export(data_type=\"YOLO\")\npath/to/dataset_dir/exports/yolo\n</code></pre>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.export--you-can-train-with-exported-dataset","title":"You can train with exported dataset","text":"<pre><code>&gt;&gt;&gt; hub.train(\"path/to/dataset_dir/exports/yolo\", ...)\n</code></pre> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>exported dataset directory</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_autocare_dlt","title":"<code>from_autocare_dlt(name, task, coco_file, coco_root_dir, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Import dataset from autocare dlt format. This method is used for importing dataset from autocare dlt format.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of dataset.</p> required <code>task</code> <code>str</code> <p>task of dataset.</p> required <code>coco_file</code> <code>Union[str, list[str]]</code> <p>coco annotation file path.</p> required <code>coco_root_dir</code> <code>Union[str, list[str]]</code> <p>root directory of coco dataset.</p> required <code>root_dir</code> <code>str</code> <p>root directory of dataset. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>if new dataset name already exist.</p> <p>Examples:</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_autocare_dlt--import-one-coco-json-file","title":"Import one coco json file.","text":"<pre><code>&gt;&gt;&gt; ds = Dataset.from_coco(\"my_dataset\", \"object_detection\", \"path/to/coco.json\", \"path/to/coco_root\")\n&gt;&gt;&gt; ds.get_images()\n{&lt;Image: 1&gt;, &lt;Image: 2&gt;, &lt;Image: 3&gt;, &lt;Image: 4&gt;, &lt;Image: 5&gt;}\n&gt;&gt;&gt; ds.get_annotations()\n{&lt;Annotation: 1&gt;, &lt;Annotation: 2&gt;, &lt;Annotation: 3&gt;, &lt;Annotation: 4&gt;, &lt;Annotation: 5&gt;}\n&gt;&gt;&gt; ds.get_categories()\n{&lt;Category: 1&gt;, &lt;Category: 2&gt;, &lt;Category: 3&gt;, &lt;Category: 4&gt;, &lt;Category: 5&gt;}\n&gt;&gt;&gt; ds.get_category_names()\n['person', 'bicycle', 'car', 'motorcycle', 'airplane']\n</code></pre> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class.</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_coco","title":"<code>from_coco(name, task, coco_file, coco_root_dir, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Import Dataset from coco format. This method imports coco format dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name.</p> required <code>task</code> <code>str</code> <p>Dataset task.</p> required <code>coco_file</code> <code>Union[str, list[str]]</code> <p>Coco json file path. If given list, it will be regarded as [train, val, test] json file.</p> required <code>coco_root_dir</code> <code>Union[str, list[str]]</code> <p>Coco image root directory. If given list, it will be regarded as [train, val, test] coco root file.</p> required <code>root_dir</code> <code>str</code> <p>Dataset root directory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>if new dataset name already exist.</p> <p>Examples:</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_coco--import-one-coco-json-file","title":"Import one coco json file.","text":"<pre><code>&gt;&gt;&gt; ds = Dataset.from_coco(\"my_dataset\", \"object_detection\", \"path/to/coco.json\", \"path/to/coco_root\")\n&gt;&gt;&gt; ds.get_images()\n{&lt;Image: 1&gt;, &lt;Image: 2&gt;, &lt;Image: 3&gt;, &lt;Image: 4&gt;, &lt;Image: 5&gt;}\n&gt;&gt;&gt; ds.get_annotations()\n{&lt;Annotation: 1&gt;, &lt;Annotation: 2&gt;, &lt;Annotation: 3&gt;, &lt;Annotation: 4&gt;, &lt;Annotation: 5&gt;}\n&gt;&gt;&gt; ds.get_categories()\n{&lt;Category: 1&gt;, &lt;Category: 2&gt;, &lt;Category: 3&gt;, &lt;Category: 4&gt;, &lt;Category: 5&gt;}\n&gt;&gt;&gt; ds.get_category_names()\n['person', 'bicycle', 'car', 'motorcycle', 'airplane']\n</code></pre>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_coco--import-multiple-coco-json-files","title":"Import multiple coco json files.","text":""},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_coco--you-can-give-coco_file-as-list","title":"You can give coco_file as list.","text":""},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_coco--given-coco-files-are-regarded-as-train-val-test-json-files","title":"Given coco files are regarded as [train, [val, [test]]] json files.","text":"<pre><code>&gt;&gt;&gt; ds = Dataset.from_coco(\"my_dataset\", \"object_detection\", [\"coco_train.json\", \"coco_val.json\"], [\"coco_train_root\", \"coco_val_root\"])\n</code></pre> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_transformers","title":"<code>from_transformers(name, task, dataset_dir, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Import Dataset from transformers datasets. This method imports transformers dataset from directory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name.</p> required <code>dataset_dir</code> <code>str</code> <p>Transformers dataset directory.</p> required <code>task</code> <code>str</code> <p>Task name.</p> required <code>root_dir</code> <code>str</code> <p>Dataset root directory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>if dataset name already exists</p> <code>ValueError</code> <p>if dataset is not Dataset or DatasetDict</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = Dataset.from_transformers(\"transformers\", \"object_detection\", \"path/to/transformers/dataset\")\n</code></pre> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.from_yolo","title":"<code>from_yolo(name, task, yaml_path, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Import Dataset from yolo format. This method imports dataset from yolo(ultralytics) yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name.</p> required <code>task</code> <code>str</code> <p>Dataset task.</p> required <code>yaml_path</code> <code>str</code> <p>Yolo yaml file path.</p> required <code>root_dir</code> <code>str</code> <p>Dataset root directory. Defaults to None.</p> <code>None</code> Example <p>ds = Dataset.from_yolo(\"yolo\", \"classification\", \"path/to/yolo.yaml\")</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Imported dataset.</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.get_annotations","title":"<code>get_annotations(image_id=None)</code>","text":"<p>Get \"Annotation\"s.</p> <p>Parameters:</p> Name Type Description Default <code>image_id</code> <code>int</code> <p>image id. None for all \"Annotation\"s. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Annotation]</code> <p>list[Annotation]: \"Annotation\" list</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.get_categories","title":"<code>get_categories(category_ids=None)</code>","text":"<p>Get \"Category\"s.</p> <p>Parameters:</p> Name Type Description Default <code>category_ids</code> <code>list[int]</code> <p>id list. None for all \"Category\"s. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Category]</code> <p>list[Category]: \"Category\" list</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.get_dataset_info","title":"<code>get_dataset_info()</code>","text":"<p>Get DatasetInfo.</p> <p>Returns:</p> Name Type Description <code>DatasetInfo</code> <code>DatasetInfo</code> <p>DatasetInfo</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.get_dataset_list","title":"<code>get_dataset_list(root_dir=None)</code>  <code>classmethod</code>","text":"<p>Get dataset name list in root_dir.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>dataset root directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: dataset name list.</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.get_images","title":"<code>get_images(image_ids=None, labeled=True)</code>","text":"<p>Get \"Image\"s.</p> <p>Parameters:</p> Name Type Description Default <code>image_ids</code> <code>list[int]</code> <p>id list. None for all \"Image\"s. Defaults to None.</p> <code>None</code> <code>labeled</code> <code>bool</code> <p>get labeled images. False for unlabeled images. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Image]</code> <p>list[Image]: \"Image\" list</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.get_predictions","title":"<code>get_predictions(image_id=None)</code>","text":"<p>Get \"Prediction\"s.</p> <p>Parameters:</p> Name Type Description Default <code>image_id</code> <code>int</code> <p>image id. None for all \"Prediction\"s. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Annotation]</code> <p>list[Annotation]: \"Prediction\" list</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.get_split_ids","title":"<code>get_split_ids()</code>","text":"<p>Get split ids</p> <p>Returns:</p> Type Description <code>list[list[int]]</code> <p>list[list[int]]: split ids</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.initialize","title":"<code>initialize()</code>","text":"<p>Initialize Dataset. It creates necessary directories under {dataset_root_dir}/{dataset_name}.</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.initialized","title":"<code>initialized()</code>","text":"<p>Check if Dataset has been initialized or not.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>initialized -&gt; True not initialized -&gt; False</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.load","title":"<code>load(name, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Load Dataset. This method loads an existing dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name that Waffle Created</p> required <code>root_dir</code> <code>str</code> <p>Dataset root directory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if source dataset does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = Dataset.load(\"my_dataset\")\n&gt;&gt;&gt; ds.name\n'my_dataset'  # dataset name\n</code></pre> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.merge","title":"<code>merge(name, root_dir, src_names, src_root_dirs, task)</code>  <code>classmethod</code>","text":"<p>Merge Datasets. This method merges multiple datasets into one dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>New Dataset name</p> required <code>root_dir</code> <code>str</code> <p>New Dataset root directory</p> required <code>src_names</code> <code>list[str]</code> <p>Source Dataset names</p> required <code>src_root_dirs</code> <code>Union[str, list[str]]</code> <p>Source Dataset root directories</p> required <code>task</code> <code>str</code> <p>Dataset task</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.new","title":"<code>new(name, task, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Create New Dataset. This method creates a new dataset directory and initialize dataset info file. If you have other types of data, you can use from_* methods to create a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name</p> required <code>task</code> <code>str</code> <p>Dataset task</p> required <code>root_dir</code> <code>str</code> <p>Dataset root directory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>if dataset name already exists</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = Dataset.new(\"my_dataset\", \"CLASSIFICATION\")\n&gt;&gt;&gt; ds.name\n'my_dataset'  # dataset name\n&gt;&gt;&gt; ds.task  # dataset task\n'CLASSIFICATION'\n</code></pre> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.sample","title":"<code>sample(name, task, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Import sample Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name.</p> required <code>task</code> <code>str</code> <p>Task name.</p> required <code>root_dir</code> <code>str</code> <p>Dataset root directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset Class</p>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.split","title":"<code>split(train_ratio, val_ratio=0.0, test_ratio=0.0, method=SplitMethod.RANDOM, seed=0)</code>","text":"<p>Split Dataset to train, validation, test, (unlabeled) sets.</p> <p>Parameters:</p> Name Type Description Default <code>train_ratio</code> <code>float</code> <p>train num ratio (0 ~ 1).</p> required <code>val_ratio</code> <code>float</code> <p>val num ratio (0 ~ 1).</p> <code>0.0</code> <code>test_ratio</code> <code>float</code> <p>test num ratio (0 ~ 1).</p> <code>0.0</code> <code>method</code> <code>Union[str, SplitMethod]</code> <p>split method. Defaults to SplitMethod.RANDOM.</p> <code>SplitMethod.RANDOM</code> <code>seed</code> <code>int</code> <p>random seed. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if train_ratio is not between 0.0 and 1.0.</p> <code>ValueError</code> <p>if train_ratio + val_ratio + test_ratio is not 1.0.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = Dataset.load(\"some_dataset\")\n&gt;&gt;&gt; dataset.split(train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)\n&gt;&gt;&gt; dataset.get_split_ids()\n[[1, 2, 3, 4, 5, 6, 7, 8], [9], [10], []]  # train, val, test, unlabeled image ids\n</code></pre>"},{"location":"waffle_hub/dataset/dataset/#waffle_hub.dataset.dataset.Dataset.trainable","title":"<code>trainable()</code>","text":"<p>Check if Dataset is trainable or not.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>trainable -&gt; True not trainable -&gt; False</p>"},{"location":"waffle_hub/dataset/field/","title":"Field","text":""},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.Annotation","title":"<code>Annotation</code>","text":"<p>         Bases: <code>BaseField</code></p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.classification","title":"<code>classification(annotation_id=None, image_id=None, category_id=None, score=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Classification Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> <code>None</code> <code>score</code> <code>float</code> <p>prediction score. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.instance_segmentation","title":"<code>instance_segmentation(annotation_id=None, image_id=None, category_id=None, bbox=None, segmentation=None, area=None, iscrowd=0, score=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Instance Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> <code>None</code> <code>bbox</code> <code>list[float]</code> <p>[x1, y1, w, h].</p> <code>None</code> <code>segmentation</code> <code>Union[list[list[float]], dict]</code> <p>[[x1, y1, x2, y2, x3, y3, ...], [polygon]] or RLE.</p> <code>None</code> <code>area</code> <code>int</code> <p>segmentation segmentation area.</p> <code>None</code> <code>iscrowd</code> <code>int</code> <p>is crowd or not. Default to 0.</p> <code>0</code> <code>score</code> <code>float</code> <p>prediction score. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.keypoint_detection","title":"<code>keypoint_detection(annotation_id=None, image_id=None, category_id=None, bbox=None, keypoints=None, num_keypoints=None, area=None, segmentation=None, iscrowd=0, score=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Keypoint Detection Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> <code>None</code> <code>bbox</code> <code>list[float]</code> <p>[x1, y1, w, h].</p> <code>None</code> <code>keypoints</code> <code>list[float]</code> <p>[x1, y1, v1(visible flag), x2, y2, v2(visible flag), ...]. visible flag is one of [0(Not labeled), 1(Labeled but not visible), 2(labeled and visible)]</p> <code>None</code> <code>num_keypoints</code> <code>int</code> <p>number of labeled keypoints</p> <code>None</code> <code>area</code> <code>int</code> <p>segmentation segmentation or bbox area.</p> <code>None</code> <code>segmentation</code> <code>list[list[float]]</code> <p>[[x1, y1, x2, y2, x3, y3, ...], [polygon]].</p> <code>None</code> <code>iscrowd</code> <code>int</code> <p>is crowd or not. Default to 0.</p> <code>0</code> <code>score</code> <code>list[float]</code> <p>prediction scores. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.new","title":"<code>new(annotation_id=None, image_id=None, category_id=None, bbox=None, segmentation=None, area=None, keypoints=None, num_keypoints=None, caption=None, value=None, iscrowd=None, score=None, task=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> <code>None</code> <code>bbox</code> <code>list[float]</code> <p>[x1, y1, w, h].</p> <code>None</code> <code>segmentation</code> <code>list[list[float]]</code> <p>[[[x1, y1, x2, y2, x3, y3, ...], [...]].</p> <code>None</code> <code>area</code> <code>int</code> <p>bbox area.</p> <code>None</code> <code>keypoints</code> <code>list[float]</code> <p>[x1, y1, v1(visible flag), x2, y2, v2(visible flag), ...]. visible flag is one of [0(Not labeled), 1(Labeled but not visible), 2(labeled and visible)]</p> <code>None</code> <code>num_keypoints</code> <code>int</code> <p>number of labeled keypoints</p> <code>None</code> <code>caption</code> <code>str</code> <p>string.</p> <code>None</code> <code>value</code> <code>float</code> <p>regression value.</p> <code>None</code> <code>iscrowd</code> <code>int</code> <p>is crowd or not. Default to None.</p> <code>None</code> <code>score</code> <code>float</code> <p>prediction score. Default to None.</p> <code>None</code> <code>task</code> <code>Union[str, TaskType]</code> <p>task type. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.object_detection","title":"<code>object_detection(annotation_id=None, image_id=None, category_id=None, bbox=None, area=None, iscrowd=0, score=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Object Detection Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> <code>None</code> <code>bbox</code> <code>list[float]</code> <p>[x1, y1, w, h].</p> <code>None</code> <code>area</code> <code>int</code> <p>bbox area.</p> <code>None</code> <code>iscrowd</code> <code>int</code> <p>is crowd or not. Default to 0.</p> <code>0</code> <code>score</code> <code>float</code> <p>prediction score. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.regression","title":"<code>regression(annotation_id=None, image_id=None, value=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Regression Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>value</code> <code>float</code> <p>regression value.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.semantic_segmentation","title":"<code>semantic_segmentation(annotation_id=None, image_id=None, category_id=None, bbox=None, segmentation=None, area=None, iscrowd=0, score=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Segmentation Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> <code>None</code> <code>bbox</code> <code>list[float]</code> <p>[x1, y1, w, h].</p> <code>None</code> <code>segmentation</code> <code>Union[list[list[float]], dict]</code> <p>[[x1, y1, x2, y2, x3, y3, ...], [polygon]] or RLE.</p> <code>None</code> <code>area</code> <code>int</code> <p>segmentation segmentation area.</p> <code>None</code> <code>iscrowd</code> <code>int</code> <p>is crowd or not. Default to 0.</p> <code>0</code> <code>score</code> <code>float</code> <p>prediction score. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.text_recognition","title":"<code>text_recognition(annotation_id=None, image_id=None, caption=None, score=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Text Recognition Annotation Format</p> <p>Parameters:</p> Name Type Description Default <code>annotation_id</code> <code>int</code> <p>annotaion id. natural number.</p> <code>None</code> <code>image_id</code> <code>int</code> <p>image id. natural number.</p> <code>None</code> <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>caption</code> <code>str</code> <p>string.</p> <code>None</code> <code>score</code> <code>float</code> <p>prediction score. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Annotation</code> <code>Annotation</code> <p>annotation class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.annotation.Annotation.to_dict","title":"<code>to_dict()</code>","text":"<p>Get Dictionary of Annotation Data</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>annotation dictionary.</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.Category","title":"<code>Category</code>","text":"<p>         Bases: <code>BaseField</code></p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.classification","title":"<code>classification(category_id, name, supercategory=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Classification Category Format</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>name</code> <code>str</code> <p>category name.</p> required <code>supercategory</code> <code>str</code> <p>supercategory name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Category</code> <code>Category</code> <p>category class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.instance_segmentation","title":"<code>instance_segmentation(category_id, name, supercategory=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Instance Category Format</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>name</code> <code>str</code> <p>category name.</p> required <code>supercategory</code> <code>str</code> <p>supercategory name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Category</code> <code>Category</code> <p>category class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.keypoint_detection","title":"<code>keypoint_detection(category_id, name, keypoints, skeleton, supercategory=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Keypoint Detection Category Format</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>name</code> <code>str</code> <p>category name.</p> required <code>keypoints</code> <code>list[str]</code> <p>category name.</p> required <code>skeleton</code> <code>list[list[int]]</code> <p>skeleton edges.</p> required <code>supercategory</code> <code>str</code> <p>supercategory name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Category</code> <code>Category</code> <p>category class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.new","title":"<code>new(category_id, name, supercategory=None, keypoints=None, skeleton=None, task=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Category Format</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>name</code> <code>str</code> <p>category name.</p> required <code>supercategory</code> <code>str</code> <p>supercategory name.</p> <code>None</code> <code>keypoints</code> <code>list[str]</code> <p>category name.</p> <code>None</code> <code>skeleton</code> <code>list[list[int]]</code> <p>skeleton edges.</p> <code>None</code> <code>task</code> <code>Union[str, TaskType]</code> <p>task type. Default to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Category</code> <code>Category</code> <p>category class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.object_detection","title":"<code>object_detection(category_id, name, supercategory=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Object Detection Category Format</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>name</code> <code>str</code> <p>category name.</p> required <code>supercategory</code> <code>str</code> <p>supercategory name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Category</code> <code>Category</code> <p>category class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.semantic_segmentation","title":"<code>semantic_segmentation(category_id, name, supercategory=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Segmentation Category Format</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>name</code> <code>str</code> <p>category name.</p> required <code>supercategory</code> <code>str</code> <p>supercategory name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Category</code> <code>Category</code> <p>category class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.text_recognition","title":"<code>text_recognition(category_id, name, supercategory=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Text Recognition Category Format</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>category id. natural number.</p> required <code>name</code> <code>str</code> <p>category name.</p> required <code>supercategory</code> <code>str</code> <p>supercategory name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Category</code> <code>Category</code> <p>category class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.category.Category.to_dict","title":"<code>to_dict()</code>","text":"<p>Get Dictionary of Category</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>annotation dictionary.</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.Image","title":"<code>Image</code>","text":"<p>         Bases: <code>BaseField</code></p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.image.Image.new","title":"<code>new(image_id, file_name, width, height, date_captured=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Image Format</p> <p>Parameters:</p> Name Type Description Default <code>image_id</code> <code>int</code> <p>image id. natural number.</p> required <code>file_name</code> <code>str</code> <p>file name. relative file path.</p> required <code>width</code> <code>int</code> <p>image width.</p> required <code>height</code> <code>int</code> <p>image height.</p> required <code>date_captured</code> <code>str</code> <p>date_captured string. \"%Y-%m-%d %H:%M:%S\"</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>image class</p>"},{"location":"waffle_hub/dataset/field/#waffle_hub.schema.fields.image.Image.to_dict","title":"<code>to_dict()</code>","text":"<p>Get Dictionary of Category</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>annotation dictionary.</p>"},{"location":"waffle_hub/hub/hub/","title":"Hub","text":"<p>         Bases: <code>Hub</code></p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.adapter.ultralytics.ultralytics_hub.UltralyticsHub.new","title":"<code>new(name, task=None, model_type=None, model_size=None, categories=None, root_dir=None, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create Ultralytics Hub.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Hub name</p> required <code>task</code> <code>str</code> <p>Task Name. See UltralyticsHub.TASKS. Defaults to None.</p> <code>None</code> <code>model_type</code> <code>str</code> <p>Model Type. See UltralyticsHub.MODEL_TYPES. Defaults to None.</p> <code>None</code> <code>model_size</code> <code>str</code> <p>Model Size. See UltralyticsHub.MODEL_SIZES. Defaults to None.</p> <code>None</code> <code>categories</code> <code>Union[list[dict], list]</code> <p>class dictionary or list. [{\"supercategory\": \"name\"}, ] or [\"name\",].</p> <code>None</code> <code>root_dir</code> <code>str</code> <p>Root directory of hub repository. Defaults to None.</p> <code>None</code>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.artifact_dir","title":"<code>artifact_dir: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Artifact Directory. This is raw output of each backend.</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.backend","title":"<code>backend: str</code>  <code>property</code> <code>writable</code>","text":"<p>Backend name</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.best_ckpt_file","title":"<code>best_ckpt_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Best Checkpoint File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.draw_dir","title":"<code>draw_dir: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Draw Results Directory</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.evaluate_file","title":"<code>evaluate_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Evaluate Json File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.hub_dir","title":"<code>hub_dir: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Hub(Model) Directory</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.inference_dir","title":"<code>inference_dir: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Inference Results Directory</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.inference_file","title":"<code>inference_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Inference Results File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.last_ckpt_file","title":"<code>last_ckpt_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Last Checkpoint File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.metric_file","title":"<code>metric_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Metric Csv File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.model_config_file","title":"<code>model_config_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Model Config yaml File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.model_size","title":"<code>model_size: str</code>  <code>property</code> <code>writable</code>","text":"<p>Model Size</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.model_type","title":"<code>model_type: str</code>  <code>property</code> <code>writable</code>","text":"<p>Model Type</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.name","title":"<code>name: str</code>  <code>property</code> <code>writable</code>","text":"<p>Hub name</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.onnx_file","title":"<code>onnx_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Best Checkpoint File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.root_dir","title":"<code>root_dir: Path</code>  <code>property</code> <code>writable</code>","text":"<p>Root Directory</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.task","title":"<code>task: str</code>  <code>property</code> <code>writable</code>","text":"<p>Task Name</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.train_config_file","title":"<code>train_config_file: Path</code>  <code>property</code> <code>cached</code>","text":"<p>Train Config yaml File</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.version","title":"<code>version: str</code>  <code>property</code> <code>writable</code>","text":"<p>Version</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.benchmark","title":"<code>benchmark(image_size=None, batch_size=16, device='0', half=False, trial=100)</code>","text":"<p>Benchmark Model</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>Union[int, list[int]]</code> <p>inference image size. None for same with train_config (recommended).</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>dynamic batch size. Defaults to 16.</p> <code>16</code> <code>device</code> <code>str</code> <p>device. \"cpu\" or \"gpu_id\". Defaults to \"0\".</p> <code>'0'</code> <code>half</code> <code>bool</code> <p>half. Defaults to False.</p> <code>False</code> <code>trial</code> <code>int</code> <p>number of trials. Defaults to 100.</p> <code>100</code> Example <p>hub.benchmark(         image_size=640,         batch_size=16,         device=\"0\",         half=False,         trial=100,     ) {     \"inference_time\": 0.123,     \"fps\": 123.123,     \"image_size\": [640, 640],     \"batch_size\": 16,     \"device\": \"0\",     \"cpu_name\": \"Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\",     \"gpu_name\": \"GeForce GTX 1080 Ti\", }</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>benchmark result</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.check_train_sanity","title":"<code>check_train_sanity()</code>","text":"<p>Check if all essential files are exist.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all files are exist else False</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.delete_artifact","title":"<code>delete_artifact()</code>","text":"<p>Delete Artifact Directory. It can be trained again.</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.delete_hub","title":"<code>delete_hub()</code>","text":"<p>Delete all artifacts of Hub. Hub name can be used again.</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.evaluate","title":"<code>evaluate(dataset, dataset_root_dir=None, set_name='test', batch_size=4, image_size=None, letter_box=None, confidence_threshold=0.25, iou_threshold=0.5, half=False, workers=2, device='0', draw=False, hold=True)</code>","text":"<p>Start Evaluate</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, str]</code> <p>Waffle Dataset object or path or name.</p> required <code>dataset_root_dir</code> <code>str</code> <p>Waffle Dataset root directory. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>batch size. Defaults to 4.</p> <code>4</code> <code>image_size</code> <code>Union[int, list[int]]</code> <p>image size. Defaults to None.</p> <code>None</code> <code>letter_box</code> <code>bool</code> <p>letter box. Defaults to None.</p> <code>None</code> <code>confidence_threshold</code> <code>float</code> <p>confidence threshold. Defaults to 0.25.</p> <code>0.25</code> <code>iou_threshold</code> <code>float</code> <p>iou threshold. Defaults to 0.5.</p> <code>0.5</code> <code>half</code> <code>bool</code> <p>half. Defaults to False.</p> <code>False</code> <code>workers</code> <code>int</code> <p>workers. Defaults to 2.</p> <code>2</code> <code>device</code> <code>str</code> <p>device. Defaults to \"0\".</p> <code>'0'</code> <code>draw</code> <code>bool</code> <p>draw. Defaults to False.</p> <code>False</code> <code>hold</code> <code>bool</code> <p>hold. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if can not detect appropriate dataset.</p> <code>e</code> <p>something gone wrong with ultralytics</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; evaluate_result = hub.evaluate(\n        dataset=detection_dataset,\n        batch_size=4,\n        image_size=640,\n        letterbox=False,\n        confidence_threshold=0.25,\n        iou_threshold=0.5,\n        workers=4,\n        device=\"0\",\n    )\n# or you can use train option by passing None\n&gt;&gt;&gt; evaluate_result = hub.evaluate(\n        ...\n        image_size=None,  # use train option\n        letterbox=None,  # use train option\n        ...\n    )\n&gt;&gt;&gt; evaluate_result.metrics\n[{\"tag\": \"mAP\", \"value\": 0.1}, ...]\n</code></pre> <p>Returns:</p> Name Type Description <code>EvaluateResult</code> <code>EvaluateResult</code> <p>evaluate result</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.export","title":"<code>export(image_size=None, batch_size=16, opset_version=11, half=False, device='0', hold=True)</code>","text":"<p>Export Model</p> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>Union[int, list[int]]</code> <p>inference image size. None for same with train_config (recommended).</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>dynamic batch size. Defaults to 16.</p> <code>16</code> <code>opset_version</code> <code>int</code> <p>onnx opset version. Defaults to 11.</p> <code>11</code> <code>half</code> <code>bool</code> <p>half. Defaults to False.</p> <code>False</code> <code>device</code> <code>str</code> <p>device. \"cpu\" or \"gpu_id\". Defaults to \"0\".</p> <code>'0'</code> <code>hold</code> <code>bool</code> <p>hold or not. If True then it holds until task finished. If False then return Inferece Callback and run in background. Defaults to True.</p> <code>True</code> Example <p>export_result = hub.export(     image_size=640,     batch_size=16,     opset_version=11, )</p> <p>Returns:</p> Name Type Description <code>ExportResult</code> <code>ExportResult</code> <p>export result</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.export--or-simply-use-train-option-by-passing-none","title":"or simply use train option by passing None","text":"<p>export_result = hub.export(     ...,     image_size=None,  # use train option     ... ) export_result.export_file hubs/my_hub/weights/model.onnx</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.from_model_config","title":"<code>from_model_config(name, model_config_file, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Create new Hub with model config.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>hub name.</p> required <code>model_config_file</code> <code>str</code> <p>model config yaml file.</p> required <code>root_dir</code> <code>str</code> <p>hub root directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Hub</code> <code>Hub</code> <p>New Hub instance</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_available_backends","title":"<code>get_available_backends()</code>  <code>classmethod</code>","text":"<p>Get available backends</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Available backends</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_available_model_sizes","title":"<code>get_available_model_sizes(backend=None, task=None, model_type=None)</code>  <code>classmethod</code>","text":"<p>Get available model sizes</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend name</p> <code>None</code> <code>task</code> <code>str</code> <p>Task name</p> <code>None</code> <code>model_type</code> <code>str</code> <p>Model type</p> <code>None</code> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If backend is not supported</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Available model sizes</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_available_model_types","title":"<code>get_available_model_types(backend=None, task=None)</code>  <code>classmethod</code>","text":"<p>Get available model types</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend name</p> <code>None</code> <code>task</code> <code>str</code> <p>Task name</p> <code>None</code> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If backend is not supported</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Available model types</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_available_tasks","title":"<code>get_available_tasks(backend=None)</code>  <code>classmethod</code>","text":"<p>Get available tasks</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend name</p> <code>None</code> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If backend is not supported</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Available tasks</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_default_train_params","title":"<code>get_default_train_params(backend=None, task=None, model_type=None, model_size=None)</code>  <code>classmethod</code>","text":"<p>Get default train params</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend name</p> <code>None</code> <code>task</code> <code>str</code> <p>Task name</p> <code>None</code> <code>model_type</code> <code>str</code> <p>Model type</p> <code>None</code> <code>model_size</code> <code>str</code> <p>Model size</p> <code>None</code> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If backend is not supported</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Default train params</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_evaluate_result","title":"<code>get_evaluate_result()</code>","text":"<p>Get evaluate result from evaluate file.</p> Example <p>hub.get_evaluate_result() [     {         \"tag\": \"mAP\",         \"value\": 0.5,     }, ]</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>evaluate result</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_hub_class","title":"<code>get_hub_class(backend=None)</code>  <code>classmethod</code>","text":"<p>Get hub class</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend name</p> <code>None</code> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If backend is not supported</p> <p>Returns:</p> Name Type Description <code>Hub</code> <code>Hub</code> <p>Backend hub Class</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_hub_list","title":"<code>get_hub_list(root_dir=None)</code>  <code>classmethod</code>","text":"<p>Get hub name list in root_dir.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>hub root directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: hub name list</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_image_loader","title":"<code>get_image_loader()</code>","text":"<p>Get image loader function.</p> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, ImageInfo]</code> <p>tuple[torch.Tensor, ImageInfo]: input transform function</p> Example <p>transform = hub.get_image_loader() image, image_info = transform(\"path/to/image.jpg\") model = hub.get_model() output = model(image.unsqueeze(0))</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_inference_result","title":"<code>get_inference_result()</code>","text":"<p>Get inference result from inference file.</p> Example <p>hub.get_inference_result() [     {         \"id\": \"00000001\",         \"category\": \"person\",         \"bbox\": [0.1, 0.2, 0.3, 0.4],         \"score\": 0.9,     }, ]</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: inference result</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get metrics per epoch from metric file.</p> Example <p>hub.get_metrics() [     [         {             \"tag\": \"epoch\",             \"value\": \"1\",         },         {             \"tag\": \"train_loss\",             \"value\": \"0.0012\",         }     ], ]</p> <p>Returns:</p> Type Description <code>list[list[dict]]</code> <p>list[dict]: metrics per epoch</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_model_config","title":"<code>get_model_config()</code>","text":"<p>Get model config from model config file.</p> <p>Returns:</p> Name Type Description <code>ModelConfig</code> <code>ModelConfig</code> <p>model config</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.get_train_config","title":"<code>get_train_config()</code>","text":"<p>Get train config from train config file.</p> <p>Returns:</p> Name Type Description <code>TrainConfig</code> <code>TrainConfig</code> <p>train config</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.inference","title":"<code>inference(source, recursive=True, image_size=None, letter_box=None, batch_size=4, confidence_threshold=0.25, iou_threshold=0.5, half=False, workers=2, device='0', draw=False, hold=True)</code>","text":"<p>Start Inference</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>source directory</p> required <code>recursive</code> <code>bool</code> <p>recursive. Defaults to True.</p> <code>True</code> <code>image_size</code> <code>Union[int, list[int]]</code> <p>image size. None for using training config. Defaults to None.</p> <code>None</code> <code>letter_box</code> <code>bool</code> <p>letter box. None for using training config. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>batch size. Defaults to 4.</p> <code>4</code> <code>confidence_threshold</code> <code>float</code> <p>confidence threshold. Defaults to 0.25.</p> <code>0.25</code> <code>iou_threshold</code> <code>float</code> <p>iou threshold. Defaults to 0.5.</p> <code>0.5</code> <code>half</code> <code>bool</code> <p>half. Defaults to False.</p> <code>False</code> <code>workers</code> <code>int</code> <p>workers. Defaults to 2.</p> <code>2</code> <code>device</code> <code>str</code> <p>device. \"cpu\" or \"gpu_id\". Defaults to \"0\".</p> <code>'0'</code> <code>draw</code> <code>bool</code> <p>draw. Defaults to False.</p> <code>False</code> <code>hold</code> <code>bool</code> <p>hold. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if can not detect appropriate dataset.</p> <code>e</code> <p>something gone wrong with ultralytics</p> Example <p>inference_result = hub.inference(         source=\"path/to/images\",         batch_size=4,         image_size=640,         letterbox=False,         confidence_threshold=0.25,         iou_threshold=0.5,         workers=4,         device=\"0\",         draw=True,     )</p> <p>Returns:</p> Name Type Description <code>InferenceResult</code> <code>InferenceResult</code> <p>inference result</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.inference--or-simply-use-train-option-by-passing-none","title":"or simply use train option by passing None","text":"<p>inference_result = hub.inference(         ...         image_size=None,  # use train option         letterbox=None,  # use train option         ...     ) inference_result.predictions [{\"relative/path/to/image/file\": [{\"category\": \"1\", \"bbox\": [0, 0, 100, 100], \"score\": 0.9}, ...]}, ...]</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.load","title":"<code>load(name, root_dir=None)</code>  <code>classmethod</code>","text":"<p>Load Hub by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>hub name.</p> required <code>root_dir</code> <code>str</code> <p>hub root directory. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if hub is not exist in root_dir</p> <p>Returns:</p> Name Type Description <code>Hub</code> <code>Hub</code> <p>Hub instance</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.new","title":"<code>new(name, backend=None, task=None, model_type=None, model_size=None, categories=None, root_dir=None, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create Hub.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Hub name</p> required <code>backend</code> <code>str</code> <p>Backend name. See Hub.BACKENDS. Defaults to None.</p> <code>None</code> <code>task</code> <code>str</code> <p>Task Name. See Hub.TASKS. Defaults to None.</p> <code>None</code> <code>model_type</code> <code>str</code> <p>Model Type. See Hub.MODEL_TYPES. Defaults to None.</p> <code>None</code> <code>model_size</code> <code>str</code> <p>Model Size. See Hub.MODEL_SIZES. Defaults to None.</p> <code>None</code> <code>categories</code> <code>Union[list[dict], list]</code> <p>class dictionary or list. [{\"supercategory\": \"name\"}, ] or [\"name\",].</p> <code>None</code> <code>root_dir</code> <code>str</code> <p>Root directory of hub repository. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Hub</code> <code>Hub</code> <p>Hub instance</p>"},{"location":"waffle_hub/hub/hub/#waffle_hub.hub.hub.Hub.train","title":"<code>train(dataset, dataset_root_dir=None, epochs=None, batch_size=None, image_size=None, learning_rate=None, letter_box=None, pretrained_model=None, device='0', workers=2, seed=0, verbose=True, hold=True)</code>","text":"<p>Start Train</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, str]</code> <p>Waffle Dataset object or path or name.</p> required <code>dataset_root_dir</code> <code>str</code> <p>Waffle Dataset root directory. Defaults to None.</p> <code>None</code> <code>epochs</code> <code>int</code> <p>number of epochs. None to use default. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>batch size. None to use default. Defaults to None.</p> <code>None</code> <code>image_size</code> <code>Union[int, list[int]]</code> <p>image size. None to use default. Defaults to None.</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>learning rate. None to use default. Defaults to None.</p> <code>None</code> <code>letter_box</code> <code>bool</code> <p>letter box. None to use default. Defaults to None.</p> <code>None</code> <code>pretrained_model</code> <code>str</code> <p>pretrained model. None to use default. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>\"cpu\" or \"gpu_id\" or comma seperated \"gpu_ids\". Defaults to \"0\".</p> <code>'0'</code> <code>workers</code> <code>int</code> <p>number of workers. Defaults to 2.</p> <code>2</code> <code>seed</code> <code>int</code> <p>random seed. Defaults to 0.</p> <code>0</code> <code>verbose</code> <code>bool</code> <p>verbose. Defaults to True.</p> <code>True</code> <code>hold</code> <code>bool</code> <p>hold process. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>if trained artifact exists.</p> <code>FileNotFoundError</code> <p>if can not detect appropriate dataset.</p> <code>ValueError</code> <p>if can not detect appropriate dataset.</p> <code>e</code> <p>something gone wrong with ultralytics</p> Example <p>train_result = hub.train(         dataset=dataset,         epochs=100,         batch_size=16,         image_size=640,         learning_rate=0.001,         letterbox=False,         device=\"0\",  # use gpu 0         # device=\"0,1,2,3\",  # use gpu 0,1,2,3         # device=\"cpu\",  # use cpu         workers=2,         seed=123     ) train_result.best_ckpt_file hubs/my_hub/weights/best_ckpt.pt train_result.metrics [[{\"tag\": \"epoch\", \"value\": 1}, {\"tag\": \"train/loss\", \"value\": 0.1}, ...], ...]</p> <p>Returns:</p> Name Type Description <code>TrainResult</code> <code>TrainResult</code> <p>train result</p>"},{"location":"waffle_hub/hub/result/","title":"Result","text":""},{"location":"waffle_hub/hub/result/#trainresult","title":"TrainResult","text":"<p>         Bases: <code>BaseSchema</code></p>"},{"location":"waffle_hub/hub/result/#waffle_hub.schema.result.TrainResult.best_ckpt_file","title":"<code>best_ckpt_file: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"waffle_hub/hub/result/#waffle_hub.schema.result.TrainResult.last_ckpt_file","title":"<code>last_ckpt_file: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"waffle_hub/hub/result/#waffle_hub.schema.result.TrainResult.metrics","title":"<code>metrics: list[list[dict]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"waffle_hub/hub/result/#evaluateresult","title":"EvaluateResult","text":"<p>         Bases: <code>BaseSchema</code></p>"},{"location":"waffle_hub/hub/result/#inferenceresult","title":"InferenceResult","text":"<p>         Bases: <code>BaseSchema</code></p>"},{"location":"waffle_hub/hub/result/#waffle_hub.schema.result.InferenceResult.predictions","title":"<code>predictions: list[dict[list]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"waffle_hub/hub/result/#waffle_hub.schema.result.InferenceResult.draw_dir","title":"<code>draw_dir: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"waffle_hub/hub/result/#exportresult","title":"ExportResult","text":"<p>         Bases: <code>BaseSchema</code></p>"},{"location":"waffle_hub/hub/result/#waffle_hub.schema.result.ExportResult.export_file","title":"<code>export_file: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"waffle_menu/","title":"Waffle Menu (private)","text":"<p><code>Waffle Menu</code> provide advanced deeplearning applications. <code>Waffle Menu</code> is based on <code>Waffle Hub</code>.</p>"},{"location":"waffle_menu/#active-learning","title":"Active Learning","text":""},{"location":"waffle_menu/active_learning/","title":"Active Learning Base Class","text":"<p>all active learning methods are inherited from <code>ActiveLearning</code> class. Only initialize methods are different. It means that you can use any active learning method by calling sample method of <code>ActiveLearning</code> class.</p>"},{"location":"waffle_menu/active_learning/#waffle_menu.active_learning.active_learning.ActiveLearning.sample","title":"<code>sample(image_dir, num_images, result_dir, save_images=True, hold=True)</code>","text":"<p>Sample images from the image directory</p> <p>Parameters:</p> Name Type Description Default <code>sampled_image_dir</code> <code>Union[Path, str]</code> <p>image directory</p> required <code>num_images</code> <code>int</code> <p>number of images to sample</p> required <code>result_dir</code> <code>Union[Path, str]</code> <p>result directory</p> required <code>save_images</code> <code>bool</code> <p>save sampled images. Defaults to True.</p> <code>True</code> <code>hold</code> <code>bool</code> <p>hold process. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>result directory</p>"},{"location":"waffle_menu/active_learning/#randomsampling","title":"RandomSampling","text":"<p>         Bases: <code>ActiveLearning</code></p>"},{"location":"waffle_menu/active_learning/#waffle_menu.active_learning.method.random.RandomSampling.__init__","title":"<code>__init__(seed=0)</code>","text":""},{"location":"waffle_menu/active_learning/#entropysampling","title":"EntropySampling","text":"<p>         Bases: <code>ActiveLearning</code></p>"},{"location":"waffle_menu/active_learning/#waffle_menu.active_learning.method.entropy.EntropySampling.__init__","title":"<code>__init__(hub, image_size=None, letter_box=None, batch_size=32, num_workers=4, device='0')</code>","text":"<p>Entropty Sampling</p> <p>Parameters:</p> Name Type Description Default <code>hub</code> <code>Hub</code> <p>Hub</p> required <code>image_size</code> <code>int</code> <p>Image size. Defaults to None.</p> <code>None</code> <code>letter_box</code> <code>bool</code> <p>Letter box. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size. Defaults to 32.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of workers. Defaults to 4.</p> <code>4</code> <code>device</code> <code>str</code> <p>Device. Defaults to \"0\".</p> <code>'0'</code>"},{"location":"waffle_menu/active_learning/#pl2nsampling","title":"PL2NSampling","text":"<p>         Bases: <code>ActiveLearning</code></p>"},{"location":"waffle_menu/active_learning/#waffle_menu.active_learning.method.pl2n.PL2NSampling.__init__","title":"<code>__init__(hub, diversity_sampling=False, image_size=None, letter_box=None, batch_size=32, num_workers=4, device='0')</code>","text":"<p>PL2N Sampling</p> <p>Parameters:</p> Name Type Description Default <code>hub</code> <code>Hub</code> <p>Hub</p> required <code>diversity_sampling</code> <code>bool</code> <p>Diversity sampling. Defaults to False.</p> <code>False</code> <code>image_size</code> <code>int</code> <p>Image size. Defaults to None.</p> <code>None</code> <code>letter_box</code> <code>bool</code> <p>Letter box. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size. Defaults to 32.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of workers. Defaults to 4.</p> <code>4</code> <code>device</code> <code>str</code> <p>Device. Defaults to \"0\".</p> <code>'0'</code>"}]}